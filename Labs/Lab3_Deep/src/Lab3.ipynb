{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical as make_class_categorical\n",
    "import _pickle as pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBatch(filename):\n",
    "    \"\"\"\n",
    "    Loads batch based on the given filename and produces the X, Y, and y arrays\n",
    "\n",
    "    :param filename: Path of the file\n",
    "    :return: X, Y and y arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # borrowed from https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    dictionary = unpickle(filename)\n",
    "\n",
    "    # borrowed from https://stackoverflow.com/questions/16977385/extract-the-nth-key-in-a-python-dictionary?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "    def ix(dic, n):  # don't use dict as  a variable name\n",
    "        try:\n",
    "            return list(dic)[n]  # or sorted(dic)[n] if you want the keys to be sorted\n",
    "        except IndexError:\n",
    "            print('not enough keys')\n",
    "\n",
    "    garbage = ix(dictionary, 1)\n",
    "    y = dictionary[garbage]\n",
    "    Y = np.transpose(make_class_categorical(y, 10))\n",
    "    garbage = ix(dictionary, 2)\n",
    "    X = np.transpose(dictionary[garbage]) / 255\n",
    "\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shapes_list, std=0.001):\n",
    "    \"\"\"\n",
    "    Initializes the weight and bias arrays for the 2 layers of the network\n",
    "\n",
    "    :param shapes_list: List that contains the shapes of the weight matrices of each layer. The number of layers can be found through\n",
    "                        estimating the length of this list.\n",
    "    :param variance (optional): The variance of the normal distribution that will be used for the initialization of the weights\n",
    "\n",
    "    :return: Weights and bias arrays for each layer of the network stored in lists\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(400)\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for shape in shapes_list:\n",
    "\n",
    "        W = np.random.normal(0, std, size=(shape[0], shape[1]))\n",
    "        b = np.zeros(shape=(shape[0], 1))\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization_k_layers(shapes_list):\n",
    "    \"\"\"\n",
    "    He initialization on the weight matrices.\n",
    "\n",
    "    :param shapes_list: List that contains the dimensions of each layer of the network.\n",
    "\n",
    "    :return: Initialized weight and bias matrices based on He initialization of the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for pair in shapes_list:\n",
    "\n",
    "        weights.append(np.random.randn(pair[0], pair[1]) * np.sqrt(2 / float(pair[0])))\n",
    "        biases.append(np.zeros(shape=(pair[0], 1)))\n",
    "\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function\n",
    "\n",
    "    :param x: Input to the function\n",
    "\n",
    "    :return: Output of ReLU(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    :param X: ND-Array. Probably should be floats.\n",
    "    :param theta: float parameter, used as a multiplier prior to exponentiation. Default = 1.0\n",
    "    :param axis (optional): axis to compute values along. Default is the first non-singleton axis.\n",
    "\n",
    "    :return: An array the same size as X. The result will sum to 1 along the specified axis\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateClassifier(X, weights, biases):\n",
    "    \"\"\"\n",
    "    Computes the Softmax output of the k-layer network, based on input data X and trained weight and bias arrays\n",
    "\n",
    "    :param X: Input data\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "\n",
    "    :return: Softmax output of the trained network, along with the intermediate layer outpouts and activations\n",
    "    \"\"\"\n",
    "\n",
    "    intermediate_outputs = [] # s's\n",
    "    intermediate_activations = [] #h's\n",
    "\n",
    "    s = np.dot(weights[0], X) + biases[0]\n",
    "    intermediate_outputs.append(s)\n",
    "    h = ReLU(s)\n",
    "    intermediate_activations.append(h)\n",
    "\n",
    "    for i in range(1, len(weights) - 1):\n",
    "\n",
    "        s = np.dot(weights[i], intermediate_activations[-1]) + biases[i]\n",
    "        intermediate_outputs.append(s)\n",
    "        h = ReLU(s)\n",
    "        intermediate_activations.append(h)\n",
    "\n",
    "    s = np.dot(weights[-1], intermediate_activations[-1]) + biases[-1]\n",
    "    p = softmax(s, axis=0)\n",
    "\n",
    "    return p, intermediate_activations, intermediate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClasses(p):\n",
    "    \"\"\"\n",
    "    Predicts classes based on the softmax output of the network\n",
    "\n",
    "    :param p: Softmax output of the network\n",
    "    :return: Predicted classes\n",
    "    \"\"\"\n",
    "\n",
    "    return np.argmax(p, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(X, y, weights, biases):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the feed-forward 2-layer network\n",
    "\n",
    "    :param X: Input data\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "\n",
    "    :return: Accuracy performance of the neural network.\n",
    "    \"\"\"\n",
    "    p, _, _ = EvaluateClassifier(X, weights, biases)\n",
    "    predictions = predictClasses(p)\n",
    "\n",
    "    accuracy = round(np.sum(np.where(predictions - y == 0, 1, 0)) * 100 / len(y), 2)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCost(X, Y, weights, biases, regularization_term=0):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss on a batch of data.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "\n",
    "    :return: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    p, _, _ = EvaluateClassifier(X, weights, biases)\n",
    "\n",
    "    cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum() / float(X.shape[1])\n",
    "\n",
    "    weight_sum = 0\n",
    "    for weight in weights:\n",
    "\n",
    "        weight_sum += np.power(weight, 2).sum()\n",
    "\n",
    "    return cross_entropy_loss + regularization_term * weight_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, weights, biases, start_index=0, h=1e-5):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data with numerical computations of great precision, thus slower computations.\n",
    "    Contributed by Josephine Sullivan for educational purposes for the DD2424 Deep Learning in Data Science course.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param Y: One-hot representation of the true labels of input data X.\n",
    "    :param weights: Weights arrays of the k layers.\n",
    "    :param biases: Bias vectors of the k layers.\n",
    "    :param start_index: In case there are already some weights and bias precomputed, we need to compute the numerical gradients for \n",
    "                        those weights and bias that have other shapes (the 2 last layers in fact).\n",
    "\n",
    "    :return: Weight and bias updates of the k layers of our network computed with numerical computations with high precision.\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    for layer_index in range(start_index, len(weights)):\n",
    "        \n",
    "        W = weights[layer_index]\n",
    "        b = biases[layer_index]\n",
    "        \n",
    "        grad_W = np.zeros(W.shape)\n",
    "        grad_b = np.zeros(b.shape)\n",
    "\n",
    "        for i in tqdm(range(b.shape[0])):\n",
    "            b_try = np.copy(b)\n",
    "            b_try[i, 0] -= h\n",
    "            temp_biases = biases.copy()\n",
    "            temp_biases[layer_index] = b_try\n",
    "            c1 = ComputeCost(X=X, Y=Y, weights=weights, biases=temp_biases)\n",
    "            b_try = np.copy(b)\n",
    "            b_try[i, 0] += h\n",
    "            temp_biases = biases.copy()\n",
    "            temp_biases[layer_index] = b_try\n",
    "            c2 = ComputeCost(X=X, Y=Y, weights=weights, biases=temp_biases)\n",
    "\n",
    "            grad_b[i, 0] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        grad_biases.append(grad_b)\n",
    "\n",
    "        for i in tqdm(range(W.shape[0])):\n",
    "            for j in range(W.shape[1]):\n",
    "                W_try = np.copy(W)\n",
    "                W_try[i, j] -= h\n",
    "                temp_weights = weights.copy()\n",
    "                temp_weights[layer_index] = W_try\n",
    "                c1 = ComputeCost(X=X, Y=Y, weights=temp_weights, biases=biases)\n",
    "                W_try = np.copy(W)\n",
    "                W_try[i, j] += h\n",
    "                temp_weights = weights.copy()\n",
    "                temp_weights[layer_index] = W_try\n",
    "                c2 = ComputeCost(X=X, Y=Y, weights=temp_weights, biases=biases)\n",
    "\n",
    "                grad_W[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        grad_weights.append(grad_W)\n",
    "\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradients(X, Y, weights, biases, p, outputs, activations, regularization_term=0):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param weights: Weight matrices of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param p: Softmax probabilities (predictions) of the network over classes.\n",
    "    :param outputs: True outputs of the intermediate layers of the network.\n",
    "    :param activations: ReLU activations of the intermediate layers of the network.\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network\n",
    "    \"\"\"\n",
    "\n",
    "    # Back-propagate output layer at first\n",
    "\n",
    "    weight_updates = []\n",
    "    bias_updates = []\n",
    "\n",
    "    g = p - Y\n",
    "    bias_updates.append(g.sum(axis=1).reshape(biases[-1].shape))\n",
    "    weight_updates.append(np.dot(g, activations[-1].T))\n",
    "\n",
    "    for i in reversed(range(len(weights) -1)):\n",
    "    # Back-propagate the gradient vector g to the layer before\n",
    "\n",
    "        g = np.dot(g.T, weights[i+1])\n",
    "        ind = 1 * (outputs[i] > 0)\n",
    "        g = g.T * ind\n",
    "\n",
    "        if i == 0:\n",
    "            weight_updates.append(np.dot(g, X.T))\n",
    "        else:\n",
    "            weight_updates.append(np.dot(g, activations[i-1].T))\n",
    "\n",
    "        bias_updates.append(np.sum(g, axis=1).reshape(biases[i].shape))\n",
    "\n",
    "    for elem in weight_updates:\n",
    "        elem /= X.shape[1]\n",
    "\n",
    "    for elem in bias_updates:\n",
    "        elem /= X.shape[1]\n",
    "\n",
    "    # Reverse the updates to match the order of the layers\n",
    "    weight_updates = list(reversed(weight_updates)).copy()\n",
    "    bias_updates = list(reversed(bias_updates)).copy()\n",
    "\n",
    "    # Add regularizers\n",
    "    for index in range(len(weight_updates)):\n",
    "        weight_updates[index] += 2*regularization_term * weight_updates[index]\n",
    "\n",
    "    return weight_updates, bias_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(grad_weights, grad_biases, num_weights, num_biases):\n",
    "    \"\"\"\n",
    "    Compares the gradients of both the analytical and numerical method and prints out a message of result\n",
    "    or failure, depending on how close these gradients are between each other.\n",
    "\n",
    "    :param grad_weights: Analytically computed gradients of the weights\n",
    "    :param grad_biases: Analytically computed gradients of the biases\n",
    "    :param num_weights: Numerically computed gradients of the weights\n",
    "    :param num_biases: Numerically computed gradients of the biases\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    for layer_index in range(len(grad_weights)):\n",
    "\n",
    "        print('-----------------')\n",
    "        print(f'Layer no. {layer_index+1}:')\n",
    "\n",
    "        weight_abs = np.abs(grad_weights[layer_index] - num_weights[layer_index])\n",
    "        bias_abs = np.abs(grad_biases[layer_index] - num_biases[layer_index])\n",
    "\n",
    "        weight_nominator = np.average(weight_abs)\n",
    "        bias_nominator = np.average(bias_abs)\n",
    "\n",
    "        grad_weight_abs = np.absolute(grad_weights[layer_index])\n",
    "        grad_weight_num_abs = np.absolute(num_weights[layer_index])\n",
    "\n",
    "        grad_bias_abs = np.absolute(grad_biases[layer_index])\n",
    "        grad_bias_num_abs = np.absolute(num_biases[layer_index])\n",
    "\n",
    "        sum_weight = grad_weight_abs + grad_weight_num_abs\n",
    "        sum_bias = grad_bias_abs + grad_bias_num_abs\n",
    "\n",
    "        print(f'Deviation on weight matrix: {weight_nominator / np.amax(sum_weight)}')\n",
    "        print(f'Deviation on bias vector: {bias_nominator / np.amax(sum_bias)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_momentum(arrays):\n",
    "    \"\"\"\n",
    "    Initializes the momentum arrays to zero numpy arrays.\n",
    "    \n",
    "    :param matrices: Weights or bias that need corresponding momentum arrays.\n",
    "    :return: Numpy zeros for each layer of the same shape\n",
    "    \"\"\"\n",
    "    momentum_matrices = []\n",
    "    for elem in arrays:\n",
    "        momentum_matrices.append(np.zeros(elem.shape))\n",
    "    return momentum_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_momentum(weights, grad_weights, momentum_weights, biases, grad_biases, momentum_biases, eta, momentum_term):\n",
    "    \"\"\"\n",
    "    Add momentum to an array (weight or bias) of the network.\n",
    "\n",
    "    :param weights: The weight matrices of the k layers.\n",
    "    :param grad_weights: The gradient updatea of the weights.\n",
    "    :param momentum_weights: Momentum arrays (v) of the weights.\n",
    "    :param biases: The bias vector of the k layers.\n",
    "    :param grad_biases: The gradient updates for the biases.\n",
    "    :param momentum_biases: Momentum vectors (v) of the weights.\n",
    "    :param eta: Learning rate of the network.\n",
    "    :param momentum_term: Amount of momentum to be taken into account in the updates.\n",
    "\n",
    "    :return: Updated weights and biases of the network with momentum contribution, updated momentumm arrays for the\n",
    "             weights and biases of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    updated_weights = []\n",
    "    updated_biases = []\n",
    "\n",
    "    for index in range(len(weights)):\n",
    "\n",
    "        new_momentum_weight = momentum_term * momentum_weights[index] + eta * grad_weights[index]\n",
    "        momentum_weights[index] = new_momentum_weight\n",
    "        updated_weights.append(weights[index] - new_momentum_weight)\n",
    "\n",
    "        new_momentum_bias = momentum_term * momentum_biases[index] + eta * grad_biases[index]\n",
    "        momentum_biases[index] = new_momentum_bias\n",
    "        updated_biases.append(biases[index] - new_momentum_bias)\n",
    "\n",
    "\n",
    "    return updated_weights, updated_biases, momentum_weights, momentum_biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MiniBatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchGDwithMomentum(X, Y, X_validation, Y_validation, y_validation, GDparams, weights, biases,\n",
    "                            regularization_term=0, momentum_term=0.9):\n",
    "    \"\"\"\n",
    "    Performs mini batch-gradient descent computations.\n",
    "\n",
    "    :param X: Input batch of data\n",
    "    :param Y: One-hot representation of the true labels of the data.\n",
    "    :param X_validation: Input batch of validation data.\n",
    "    :param Y_validation: One-hot representation of the true labels of the validation data.\n",
    "    :param y_validation: True labels of the validation data.\n",
    "    :param GDparams: Gradient descent parameters (number of mini batches to construct, learning rate, epochs)\n",
    "    :param weights: Weight matrices of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "\n",
    "    :return: The weight and bias matrices learnt (trained) from the training process, loss in training and validation set.\n",
    "    \"\"\"\n",
    "    number_of_mini_batches = GDparams[0]\n",
    "    eta = GDparams[1]\n",
    "    epoches = GDparams[2]\n",
    "\n",
    "    cost = []\n",
    "    val_cost = []\n",
    "\n",
    "    momentum_weights = initialize_momentum(weights)\n",
    "    momentum_biases = initialize_momentum(biases)\n",
    "\n",
    "    original_training_cost= ComputeCost(X, Y, weights, biases, regularization_term)\n",
    "    # print('Training set loss before start of training process: '+str(ComputeCost(X, Y, W1, W2, b1, b2, regularization_term)))\n",
    "\n",
    "    best_weights = weights\n",
    "    best_biases = biases\n",
    "\n",
    "    best_validation_set_accuracy = 0\n",
    "\n",
    "    for _ in tqdm(range(epoches)):\n",
    "        # for epoch in range(epoches):\n",
    "\n",
    "        for batch in range(1, int(X.shape[1] / number_of_mini_batches)):\n",
    "            start = (batch - 1) * number_of_mini_batches + 1\n",
    "            end = batch * number_of_mini_batches + 1\n",
    "\n",
    "            p, intermediate_activations, intermediate_outputs = EvaluateClassifier(X[:, start:end], weights, biases)\n",
    "\n",
    "            grad_weights, grad_biases = ComputeGradients(X[:, start:end], Y[:, start:end], weights, biases, p, intermediate_outputs, intermediate_activations, regularization_term)\n",
    "\n",
    "            weights, biases, momentum_weights, momentum_biases = add_momentum(weights, grad_weights, momentum_weights, biases, grad_biases, momentum_biases, eta, momentum_term)\n",
    "\n",
    "        validation_set_accuracy = ComputeAccuracy(X_validation, y_validation, weights, biases)\n",
    "\n",
    "        if validation_set_accuracy > best_validation_set_accuracy:\n",
    "\n",
    "            best_weights = weights\n",
    "            best_biases = biases\n",
    "            best_validation_set_accuracy = validation_set_accuracy\n",
    "\n",
    "        epoch_cost = ComputeCost(X, Y, weights, biases, regularization_term)\n",
    "        # print('Training set loss after epoch number '+str(epoch)+' is: '+str(epoch_cost))\n",
    "        if epoch_cost > 3 * original_training_cost:\n",
    "            break\n",
    "        val_epoch_cost = ComputeCost(X_validation, Y_validation, weights, biases, regularization_term)\n",
    "\n",
    "        cost.append(epoch_cost)\n",
    "        val_cost.append(val_epoch_cost)\n",
    "\n",
    "        # Decay the learning rate\n",
    "        eta *= 0.95\n",
    "\n",
    "    return best_weights, best_biases, cost, val_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_costs(loss, val_loss, display=False, title=None, save_name=None, save_path='../figures/'):\n",
    "    \"\"\"\n",
    "    Visualization and saving the losses of the network.\n",
    "\n",
    "    :param loss: Loss of the network.\n",
    "    :param val_loss: Loss of the network in the validation set.\n",
    "    :param display: (Optional) Boolean, set to True for displaying the loss evolution plot.\n",
    "    :param title: (Optional) Title of the plot.\n",
    "    :param save_name: (Optional) name of the file to save the plot.\n",
    "    :param save_path: (Optional) Path of the folder to save the plot in your local computer.\n",
    "\n",
    "    :return: None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.plot(loss, 'g', label='Training set ')\n",
    "    plt.plot(val_loss, 'r', label='Validation set')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    if save_name is not None:\n",
    "        if save_path[-1] != '/':\n",
    "            save_path += '/'\n",
    "        plt.savefig(save_path + save_name)\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1: Upgrade your code from assignment 2 so that you can train a k-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with numerically computed gradients of a 2-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [10, 50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_1, Y_training_1, y_training_1 = LoadBatch('../../cifar-10-batches-py/data_batch_1')\n",
    "X_training_2, Y_training_2, y_training_2 = LoadBatch('../../cifar-10-batches-py/data_batch_2')\n",
    "X_test, _, y_test = LoadBatch('../../cifar-10-batches-py/test_batch')\n",
    "\n",
    "mean = np.mean(X_training_1)\n",
    "X_training_1 -= mean\n",
    "X_training_2 -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already available the numerical gradients for 2 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_num = np.load('grad_W1_num.npy')\n",
    "W2_num = np.load('grad_W2_num.npy')\n",
    "\n",
    "b1_num = np.load('grad_b1_num.npy')\n",
    "b2_num = np.load('grad_b2_num.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_weights = [W1_num, W2_num]\n",
    "num_biases = [b1_num, b2_num]\n",
    "\n",
    "p, activations, outputs = EvaluateClassifier(X_training_1[:, 0:2], weights, biases)\n",
    "grad_weights, grad_biases = ComputeGradients(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases, p, outputs, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 7.52283628908819e-09\n",
      "Deviation on bias vector: 4.602426291547493e-09\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 3.0747216543846533e-10\n",
      "Deviation on bias vector: 1.1936382115255306e-11\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_weights, grad_biases, num_weights, num_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviation of at most $10^{-9}$, all good to this point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with computing gradients for a 3-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [20, 50], [10, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 99.05it/s]\n",
      "100%|██████████| 50/50 [24:35<00:00, 29.50s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 112.14it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 111.00it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.53it/s]\n"
     ]
    }
   ],
   "source": [
    "grad_weights_3_num, grad_bias_3_num = ComputeGradsNumSlow(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_index in range(len(grad_weights_3_num)):\n",
    "    \n",
    "    np.save(f'3_layers_num_weights{weight_index}', grad_weights_3_num[weight_index])\n",
    "    np.save(f'3_layers_num_bias{weight_index}', grad_bias_3_num[weight_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, activations, outputs = EvaluateClassifier(X_training_1[:, 0:2], weights, biases)\n",
    "grad_weights, grad_biases = ComputeGradients(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases, p, outputs, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 2.1237177447134792e-06\n",
      "Deviation on bias vector: 1.047827661340416e-06\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 1.197615459525251e-07\n",
      "Deviation on bias vector: 0.0032312506974258464\n",
      "-----------------\n",
      "Layer no. 3:\n",
      "Deviation on weight matrix: 7.368229647407527e-08\n",
      "Deviation on bias vector: 1.677023298915395e-11\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_weights, grad_biases, grad_weights_3_num, grad_bias_3_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good! Let's check 4 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [20, 50], [15, 20], [10,15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the correctly shaped matrices to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 97.39it/s]\n",
      "100%|██████████| 50/50 [25:01<00:00, 30.02s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 101.33it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 111.27it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.65it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 110.81it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.57it/s]\n"
     ]
    }
   ],
   "source": [
    "grad_weights_4_num, grad_bias_4_num = ComputeGradsNumSlow(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_index in range(len(grad_weights_4_num)):\n",
    "    \n",
    "    np.save(f'4_layers_num_weights{weight_index}', grad_weights_4_num[weight_index])\n",
    "    np.save(f'4_layers_num_bias{weight_index}', grad_bias_4_num[weight_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, activations, outputs = EvaluateClassifier(X_training_1[:, 0:2], weights, biases)\n",
    "grad_weights, grad_biases = ComputeGradients(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases, p, outputs, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 0.0004455622900617491\n",
      "Deviation on bias vector: 0.00021413577255454145\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 2.8193570737353638e-05\n",
      "Deviation on bias vector: 0.014347056990204365\n",
      "-----------------\n",
      "Layer no. 3:\n",
      "Deviation on weight matrix: 8.838559097221378e-06\n",
      "Deviation on bias vector: 0.17027488699016166\n",
      "-----------------\n",
      "Layer no. 4:\n",
      "Deviation on weight matrix: 2.0539890893159423e-05\n",
      "Deviation on bias vector: 9.891207642802771e-12\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_weights, grad_biases, grad_weights_4_num, grad_bias_4_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2: Can I train a 3-layer network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test that you are able to replicate the results of a 2-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [10, 50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to replicate the performance of 44.44% for 10 epochs of training and $(\\eta, \\lambda) = (0.01713848118474131, 0.0001)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:03<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "GD_params = [100, 0.0171384811847413, 10]\n",
    "\n",
    "weights, biases, training_cost, validation_cost =  MiniBatchGDwithMomentum(  X_training_1,\n",
    "                                                                             Y_training_1,\n",
    "                                                                             X_training_2,\n",
    "                                                                             Y_training_2,\n",
    "                                                                             y_training_2,\n",
    "                                                                             GD_params,\n",
    "                                                                             weights, biases,\n",
    "                                                                             regularization_term=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_accuracy = ComputeAccuracy(X_training_2, y_training_2, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.16"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the randomness of the initialization on the weights, the deviation is well justified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens after a few epochs? Are you learning anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:24<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3022672535670137\n",
      "Cost at training epoch 2 is 2.3022666352616192\n",
      "Cost at training epoch 3 is 2.302262164108896\n",
      "Cost at training epoch 4 is 2.302256330131588\n",
      "Cost at training epoch 5 is 2.3022509063293954\n",
      "Cost at training epoch 6 is 2.302246087045059\n",
      "Cost at training epoch 7 is 2.30224183447161\n",
      "Cost at training epoch 8 is 2.3022380835454994\n",
      "Cost at training epoch 9 is 2.3022347717577407\n",
      "Cost at training epoch 10 is 2.3022318440902114\n",
      "Cost at training epoch 11 is 2.3022292524125856\n",
      "Cost at training epoch 12 is 2.3022269539491447\n",
      "Cost at training epoch 13 is 2.3022249115918183\n",
      "Cost at training epoch 14 is 2.302223092730126\n",
      "Cost at training epoch 15 is 2.3022214685589133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [30, 50], [10,30]])\n",
    "\n",
    "GD_params = [100, 0.0171384811847413, 15]\n",
    "\n",
    "weights, biases, training_cost, validation_cost = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights, biases,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n",
    "for i in range(len(training_cost)):\n",
    "\n",
    "    print(f'Cost at training epoch {i+1} is {training_cost[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss after at each one of the 15 epochs of training is almost stable at 2.3! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if you play around with the learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Eta:  1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.30260046125\n",
      "Cost at training epoch 2 is 2.3026003916138555\n",
      "Cost at training epoch 3 is 2.302600325471149\n",
      "Cost at training epoch 4 is 2.3026002626463438\n",
      "Cost at training epoch 5 is 2.3026002029724792\n",
      "Cost at training epoch 6 is 2.3026001462910486\n",
      "Cost at training epoch 7 is 2.3026000924515975\n",
      "Cost at training epoch 8 is 2.302600041311279\n",
      "Cost at training epoch 9 is 2.3025999927344216\n",
      "Cost at training epoch 10 is 2.3025999465922684\n",
      "------------------\n",
      "Eta:  1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3025999005718703\n",
      "Cost at training epoch 2 is 2.302599205749114\n",
      "Cost at training epoch 3 is 2.3025985468529453\n",
      "Cost at training epoch 4 is 2.3025979219724637\n",
      "Cost at training epoch 5 is 2.302597329300965\n",
      "Cost at training epoch 6 is 2.302596767131986\n",
      "Cost at training epoch 7 is 2.3025962338540733\n",
      "Cost at training epoch 8 is 2.302595727947948\n",
      "Cost at training epoch 9 is 2.3025952479739393\n",
      "Cost at training epoch 10 is 2.3025947925707957\n",
      "------------------\n",
      "Eta:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3025943342251414\n",
      "Cost at training epoch 2 is 2.30258753778816\n",
      "Cost at training epoch 3 is 2.3025811965366985\n",
      "Cost at training epoch 4 is 2.302575274681294\n",
      "Cost at training epoch 5 is 2.3025697397215943\n",
      "Cost at training epoch 6 is 2.3025645621776087\n",
      "Cost at training epoch 7 is 2.3025597151526602\n",
      "Cost at training epoch 8 is 2.302555174170237\n",
      "Cost at training epoch 9 is 2.302550916949251\n",
      "Cost at training epoch 10 is 2.302546923101441\n",
      "------------------\n",
      "Eta:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.302542530097267\n",
      "Cost at training epoch 2 is 2.3024880377323287\n",
      "Cost at training epoch 3 is 2.302444833211863\n",
      "Cost at training epoch 4 is 2.3024102404890057\n",
      "Cost at training epoch 5 is 2.302382282712888\n",
      "Cost at training epoch 6 is 2.3023594877124416\n",
      "Cost at training epoch 7 is 2.3023407475778948\n",
      "Cost at training epoch 8 is 2.302325219905004\n",
      "Cost at training epoch 9 is 2.3023122586043185\n",
      "Cost at training epoch 10 is 2.3023013637986485\n",
      "------------------\n",
      "Eta:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3022832235898933\n",
      "Cost at training epoch 2 is 2.3022314934409445\n",
      "Cost at training epoch 3 is 2.3022246500421755\n",
      "Cost at training epoch 4 is 2.302222636213854\n",
      "Cost at training epoch 5 is 2.302221210438927\n",
      "Cost at training epoch 6 is 2.302219944464948\n",
      "Cost at training epoch 7 is 2.3022188047640544\n",
      "Cost at training epoch 8 is 2.3022177857331054\n",
      "Cost at training epoch 9 is 2.3022168786733173\n",
      "Cost at training epoch 10 is 2.3022160723738483\n",
      "------------------\n",
      "Eta:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.304083147255321\n",
      "Cost at training epoch 2 is 2.3040527762143457\n",
      "Cost at training epoch 3 is 2.3040247984649436\n",
      "Cost at training epoch 4 is 2.303963513926076\n",
      "Cost at training epoch 5 is 2.1974427096362485\n",
      "Cost at training epoch 6 is 2.007216352396664\n",
      "Cost at training epoch 7 is 1.8656019902160936\n",
      "Cost at training epoch 8 is 1.7714920839464587\n",
      "Cost at training epoch 9 is 1.6672864486126826\n",
      "Cost at training epoch 10 is 1.593195430942723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for eta in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1]:\n",
    "\n",
    "    weights, biases = initialize_weights([[50, 3072], [30, 50], [10,30]])\n",
    "\n",
    "    print('------------------')\n",
    "    print('Eta: ', eta)\n",
    "    GD_params = [100, eta, 10]\n",
    "\n",
    "    weights, biases, training_cost, validation_cost = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                              Y_training_1,\n",
    "                                                                              X_training_2,\n",
    "                                                                              Y_training_2,\n",
    "                                                                              y_training_2,\n",
    "                                                                              GD_params,\n",
    "                                                                              weights, biases,\n",
    "                                                                              regularization_term=0.0001)\n",
    "\n",
    "    for i in range(len(training_cost)):\n",
    "        print(f'Cost at training epoch {i+1} is {training_cost[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the loss remains at the same levels no matter how many update steps take part in the training! Only with a fairly big learning rate of 0.1 we can see a small drop in the loss, but again a few epochs in the beginning where the loss remain stable can be observed! We can also suspect that with such a big learning rate, the learning might be unstable. The last experiment is conducted for more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Eta:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:09<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.304083147255321\n",
      "Cost at training epoch 2 is 2.3040527762143457\n",
      "Cost at training epoch 3 is 2.3040247984649436\n",
      "Cost at training epoch 4 is 2.303963513926076\n",
      "Cost at training epoch 5 is 2.1974427096362485\n",
      "Cost at training epoch 6 is 2.007216352396664\n",
      "Cost at training epoch 7 is 1.8656019902160936\n",
      "Cost at training epoch 8 is 1.7714920839464587\n",
      "Cost at training epoch 9 is 1.6672864486126826\n",
      "Cost at training epoch 10 is 1.593195430942723\n",
      "Cost at training epoch 11 is 1.5485123476073677\n",
      "Cost at training epoch 12 is 1.4758706564115107\n",
      "Cost at training epoch 13 is 1.4193924744826478\n",
      "Cost at training epoch 14 is 1.3805438066881317\n",
      "Cost at training epoch 15 is 1.368697249854167\n",
      "Cost at training epoch 16 is 1.3265087669239681\n",
      "Cost at training epoch 17 is 1.2723064324945854\n",
      "Cost at training epoch 18 is 1.2625014563672678\n",
      "Cost at training epoch 19 is 1.2666867686005\n",
      "Cost at training epoch 20 is 1.25163518706638\n",
      "Cost at training epoch 21 is 1.1950692780730536\n",
      "Cost at training epoch 22 is 1.1405550951346806\n",
      "Cost at training epoch 23 is 1.0933375948769273\n",
      "Cost at training epoch 24 is 1.0584354555304865\n",
      "Cost at training epoch 25 is 1.0211788369490433\n",
      "Cost at training epoch 26 is 1.0002588070688778\n",
      "Cost at training epoch 27 is 0.9777619847054716\n",
      "Cost at training epoch 28 is 0.9753313784903359\n",
      "Cost at training epoch 29 is 0.9558171620564835\n",
      "Cost at training epoch 30 is 0.9544156273418551\n",
      "Cost at training epoch 31 is 0.9515871093045444\n",
      "Cost at training epoch 32 is 0.93641149448918\n",
      "Cost at training epoch 33 is 0.9511512546042148\n",
      "Cost at training epoch 34 is 0.8900400762553463\n",
      "Cost at training epoch 35 is 0.891207534551131\n",
      "Cost at training epoch 36 is 0.840768908056229\n",
      "Cost at training epoch 37 is 0.8274775595253576\n",
      "Cost at training epoch 38 is 0.799688813021933\n",
      "Cost at training epoch 39 is 0.7950914142037012\n",
      "Cost at training epoch 40 is 0.7716458010015995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [30, 50], [10,30]])\n",
    "\n",
    "print('------------------')\n",
    "print('Eta: ', 0.1)\n",
    "GD_params = [100, 0.1, 40]\n",
    "\n",
    "weights, biases, training_cost, validation_cost = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights, biases,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n",
    "for i in range(len(training_cost)):\n",
    "    print(f'Cost at training epoch {i+1} is {training_cost[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_costs(training_cost, validation_cost, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting with this setting, occurs very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if you use He initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of training with He initialization at epoch 1 is 2.573371144365728\n",
      "Cost of training with He initialization at epoch 2 is 2.4388440346442506\n",
      "Cost of training with He initialization at epoch 3 is 2.3685662732480903\n",
      "Cost of training with He initialization at epoch 4 is 2.3142257340862384\n",
      "Cost of training with He initialization at epoch 5 is 2.272922477058916\n",
      "Cost of training with He initialization at epoch 6 is 2.236912218216597\n",
      "Cost of training with He initialization at epoch 7 is 2.2052890641621268\n",
      "Cost of training with He initialization at epoch 8 is 2.1762496203378463\n",
      "Cost of training with He initialization at epoch 9 is 2.1514607505444694\n",
      "Cost of training with He initialization at epoch 10 is 2.1297865298452456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights_he, biases_he = he_initialization_k_layers([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "GD_params = [100, 0.0171384811847413, 10]\n",
    "\n",
    "weights_he, biases_he, training_cost_he, validation_cost_he = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights_he, biases_he,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n",
    "for i in range(len(training_cost_he)):\n",
    "    print(f'Cost of training with He initialization at epoch {i+1} is {training_cost_he[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the loss does not remain stable, we don't observe much of a change. Again, we try for some more epochs and now plot the evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:06<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "weights_he, biases_he = he_initialization_k_layers([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "GD_params = [100, 0.0171384811847413, 40]\n",
    "\n",
    "weights_he, biases_he, training_cost_he, validation_cost_he = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights_he, biases_he,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
