{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical as make_class_categorical\n",
    "import _pickle as pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBatch(filename):\n",
    "    \"\"\"\n",
    "    Loads batch based on the given filename and produces the X, Y, and y arrays\n",
    "\n",
    "    :param filename: Path of the file\n",
    "    :return: X, Y and y arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # borrowed from https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    dictionary = unpickle(filename)\n",
    "\n",
    "    # borrowed from https://stackoverflow.com/questions/16977385/extract-the-nth-key-in-a-python-dictionary?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "    def ix(dic, n):  # don't use dict as  a variable name\n",
    "        try:\n",
    "            return list(dic)[n]  # or sorted(dic)[n] if you want the keys to be sorted\n",
    "        except IndexError:\n",
    "            print('not enough keys')\n",
    "\n",
    "    garbage = ix(dictionary, 1)\n",
    "    y = dictionary[garbage]\n",
    "    Y = np.transpose(make_class_categorical(y, 10))\n",
    "    garbage = ix(dictionary, 2)\n",
    "    X = np.transpose(dictionary[garbage]) / 255\n",
    "\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shapes_list, std=0.001):\n",
    "    \"\"\"\n",
    "    Initializes the weight and bias arrays for the 2 layers of the network\n",
    "\n",
    "    :param shapes_list: List that contains the shapes of the weight matrices of each layer. The number of layers can be found through\n",
    "                        estimating the length of this list.\n",
    "    :param variance (optional): The variance of the normal distribution that will be used for the initialization of the weights\n",
    "\n",
    "    :return: Weights and bias arrays for each layer of the network stored in lists\n",
    "    \"\"\"\n",
    "\n",
    "#     np.random.seed(400)\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for shape in shapes_list:\n",
    "\n",
    "        W = np.random.normal(0, std, size=(shape[0], shape[1]))\n",
    "        b = np.zeros(shape=(shape[0], 1))\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization_k_layers(shapes_list):\n",
    "    \"\"\"\n",
    "    He initialization on the weight matrices.\n",
    "\n",
    "    :param shapes_list: List that contains the dimensions of each layer of the network.\n",
    "\n",
    "    :return: Initialized weight and bias matrices based on He initialization of the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for pair in shapes_list:\n",
    "\n",
    "        weights.append(np.random.randn(pair[0], pair[1]) * np.sqrt(2 / float(pair[0])))\n",
    "        biases.append(np.zeros(shape=(pair[0], 1)))\n",
    "\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function\n",
    "\n",
    "    :param x: Input to the function\n",
    "\n",
    "    :return: Output of ReLU(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    :param X: ND-Array. Probably should be floats.\n",
    "    :param theta: float parameter, used as a multiplier prior to exponentiation. Default = 1.0\n",
    "    :param axis (optional): axis to compute values along. Default is the first non-singleton axis.\n",
    "\n",
    "    :return: An array the same size as X. The result will sum to 1 along the specified axis\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateClassifier(X, weights, biases):\n",
    "    \"\"\"\n",
    "    Computes the Softmax output of the k-layer network, based on input data X and trained weight and bias arrays\n",
    "\n",
    "    :param X: Input data\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "\n",
    "    :return: Softmax output of the trained network, along with the intermediate layer outpouts and activations\n",
    "    \"\"\"\n",
    "\n",
    "    intermediate_outputs = [] # s's\n",
    "    intermediate_activations = [] #h's\n",
    "\n",
    "    s = np.dot(weights[0], X) + biases[0]\n",
    "    intermediate_outputs.append(s)\n",
    "    h = ReLU(s)\n",
    "    intermediate_activations.append(h)\n",
    "\n",
    "    for i in range(1, len(weights) - 1):\n",
    "\n",
    "        s = np.dot(weights[i], intermediate_activations[-1]) + biases[i]\n",
    "        intermediate_outputs.append(s)\n",
    "        h = ReLU(s)\n",
    "        intermediate_activations.append(h)\n",
    "\n",
    "    s = np.dot(weights[-1], intermediate_activations[-1]) + biases[-1]\n",
    "    p = softmax(s, axis=0)\n",
    "\n",
    "    return p, intermediate_activations, intermediate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClasses(p):\n",
    "    \"\"\"\n",
    "    Predicts classes based on the softmax output of the network\n",
    "\n",
    "    :param p: Softmax output of the network\n",
    "    :return: Predicted classes\n",
    "    \"\"\"\n",
    "\n",
    "    return np.argmax(p, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(X, y, weights, biases):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the feed-forward 2-layer network\n",
    "\n",
    "    :param X: Input data\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "\n",
    "    :return: Accuracy performance of the neural network.\n",
    "    \"\"\"\n",
    "    p, _, _ = EvaluateClassifier(X, weights, biases)\n",
    "    predictions = predictClasses(p)\n",
    "\n",
    "    accuracy = round(np.sum(np.where(predictions - y == 0, 1, 0)) * 100 / len(y), 2)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCost(X, Y, weights, biases, regularization_term=0):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss on a batch of data.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "\n",
    "    :return: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    p, _, _ = EvaluateClassifier(X, weights, biases)\n",
    "\n",
    "    cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum() / float(X.shape[1])\n",
    "\n",
    "    weight_sum = 0\n",
    "    for weight in weights:\n",
    "\n",
    "        weight_sum += np.power(weight, 2).sum()\n",
    "\n",
    "    return cross_entropy_loss + regularization_term * weight_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, weights, biases, start_index=0, h=1e-5):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data with numerical computations of great precision, thus slower computations.\n",
    "    Contributed by Josephine Sullivan for educational purposes for the DD2424 Deep Learning in Data Science course.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param Y: One-hot representation of the true labels of input data X.\n",
    "    :param weights: Weights arrays of the k layers.\n",
    "    :param biases: Bias vectors of the k layers.\n",
    "    :param start_index: In case there are already some weights and bias precomputed, we need to compute the numerical gradients for \n",
    "                        those weights and bias that have other shapes (the 2 last layers in fact).\n",
    "\n",
    "    :return: Weight and bias updates of the k layers of our network computed with numerical computations with high precision.\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "    \n",
    "    for layer_index in range(start_index, len(weights)):\n",
    "        \n",
    "        W = weights[layer_index]\n",
    "        b = biases[layer_index]\n",
    "        \n",
    "        grad_W = np.zeros(W.shape)\n",
    "        grad_b = np.zeros(b.shape)\n",
    "\n",
    "        for i in tqdm(range(b.shape[0])):\n",
    "            b_try = np.copy(b)\n",
    "            b_try[i, 0] -= h\n",
    "            temp_biases = biases.copy()\n",
    "            temp_biases[layer_index] = b_try\n",
    "            c1 = ComputeCost(X=X, Y=Y, weights=weights, biases=temp_biases)\n",
    "            b_try = np.copy(b)\n",
    "            b_try[i, 0] += h\n",
    "            temp_biases = biases.copy()\n",
    "            temp_biases[layer_index] = b_try\n",
    "            c2 = ComputeCost(X=X, Y=Y, weights=weights, biases=temp_biases)\n",
    "\n",
    "            grad_b[i, 0] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        grad_biases.append(grad_b)\n",
    "\n",
    "        for i in tqdm(range(W.shape[0])):\n",
    "            for j in range(W.shape[1]):\n",
    "                W_try = np.copy(W)\n",
    "                W_try[i, j] -= h\n",
    "                temp_weights = weights.copy()\n",
    "                temp_weights[layer_index] = W_try\n",
    "                c1 = ComputeCost(X=X, Y=Y, weights=temp_weights, biases=biases)\n",
    "                W_try = np.copy(W)\n",
    "                W_try[i, j] += h\n",
    "                temp_weights = weights.copy()\n",
    "                temp_weights[layer_index] = W_try\n",
    "                c2 = ComputeCost(X=X, Y=Y, weights=temp_weights, biases=biases)\n",
    "\n",
    "                grad_W[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        grad_weights.append(grad_W)\n",
    "\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlowBatchNorm(X, Y, weights, biases, start_index=0, h=1e-5):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data with numerical computations of great precision, thus slower computations.\n",
    "    Contributed by Josephine Sullivan for educational purposes for the DD2424 Deep Learning in Data Science course.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param Y: One-hot representation of the true labels of input data X.\n",
    "    :param weights: Weights arrays of the k layers.\n",
    "    :param biases: Bias vectors of the k layers.\n",
    "    :param start_index: In case there are already some weights and bias precomputed, we need to compute the numerical gradients for\n",
    "                        those weights and bias that have other shapes (the 2 last layers in fact).\n",
    "\n",
    "    :return: Weight and bias updates of the k layers of our network computed with numerical computations with high precision.\n",
    "    \"\"\"\n",
    "\n",
    "    grad_weights = []\n",
    "    grad_biases = []\n",
    "\n",
    "    for layer_index in range(start_index, len(weights)):\n",
    "\n",
    "        W = weights[layer_index]\n",
    "        b = biases[layer_index]\n",
    "\n",
    "        grad_W = np.zeros(W.shape)\n",
    "        grad_b = np.zeros(b.shape)\n",
    "\n",
    "        for i in tqdm(range(b.shape[0])):\n",
    "            b_try = np.copy(b)\n",
    "            b_try[i, 0] -= h\n",
    "            temp_biases = biases.copy()\n",
    "            temp_biases[layer_index] = b_try\n",
    "            c1 = ComputeCostBatchNormalization(X=X, Y=Y, weights=weights, biases=temp_biases, regularization_term=0)\n",
    "            b_try = np.copy(b)\n",
    "            b_try[i, 0] += h\n",
    "            temp_biases = biases.copy()\n",
    "            temp_biases[layer_index] = b_try\n",
    "            c2 = ComputeCostBatchNormalization(X=X, Y=Y, weights=weights, biases=temp_biases, regularization_term=0)\n",
    "\n",
    "            grad_b[i, 0] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        grad_biases.append(grad_b)\n",
    "\n",
    "        for i in tqdm(range(W.shape[0])):\n",
    "            for j in range(W.shape[1]):\n",
    "                W_try = np.copy(W)\n",
    "                W_try[i, j] -= h\n",
    "                temp_weights = weights.copy()\n",
    "                temp_weights[layer_index] = W_try\n",
    "                c1 = ComputeCostBatchNormalization(X=X, Y=Y, weights=temp_weights, biases=biases, regularization_term=0)\n",
    "                W_try = np.copy(W)\n",
    "                W_try[i, j] += h\n",
    "                temp_weights = weights.copy()\n",
    "                temp_weights[layer_index] = W_try\n",
    "                c2 = ComputeCostBatchNormalization(X=X, Y=Y, weights=temp_weights, biases=biases, regularization_term=0)\n",
    "\n",
    "                grad_W[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "        grad_weights.append(grad_W)\n",
    "\n",
    "    return grad_weights, grad_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradients(X, Y, weights, biases, p, outputs, activations, regularization_term=0):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param weights: Weight matrices of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param p: Softmax probabilities (predictions) of the network over classes.\n",
    "    :param outputs: True outputs of the intermediate layers of the network.\n",
    "    :param activations: ReLU activations of the intermediate layers of the network.\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network\n",
    "    \"\"\"\n",
    "\n",
    "    # Back-propagate output layer at first\n",
    "\n",
    "    weight_updates = []\n",
    "    bias_updates = []\n",
    "\n",
    "    g = p - Y\n",
    "    bias_updates.append(g.sum(axis=1).reshape(biases[-1].shape))\n",
    "    weight_updates.append(np.dot(g, activations[-1].T))\n",
    "\n",
    "    for i in reversed(range(len(weights) -1)):\n",
    "    # Back-propagate the gradient vector g to the layer before\n",
    "\n",
    "        g = np.dot(g.T, weights[i+1])\n",
    "        ind = 1 * (outputs[i] > 0)\n",
    "        g = g.T * ind\n",
    "\n",
    "        if i == 0:\n",
    "            weight_updates.append(np.dot(g, X.T))\n",
    "        else:\n",
    "            weight_updates.append(np.dot(g, activations[i-1].T))\n",
    "\n",
    "        bias_updates.append(np.sum(g, axis=1).reshape(biases[i].shape))\n",
    "\n",
    "    for elem in weight_updates:\n",
    "        elem /= X.shape[1]\n",
    "\n",
    "    for elem in bias_updates:\n",
    "        elem /= X.shape[1]\n",
    "\n",
    "    # Reverse the updates to match the order of the layers\n",
    "    weight_updates = list(reversed(weight_updates)).copy()\n",
    "    bias_updates = list(reversed(bias_updates)).copy()\n",
    "\n",
    "    # Add regularizers\n",
    "    for index in range(len(weight_updates)):\n",
    "        weight_updates[index] += 2*regularization_term * weight_updates[index]\n",
    "\n",
    "    return weight_updates, bias_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(grad_weights, grad_biases, num_weights, num_biases):\n",
    "    \"\"\"\n",
    "    Compares the gradients of both the analytical and numerical method and prints out a message of result\n",
    "    or failure, depending on how close these gradients are between each other.\n",
    "\n",
    "    :param grad_weights: Analytically computed gradients of the weights\n",
    "    :param grad_biases: Analytically computed gradients of the biases\n",
    "    :param num_weights: Numerically computed gradients of the weights\n",
    "    :param num_biases: Numerically computed gradients of the biases\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    for layer_index in range(len(grad_weights)):\n",
    "\n",
    "        print('-----------------')\n",
    "        print(f'Layer no. {layer_index+1}:')\n",
    "\n",
    "        weight_abs = np.abs(grad_weights[layer_index] - num_weights[layer_index])\n",
    "        bias_abs = np.abs(grad_biases[layer_index] - num_biases[layer_index])\n",
    "\n",
    "        weight_nominator = np.average(weight_abs)\n",
    "        bias_nominator = np.average(bias_abs)\n",
    "\n",
    "        grad_weight_abs = np.absolute(grad_weights[layer_index])\n",
    "        grad_weight_num_abs = np.absolute(num_weights[layer_index])\n",
    "\n",
    "        grad_bias_abs = np.absolute(grad_biases[layer_index])\n",
    "        grad_bias_num_abs = np.absolute(num_biases[layer_index])\n",
    "\n",
    "        sum_weight = grad_weight_abs + grad_weight_num_abs\n",
    "        sum_bias = grad_bias_abs + grad_bias_num_abs\n",
    "\n",
    "        print(f'Deviation on weight matrix: {weight_nominator / np.amax(sum_weight)}')\n",
    "        print(f'Deviation on bias vector: {bias_nominator / np.amax(sum_bias)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_momentum(arrays):\n",
    "    \"\"\"\n",
    "    Initializes the momentum arrays to zero numpy arrays.\n",
    "    \n",
    "    :param matrices: Weights or bias that need corresponding momentum arrays.\n",
    "    :return: Numpy zeros for each layer of the same shape\n",
    "    \"\"\"\n",
    "    momentum_matrices = []\n",
    "    for elem in arrays:\n",
    "        momentum_matrices.append(np.zeros(elem.shape))\n",
    "    return momentum_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_momentum(weights, grad_weights, momentum_weights, biases, grad_biases, momentum_biases, eta, momentum_term=0.9):\n",
    "    \"\"\"\n",
    "    Add momentum to an array (weight or bias) of the network.\n",
    "\n",
    "    :param weights: The weight matrices of the k layers.\n",
    "    :param grad_weights: The gradient updatea of the weights.\n",
    "    :param momentum_weights: Momentum arrays (v) of the weights.\n",
    "    :param biases: The bias vector of the k layers.\n",
    "    :param grad_biases: The gradient updates for the biases.\n",
    "    :param momentum_biases: Momentum vectors (v) of the weights.\n",
    "    :param eta: Learning rate of the network.\n",
    "    :param momentum_term: Amount of momentum to be taken into account in the updates.\n",
    "\n",
    "    :return: Updated weights and biases of the network with momentum contribution, updated momentumm arrays for the\n",
    "             weights and biases of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    updated_weights = []\n",
    "    updated_biases = []\n",
    "\n",
    "    for index in range(len(weights)):\n",
    "\n",
    "        new_momentum_weight = momentum_term * momentum_weights[index] + eta * grad_weights[index]\n",
    "        momentum_weights[index] = new_momentum_weight\n",
    "        updated_weights.append(weights[index] - new_momentum_weight)\n",
    "\n",
    "        new_momentum_bias = momentum_term * momentum_biases[index] + eta * grad_biases[index]\n",
    "        momentum_biases[index] = new_momentum_bias\n",
    "        updated_biases.append(biases[index] - new_momentum_bias)\n",
    "\n",
    "\n",
    "    return updated_weights, updated_biases, momentum_weights, momentum_biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MiniBatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchGDwithMomentum(X, Y, X_validation, Y_validation, y_validation, GDparams, weights, biases,\n",
    "                            regularization_term=0, momentum_term=0.9):\n",
    "    \"\"\"\n",
    "    Performs mini batch-gradient descent computations.\n",
    "\n",
    "    :param X: Input batch of data\n",
    "    :param Y: One-hot representation of the true labels of the data.\n",
    "    :param X_validation: Input batch of validation data.\n",
    "    :param Y_validation: One-hot representation of the true labels of the validation data.\n",
    "    :param y_validation: True labels of the validation data.\n",
    "    :param GDparams: Gradient descent parameters (number of mini batches to construct, learning rate, epochs)\n",
    "    :param weights: Weight matrices of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "\n",
    "    :return: The weight and bias matrices learnt (trained) from the training process, loss in training and validation set.\n",
    "    \"\"\"\n",
    "    number_of_mini_batches = GDparams[0]\n",
    "    eta = GDparams[1]\n",
    "    epoches = GDparams[2]\n",
    "\n",
    "    cost = []\n",
    "    val_cost = []\n",
    "\n",
    "    momentum_weights = initialize_momentum(weights)\n",
    "    momentum_biases = initialize_momentum(biases)\n",
    "\n",
    "    original_training_cost= ComputeCost(X, Y, weights, biases, regularization_term)\n",
    "    # print('Training set loss before start of training process: '+str(ComputeCost(X, Y, W1, W2, b1, b2, regularization_term)))\n",
    "\n",
    "    best_weights = weights\n",
    "    best_biases = biases\n",
    "\n",
    "    best_validation_set_accuracy = 0\n",
    "\n",
    "    for _ in tqdm(range(epoches)):\n",
    "        # for epoch in range(epoches):\n",
    "\n",
    "        for batch in range(1, int(X.shape[1] / number_of_mini_batches)):\n",
    "            start = (batch - 1) * number_of_mini_batches + 1\n",
    "            end = batch * number_of_mini_batches + 1\n",
    "\n",
    "            p, intermediate_activations, intermediate_outputs = EvaluateClassifier(X[:, start:end], weights, biases)\n",
    "\n",
    "            grad_weights, grad_biases = ComputeGradients(X[:, start:end], Y[:, start:end], weights, biases, p, intermediate_outputs, intermediate_activations, regularization_term)\n",
    "\n",
    "            weights, biases, momentum_weights, momentum_biases = add_momentum(weights, grad_weights, momentum_weights, biases, grad_biases, momentum_biases, eta, momentum_term)\n",
    "\n",
    "        validation_set_accuracy = ComputeAccuracy(X_validation, y_validation, weights, biases)\n",
    "\n",
    "        if validation_set_accuracy > best_validation_set_accuracy:\n",
    "\n",
    "            best_weights = weights\n",
    "            best_biases = biases\n",
    "            best_validation_set_accuracy = validation_set_accuracy\n",
    "\n",
    "        epoch_cost = ComputeCost(X, Y, weights, biases, regularization_term)\n",
    "        # print('Training set loss after epoch number '+str(epoch)+' is: '+str(epoch_cost))\n",
    "        if epoch_cost > 3 * original_training_cost:\n",
    "            break\n",
    "        val_epoch_cost = ComputeCost(X_validation, Y_validation, weights, biases, regularization_term)\n",
    "\n",
    "        cost.append(epoch_cost)\n",
    "        val_cost.append(val_epoch_cost)\n",
    "\n",
    "        # Decay the learning rate\n",
    "        eta *= 0.95\n",
    "\n",
    "    return best_weights, best_biases, cost, val_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_costs(loss, val_loss, display=False, title=None, save_name=None, save_path='../figures/'):\n",
    "    \"\"\"\n",
    "    Visualization and saving the losses of the network.\n",
    "\n",
    "    :param loss: Loss of the network.\n",
    "    :param val_loss: Loss of the network in the validation set.\n",
    "    :param display: (Optional) Boolean, set to True for displaying the loss evolution plot.\n",
    "    :param title: (Optional) Title of the plot.\n",
    "    :param save_name: (Optional) name of the file to save the plot.\n",
    "    :param save_path: (Optional) Path of the folder to save the plot in your local computer.\n",
    "\n",
    "    :return: None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.plot(loss, 'g', label='Training set ')\n",
    "    plt.plot(val_loss, 'r', label='Validation set')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    if save_name is not None:\n",
    "        if save_path[-1] != '/':\n",
    "            save_path += '/'\n",
    "        plt.savefig(save_path + save_name)\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormalize(s, mean_s, var_s, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Normalizes the scores of a batch based on their mean and variance.\n",
    "\n",
    "    :param s: Scores evaluated as output of a layer of the network.\n",
    "    :param mean_s: Mean of the scores.\n",
    "    :param var_s: Variance of the scores.\n",
    "    :param epsilon: A small number that is present to ensure that no division by zero will be performed.\n",
    "\n",
    "    :return: The normalized scores,\n",
    "    \"\"\"\n",
    "\n",
    "    diff = s - mean_s\n",
    "\n",
    "    return diff / (np.sqrt(var_s + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPassBatchNormalization(X, weights, biases, exponentials= None):\n",
    "    \"\"\"\n",
    "    Evaluates the forward pass result of the classifier network using batch normalization.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param weights: Weight arrays of the k-layer network.\n",
    "    :param biases: Bias vectors of the k-layer network.\n",
    "\n",
    "    :return: Softmax probabilities (predictions) of the true labels of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    s = np.dot(weights[0], X) + biases[0]\n",
    "\n",
    "    intermediate_outputs = [s]\n",
    "\n",
    "    if exponentials is not None:\n",
    "\n",
    "        exponential_means = exponentials[0]\n",
    "        exponential_variances = exponentials[1]\n",
    "\n",
    "        mean_s = exponential_means[0]\n",
    "        var_s = exponential_variances[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        mean_s = s.mean(axis=1).reshape(s.shape[0], 1)\n",
    "        var_s = s.var(axis=1).reshape(s.shape[0], 1)\n",
    "\n",
    "        means = [mean_s]\n",
    "        variances = [var_s]\n",
    "\n",
    "    normalized_score = BatchNormalize(s, mean_s, var_s)\n",
    "\n",
    "    batch_normalization_outputs = [normalized_score]\n",
    "    batch_normalization_activations = [ReLU(normalized_score)]\n",
    "\n",
    "    for index in range(1, len(weights) - 1):\n",
    "\n",
    "        s = np.dot(weights[index], batch_normalization_activations[-1]) + biases[index]\n",
    "\n",
    "        intermediate_outputs.append(s)\n",
    "\n",
    "        if exponentials is None:\n",
    "            mean_s = s.mean(axis=1).reshape(s.shape[0], 1)\n",
    "            var_s = s.var(axis=1).reshape(s.shape[0], 1)\n",
    "\n",
    "            means.append(mean_s)\n",
    "            variances.append(var_s)\n",
    "\n",
    "        else:\n",
    "\n",
    "            mean_s = exponential_means[index]\n",
    "            var_s = exponential_variances[index]\n",
    "\n",
    "        normalized_score = BatchNormalize(s, mean_s, var_s)\n",
    "\n",
    "        batch_normalization_outputs.append(normalized_score)\n",
    "        batch_normalization_activations.append(ReLU(normalized_score))\n",
    "\n",
    "    s = np.dot(weights[-1], batch_normalization_activations[-1]) + biases[-1]\n",
    "\n",
    "    p = softmax(s, axis=0)\n",
    "\n",
    "    if exponentials is not None:\n",
    "        return p\n",
    "    else:\n",
    "        return p, batch_normalization_activations, batch_normalization_outputs, intermediate_outputs, means, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracyBatchNormalization(X, y, weights, biases, exponentials = None):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the feed-forward k-layer network\n",
    "\n",
    "    :param X: Input data\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param exponentials: Contains the exponential means and variances computed, they are used in call after training.\n",
    "\n",
    "    :return: Accuracy performance of the neural network.\n",
    "    \"\"\"\n",
    "    if exponentials is not None:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)\n",
    "    else:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)[0]\n",
    "    predictions = predictClasses(p)\n",
    "\n",
    "    accuracy = round(np.sum(np.where(predictions - y == 0, 1, 0)) * 100 / len(y), 2)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCostBatchNormalization(X, Y, weights, biases, regularization_term, exponentials=None):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss on a batch of data.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "    :param exponentials: Contains the exponential means and variances computed, they are used in call after training.\n",
    "\n",
    "    :return: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "    if exponentials is None:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)[0]\n",
    "    else:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)\n",
    "\n",
    "    cross_entropy_loss = 0\n",
    "    for datum_index in range(X.shape[1]):\n",
    "\n",
    "        cross_entropy_loss -= np.log(np.dot(Y[:, datum_index].T, p[:, datum_index]))\n",
    "\n",
    "    weight_sum = 0\n",
    "    for weight in weights:\n",
    "\n",
    "        weight_sum += np.power(weight, 2).sum()\n",
    "\n",
    "    return (cross_entropy_loss / float(X.shape[1]))+ regularization_term * weight_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormBackPass(g, s, mean_s, var_s, epsilon=1e-10):\n",
    "\n",
    "    # First part of the gradient:\n",
    "    V_b = (var_s+ epsilon) ** (-0.5)\n",
    "    V_b = np.power( (var_s+ epsilon), -0.5)\n",
    "    part_1 = g * V_b\n",
    "\n",
    "    # Second part pf the gradient\n",
    "    diff = s - mean_s\n",
    "    grad_J_vb = -0.5 * np.sum(g * np.power((var_s+epsilon), -1.5) * diff, axis=1)\n",
    "    grad_J_vb = -0.5 * np.sum(g * (var_s+epsilon) ** (-1.5) * diff, axis=1)\n",
    "    grad_J_vb = np.expand_dims(grad_J_vb, axis=1)\n",
    "    part_2 = (2/float(s.shape[1])) * grad_J_vb * diff\n",
    "\n",
    "    # Third part of the gradient\n",
    "    grad_J_mb = -np.sum(g * V_b, axis=1)\n",
    "    grad_J_mb = np.expand_dims(grad_J_mb, axis=1)\n",
    "    part_3 = grad_J_mb / float(s.shape[1])\n",
    "\n",
    "    return part_1 + part_2 + part_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackwardPassBatchNormalization(X, Y, weights, biases, p, bn_outputs, bn_activations, intermediate_outputs, means, variances, regularization_term):\n",
    "\n",
    "    # Output layer\n",
    "    g = p - Y\n",
    "\n",
    "    bias_updates = [np.sum(g, axis=1).reshape(biases[-1].shape)]\n",
    "    weight_updates = [np.dot(g, bn_activations[-1].T)]\n",
    "\n",
    "    g = np.dot(g.T, weights[-1])\n",
    "    ind = 1 * (bn_outputs[-1] > 0)\n",
    "    g = g.T * ind\n",
    "\n",
    "    for layer in reversed(range(len(weights) -1)):\n",
    "\n",
    "        g = BatchNormBackPass(g, intermediate_outputs[layer], means[layer], variances[layer])\n",
    "\n",
    "        if layer > 0:\n",
    "\n",
    "            bias_updates.append(np.sum(g, axis=1).reshape(biases[layer].shape))\n",
    "            weight_updates.append(np.dot(g, bn_activations[layer-1].T))\n",
    "\n",
    "            g = np.dot(g.T, weights[layer])\n",
    "            ind = 1 * (bn_outputs[layer - 1] > 0)\n",
    "            g = g.T * ind\n",
    "\n",
    "        else:\n",
    "\n",
    "            bias_updates.append(np.sum(g, axis=1).reshape(biases[layer].shape))\n",
    "            weight_updates.append(np.dot(g, X.T))\n",
    "\n",
    "    weight_updates=list(reversed(weight_updates))\n",
    "    bias_updates=list(reversed(bias_updates))\n",
    "\n",
    "    for index, elem in enumerate(weight_updates):\n",
    "\n",
    "        elem /= X.shape[1]\n",
    "        bias_updates[index] /= X.shape[1]\n",
    "        elem += 2 * regularization_term * weights[index]\n",
    "\n",
    "    return weight_updates, bias_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExponentialMovingAverage(means, exponential_means, variances, exponential_variances, a=0.99):\n",
    "\n",
    "    for index, elem in enumerate(exponential_means):\n",
    "\n",
    "        exponential_means[index] = a * elem + (1-a) * means[index]\n",
    "        exponential_variances[index] = a * exponential_variances[index] + (1-a) * variances[index]\n",
    "\n",
    "    return exponential_means, exponential_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 29%|██▉       | 58/200 [00:22<00:54,  2.59it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "def MiniBatchGDBatchNormalization(X, Y, X_validation, Y_validation, y_validation, GDparams, weights, biases, regularization_term, momentum_term=0.9):\n",
    "\n",
    "    \"\"\"\n",
    "    Performs mini-gradient descent training in the weights and biases of the network, and computes\n",
    "    the loss on training and validation sets, and the exponential moving averages.\n",
    "\n",
    "    :param X: Input training data of the network.\n",
    "    :param Y: One-hot representations of X.\n",
    "    :param X_validation: Validation input data.\n",
    "    :param Y_validation: One-hot representations of X_validation.\n",
    "    :param y_validation: True labels of the validation data\n",
    "    :param GDparams: Gradient-descent parameters (number of mini batches per epoch, learning rate, epochs)\n",
    "    :param weights: Original pre-training weights of the network.\n",
    "    :param biases: Original pre-training biases of the network.\n",
    "    :param regularization_term: Amount of regularization applied to the weights.\n",
    "    :param momentum_term: Percentage of previous state to account for in every weight and bias update.\n",
    "    \n",
    "    :return: Trained weights and biases, loss on training and validation set per training epoch, expoenntial moving means and variances.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loss = [ComputeCostBatchNormalization(X, Y, weights, biases, regularization_term)]\n",
    "    validation_loss = [ComputeCostBatchNormalization(X_validation, Y_validation, weights, biases, regularization_term)]\n",
    "\n",
    "    momentum_weights, momentum_biases = initialize_momentum(weights), initialize_momentum(biases)\n",
    "\n",
    "    for epoch in range(GDparams[2]):\n",
    "\n",
    "        for batch in range(1, int(X.shape[1] / GDparams[0])):\n",
    "\n",
    "            start = (batch - 1) * GDparams[0]\n",
    "            end = batch * GDparams[0]\n",
    "\n",
    "            p, bn_activations, bn_outputs, intermediate_scores, mean, var = ForwardPassBatchNormalization(X[:, start:end], weights, biases)\n",
    "            grad_LW, grad_Lb = BackwardPassBatchNormalization(X[:, start:end], Y[:, start:end], weights, biases, p, intermediate_scores, bn_activations, intermediate_scores, mean, var, regularization_term)\n",
    "\n",
    "            if (epoch == 0) and (start == 0):\n",
    "\n",
    "                exponentials = [mean, var]\n",
    "\n",
    "                for i in range(len(weights)):\n",
    "                    weights[i] -= momentum_weights[i]\n",
    "                    biases[i] -= momentum_biases[i]\n",
    "\n",
    "            else:\n",
    "\n",
    "                exponentials = ExponentialMovingAverage(mean, exponentials[0], var, exponentials[1])\n",
    "\n",
    "                weights, biases, momentum_weights, momentum_biases = add_momentum(weights, grad_LW, momentum_weights, biases, grad_Lb, momentum_biases, GDparams[1])\n",
    "\n",
    "        train_loss.append(ComputeCostBatchNormalization(X, Y, weights, biases, regularization_term))\n",
    "        if train_loss[-1] > 3 * train_loss[0]:\n",
    "            print(f'Train loss at epoch no.{epoch} loss exceeded by 3 times or more the original train loss. Exiting training process...')\n",
    "            break\n",
    "        validation_loss.append(ComputeCostBatchNormalization(X_validation, Y_validation, weights, biases, regularization_term))\n",
    "\n",
    "        GDparams[1] *= 0.95\n",
    "\n",
    "    return weights, biases, train_loss, validation_loss, exponentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training, validation and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets():\n",
    "    \"\"\"\n",
    "    Creates the full dataset, containing all the available data for training except 1000 images\n",
    "    used for the validation set.\n",
    "\n",
    "    :return: Training, validation and test sets (features, ground-truth labels, and their one-hot representation\n",
    "    \"\"\"\n",
    "\n",
    "    X_training_1, Y_training_1, y_training_1 = LoadBatch('../../cifar-10-batches-py/data_batch_1')\n",
    "    X_training_2, Y_training_2, y_training_2 = LoadBatch('../../cifar-10-batches-py/data_batch_2')\n",
    "    X_training_3, Y_training_3, y_training_3 = LoadBatch('../../cifar-10-batches-py/data_batch_3')\n",
    "    X_training_4, Y_training_4, y_training_4 = LoadBatch('../../cifar-10-batches-py/data_batch_4')\n",
    "    X_training_5, Y_training_5, y_training_5 = LoadBatch('../../cifar-10-batches-py/data_batch_5')\n",
    "\n",
    "    X_training = np.concatenate((X_training_1, X_training_3), axis=1)\n",
    "    X_training = np.copy(np.concatenate((X_training, X_training_4), axis=1))\n",
    "    X_training = np.copy(np.concatenate((X_training, X_training_5), axis=1))\n",
    "\n",
    "    X_training = np.concatenate((X_training, X_training_2[:, :9000]), axis=1)\n",
    "\n",
    "    Y_training = np.concatenate((Y_training_1, Y_training_3), axis=1)\n",
    "    Y_training = np.copy(np.concatenate((Y_training, Y_training_4), axis=1))\n",
    "    Y_training = np.copy(np.concatenate((Y_training, Y_training_5), axis=1))\n",
    "\n",
    "    Y_training = np.concatenate((Y_training, Y_training_2[:, :9000]), axis=1)\n",
    "\n",
    "    y_training = y_training_1 + y_training_3 + y_training_4 + y_training_5 + y_training_2[:9000]\n",
    "\n",
    "    X_validation = np.copy(X_training_2[:, 9000:])\n",
    "    Y_validation = np.copy(Y_training_2[:, 9000:])\n",
    "    y_validation = y_training_2[9000:]\n",
    "\n",
    "    X_test, _, y_test = LoadBatch('../../cifar-10-batches-py/test_batch')\n",
    "\n",
    "    mean = np.mean(X_training)\n",
    "    X_training -= mean\n",
    "    X_validation -= mean\n",
    "    X_test -= mean\n",
    "\n",
    "    return [X_training, Y_training, y_training], [X_validation, Y_validation, y_validation], [X_test, y_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1: Upgrade your code from assignment 2 so that you can train a k-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with numerically computed gradients of a 2-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [10, 50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_1, Y_training_1, y_training_1 = LoadBatch('../../cifar-10-batches-py/data_batch_1')\n",
    "X_training_2, Y_training_2, y_training_2 = LoadBatch('../../cifar-10-batches-py/data_batch_2')\n",
    "X_test, _, y_test = LoadBatch('../../cifar-10-batches-py/test_batch')\n",
    "\n",
    "mean = np.mean(X_training_1)\n",
    "X_training_1 -= mean\n",
    "X_training_2 -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already available the numerical gradients for 2 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_num = np.load('grad_W1_num.npy')\n",
    "W2_num = np.load('grad_W2_num.npy')\n",
    "\n",
    "b1_num = np.load('grad_b1_num.npy')\n",
    "b2_num = np.load('grad_b2_num.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_weights = [W1_num, W2_num]\n",
    "num_biases = [b1_num, b2_num]\n",
    "\n",
    "p, activations, outputs = EvaluateClassifier(X_training_1[:, 0:2], weights, biases)\n",
    "grad_weights, grad_biases = ComputeGradients(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases, p, outputs, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 7.52283628908819e-09\n",
      "Deviation on bias vector: 4.602426291547493e-09\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 3.0747216543846533e-10\n",
      "Deviation on bias vector: 1.1936382115255306e-11\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_weights, grad_biases, num_weights, num_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviation of at most $10^{-9}$, all good to this point!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with computing gradients for a 3-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [20, 50], [10, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 99.05it/s]\n",
      "100%|██████████| 50/50 [24:35<00:00, 29.50s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 112.14it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 111.00it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.53it/s]\n"
     ]
    }
   ],
   "source": [
    "grad_weights_3_num, grad_bias_3_num = ComputeGradsNumSlow(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_index in range(len(grad_weights_3_num)):\n",
    "    \n",
    "    np.save(f'3_layers_num_weights{weight_index}', grad_weights_3_num[weight_index])\n",
    "    np.save(f'3_layers_num_bias{weight_index}', grad_bias_3_num[weight_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, activations, outputs = EvaluateClassifier(X_training_1[:, 0:2], weights, biases)\n",
    "grad_weights, grad_biases = ComputeGradients(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases, p, outputs, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 2.1237177447134792e-06\n",
      "Deviation on bias vector: 1.047827661340416e-06\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 1.197615459525251e-07\n",
      "Deviation on bias vector: 0.0032312506974258464\n",
      "-----------------\n",
      "Layer no. 3:\n",
      "Deviation on weight matrix: 7.368229647407527e-08\n",
      "Deviation on bias vector: 1.677023298915395e-11\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_weights, grad_biases, grad_weights_3_num, grad_bias_3_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good! Let's check 4 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [20, 50], [15, 20], [10,15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form the correctly shaped matrices to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 97.39it/s]\n",
      "100%|██████████| 50/50 [25:01<00:00, 30.02s/it]\n",
      "100%|██████████| 20/20 [00:00<00:00, 101.33it/s]\n",
      "100%|██████████| 20/20 [00:08<00:00,  2.31it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 111.27it/s]\n",
      "100%|██████████| 15/15 [00:02<00:00,  5.65it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 110.81it/s]\n",
      "100%|██████████| 10/10 [00:01<00:00,  7.57it/s]\n"
     ]
    }
   ],
   "source": [
    "grad_weights_4_num, grad_bias_4_num = ComputeGradsNumSlow(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_index in range(len(grad_weights_4_num)):\n",
    "    \n",
    "    np.save(f'4_layers_num_weights{weight_index}', grad_weights_4_num[weight_index])\n",
    "    np.save(f'4_layers_num_bias{weight_index}', grad_bias_4_num[weight_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, activations, outputs = EvaluateClassifier(X_training_1[:, 0:2], weights, biases)\n",
    "grad_weights, grad_biases = ComputeGradients(X_training_1[:, 0:2], Y_training_1[:, 0:2], weights, biases, p, outputs, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 0.0004455622900617491\n",
      "Deviation on bias vector: 0.00021413577255454145\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 2.8193570737353638e-05\n",
      "Deviation on bias vector: 0.014347056990204365\n",
      "-----------------\n",
      "Layer no. 3:\n",
      "Deviation on weight matrix: 8.838559097221378e-06\n",
      "Deviation on bias vector: 0.17027488699016166\n",
      "-----------------\n",
      "Layer no. 4:\n",
      "Deviation on weight matrix: 2.0539890893159423e-05\n",
      "Deviation on bias vector: 9.891207642802771e-12\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_weights, grad_biases, grad_weights_4_num, grad_bias_4_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2: Can I train a 3-layer network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test that you are able to replicate the results of a 2-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [10, 50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to replicate the performance of 44.44% for 10 epochs of training and $(\\eta, \\lambda) = (0.01713848118474131, 0.0001)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:03<00:00,  6.33s/it]\n"
     ]
    }
   ],
   "source": [
    "GD_params = [100, 0.0171384811847413, 10]\n",
    "\n",
    "weights, biases, training_cost, validation_cost =  MiniBatchGDwithMomentum(  X_training_1,\n",
    "                                                                             Y_training_1,\n",
    "                                                                             X_training_2,\n",
    "                                                                             Y_training_2,\n",
    "                                                                             y_training_2,\n",
    "                                                                             GD_params,\n",
    "                                                                             weights, biases,\n",
    "                                                                             regularization_term=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_accuracy = ComputeAccuracy(X_training_2, y_training_2, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.16"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the randomness of the initialization on the weights, the deviation is well justified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens after a few epochs? Are you learning anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:24<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3022672535670137\n",
      "Cost at training epoch 2 is 2.3022666352616192\n",
      "Cost at training epoch 3 is 2.302262164108896\n",
      "Cost at training epoch 4 is 2.302256330131588\n",
      "Cost at training epoch 5 is 2.3022509063293954\n",
      "Cost at training epoch 6 is 2.302246087045059\n",
      "Cost at training epoch 7 is 2.30224183447161\n",
      "Cost at training epoch 8 is 2.3022380835454994\n",
      "Cost at training epoch 9 is 2.3022347717577407\n",
      "Cost at training epoch 10 is 2.3022318440902114\n",
      "Cost at training epoch 11 is 2.3022292524125856\n",
      "Cost at training epoch 12 is 2.3022269539491447\n",
      "Cost at training epoch 13 is 2.3022249115918183\n",
      "Cost at training epoch 14 is 2.302223092730126\n",
      "Cost at training epoch 15 is 2.3022214685589133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [30, 50], [10,30]])\n",
    "\n",
    "GD_params = [100, 0.0171384811847413, 15]\n",
    "\n",
    "weights, biases, training_cost, validation_cost = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights, biases,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n",
    "for i in range(len(training_cost)):\n",
    "\n",
    "    print(f'Cost at training epoch {i+1} is {training_cost[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss after at each one of the 15 epochs of training is almost stable at 2.3! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if you play around with the learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Eta:  1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.30260046125\n",
      "Cost at training epoch 2 is 2.3026003916138555\n",
      "Cost at training epoch 3 is 2.302600325471149\n",
      "Cost at training epoch 4 is 2.3026002626463438\n",
      "Cost at training epoch 5 is 2.3026002029724792\n",
      "Cost at training epoch 6 is 2.3026001462910486\n",
      "Cost at training epoch 7 is 2.3026000924515975\n",
      "Cost at training epoch 8 is 2.302600041311279\n",
      "Cost at training epoch 9 is 2.3025999927344216\n",
      "Cost at training epoch 10 is 2.3025999465922684\n",
      "------------------\n",
      "Eta:  1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3025999005718703\n",
      "Cost at training epoch 2 is 2.302599205749114\n",
      "Cost at training epoch 3 is 2.3025985468529453\n",
      "Cost at training epoch 4 is 2.3025979219724637\n",
      "Cost at training epoch 5 is 2.302597329300965\n",
      "Cost at training epoch 6 is 2.302596767131986\n",
      "Cost at training epoch 7 is 2.3025962338540733\n",
      "Cost at training epoch 8 is 2.302595727947948\n",
      "Cost at training epoch 9 is 2.3025952479739393\n",
      "Cost at training epoch 10 is 2.3025947925707957\n",
      "------------------\n",
      "Eta:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3025943342251414\n",
      "Cost at training epoch 2 is 2.30258753778816\n",
      "Cost at training epoch 3 is 2.3025811965366985\n",
      "Cost at training epoch 4 is 2.302575274681294\n",
      "Cost at training epoch 5 is 2.3025697397215943\n",
      "Cost at training epoch 6 is 2.3025645621776087\n",
      "Cost at training epoch 7 is 2.3025597151526602\n",
      "Cost at training epoch 8 is 2.302555174170237\n",
      "Cost at training epoch 9 is 2.302550916949251\n",
      "Cost at training epoch 10 is 2.302546923101441\n",
      "------------------\n",
      "Eta:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.302542530097267\n",
      "Cost at training epoch 2 is 2.3024880377323287\n",
      "Cost at training epoch 3 is 2.302444833211863\n",
      "Cost at training epoch 4 is 2.3024102404890057\n",
      "Cost at training epoch 5 is 2.302382282712888\n",
      "Cost at training epoch 6 is 2.3023594877124416\n",
      "Cost at training epoch 7 is 2.3023407475778948\n",
      "Cost at training epoch 8 is 2.302325219905004\n",
      "Cost at training epoch 9 is 2.3023122586043185\n",
      "Cost at training epoch 10 is 2.3023013637986485\n",
      "------------------\n",
      "Eta:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.3022832235898933\n",
      "Cost at training epoch 2 is 2.3022314934409445\n",
      "Cost at training epoch 3 is 2.3022246500421755\n",
      "Cost at training epoch 4 is 2.302222636213854\n",
      "Cost at training epoch 5 is 2.302221210438927\n",
      "Cost at training epoch 6 is 2.302219944464948\n",
      "Cost at training epoch 7 is 2.3022188047640544\n",
      "Cost at training epoch 8 is 2.3022177857331054\n",
      "Cost at training epoch 9 is 2.3022168786733173\n",
      "Cost at training epoch 10 is 2.3022160723738483\n",
      "------------------\n",
      "Eta:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.304083147255321\n",
      "Cost at training epoch 2 is 2.3040527762143457\n",
      "Cost at training epoch 3 is 2.3040247984649436\n",
      "Cost at training epoch 4 is 2.303963513926076\n",
      "Cost at training epoch 5 is 2.1974427096362485\n",
      "Cost at training epoch 6 is 2.007216352396664\n",
      "Cost at training epoch 7 is 1.8656019902160936\n",
      "Cost at training epoch 8 is 1.7714920839464587\n",
      "Cost at training epoch 9 is 1.6672864486126826\n",
      "Cost at training epoch 10 is 1.593195430942723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for eta in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.1]:\n",
    "\n",
    "    weights, biases = initialize_weights([[50, 3072], [30, 50], [10,30]])\n",
    "\n",
    "    print('------------------')\n",
    "    print('Eta: ', eta)\n",
    "    GD_params = [100, eta, 10]\n",
    "\n",
    "    weights, biases, training_cost, validation_cost = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                              Y_training_1,\n",
    "                                                                              X_training_2,\n",
    "                                                                              Y_training_2,\n",
    "                                                                              y_training_2,\n",
    "                                                                              GD_params,\n",
    "                                                                              weights, biases,\n",
    "                                                                              regularization_term=0.0001)\n",
    "\n",
    "    for i in range(len(training_cost)):\n",
    "        print(f'Cost at training epoch {i+1} is {training_cost[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the loss remains at the same levels no matter how many update steps take part in the training! Only with a fairly big learning rate of 0.1 we can see a small drop in the loss, but again a few epochs in the beginning where the loss remain stable can be observed! We can also suspect that with such a big learning rate, the learning might be unstable. The last experiment is conducted for more epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Eta:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:09<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at training epoch 1 is 2.304083147255321\n",
      "Cost at training epoch 2 is 2.3040527762143457\n",
      "Cost at training epoch 3 is 2.3040247984649436\n",
      "Cost at training epoch 4 is 2.303963513926076\n",
      "Cost at training epoch 5 is 2.1974427096362485\n",
      "Cost at training epoch 6 is 2.007216352396664\n",
      "Cost at training epoch 7 is 1.8656019902160936\n",
      "Cost at training epoch 8 is 1.7714920839464587\n",
      "Cost at training epoch 9 is 1.6672864486126826\n",
      "Cost at training epoch 10 is 1.593195430942723\n",
      "Cost at training epoch 11 is 1.5485123476073677\n",
      "Cost at training epoch 12 is 1.4758706564115107\n",
      "Cost at training epoch 13 is 1.4193924744826478\n",
      "Cost at training epoch 14 is 1.3805438066881317\n",
      "Cost at training epoch 15 is 1.368697249854167\n",
      "Cost at training epoch 16 is 1.3265087669239681\n",
      "Cost at training epoch 17 is 1.2723064324945854\n",
      "Cost at training epoch 18 is 1.2625014563672678\n",
      "Cost at training epoch 19 is 1.2666867686005\n",
      "Cost at training epoch 20 is 1.25163518706638\n",
      "Cost at training epoch 21 is 1.1950692780730536\n",
      "Cost at training epoch 22 is 1.1405550951346806\n",
      "Cost at training epoch 23 is 1.0933375948769273\n",
      "Cost at training epoch 24 is 1.0584354555304865\n",
      "Cost at training epoch 25 is 1.0211788369490433\n",
      "Cost at training epoch 26 is 1.0002588070688778\n",
      "Cost at training epoch 27 is 0.9777619847054716\n",
      "Cost at training epoch 28 is 0.9753313784903359\n",
      "Cost at training epoch 29 is 0.9558171620564835\n",
      "Cost at training epoch 30 is 0.9544156273418551\n",
      "Cost at training epoch 31 is 0.9515871093045444\n",
      "Cost at training epoch 32 is 0.93641149448918\n",
      "Cost at training epoch 33 is 0.9511512546042148\n",
      "Cost at training epoch 34 is 0.8900400762553463\n",
      "Cost at training epoch 35 is 0.891207534551131\n",
      "Cost at training epoch 36 is 0.840768908056229\n",
      "Cost at training epoch 37 is 0.8274775595253576\n",
      "Cost at training epoch 38 is 0.799688813021933\n",
      "Cost at training epoch 39 is 0.7950914142037012\n",
      "Cost at training epoch 40 is 0.7716458010015995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [30, 50], [10,30]])\n",
    "\n",
    "print('------------------')\n",
    "print('Eta: ', 0.1)\n",
    "GD_params = [100, 0.1, 40]\n",
    "\n",
    "weights, biases, training_cost, validation_cost = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights, biases,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n",
    "for i in range(len(training_cost)):\n",
    "    print(f'Cost at training epoch {i+1} is {training_cost[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_costs(training_cost, validation_cost, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting with this setting, and it occurs very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if you use He initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of training with He initialization at epoch 1 is 2.573371144365728\n",
      "Cost of training with He initialization at epoch 2 is 2.4388440346442506\n",
      "Cost of training with He initialization at epoch 3 is 2.3685662732480903\n",
      "Cost of training with He initialization at epoch 4 is 2.3142257340862384\n",
      "Cost of training with He initialization at epoch 5 is 2.272922477058916\n",
      "Cost of training with He initialization at epoch 6 is 2.236912218216597\n",
      "Cost of training with He initialization at epoch 7 is 2.2052890641621268\n",
      "Cost of training with He initialization at epoch 8 is 2.1762496203378463\n",
      "Cost of training with He initialization at epoch 9 is 2.1514607505444694\n",
      "Cost of training with He initialization at epoch 10 is 2.1297865298452456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights_he, biases_he = he_initialization_k_layers([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "GD_params = [100, 0.0171384811847413, 10]\n",
    "\n",
    "weights_he, biases_he, training_cost_he, validation_cost_he = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights_he, biases_he,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n",
    "for i in range(len(training_cost_he)):\n",
    "    print(f'Cost of training with He initialization at epoch {i+1} is {training_cost_he[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the loss does not remain stable, we don't observe much of a change. Again, we try for some more epochs and now plot the evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:06<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "weights_he, biases_he = he_initialization_k_layers([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "GD_params = [100, 0.0171384811847413, 40]\n",
    "\n",
    "weights_he, biases_he, training_cost_he, validation_cost_he = MiniBatchGDwithMomentum(X_training_1,\n",
    "                                                                          Y_training_1,\n",
    "                                                                          X_training_2,\n",
    "                                                                          Y_training_2,\n",
    "                                                                          y_training_2,\n",
    "                                                                          GD_params,\n",
    "                                                                          weights_he, biases_he,\n",
    "                                                                          regularization_term=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_costs(training_cost_he, validation_cost_he, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoother than earlier tries, but still overfitting after few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 3: Implement batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your analytical gradients computation in comparison with the numerical ones for 2 layers and 50 hidden nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare for 2 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 1.2597928505427309e-08\n",
      "Deviation on bias vector: 0.04489583333333333\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 1.061126355032502e-09\n",
      "Deviation on bias vector: 1.085592909420537e-09\n"
     ]
    }
   ],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [10, 50]])\n",
    "\n",
    "p, batch_normalization_activations, batch_normalization_outputs, intermediate_outputs, means, variances = ForwardPassBatchNormalization(X_training_1[:, :4], weights, biases)\n",
    "weights_2, biases_2 = BackwardPassBatchNormalization(X_training_1[:, :4], Y_training_1[:, :4], weights, biases, p, batch_normalization_outputs, batch_normalization_activations, intermediate_outputs, means, variances, regularization_term=0)\n",
    "\n",
    "two_layers = np.load('2_layers.npz')\n",
    "\n",
    "check_similarity(weights_2, biases_2, [two_layers['w0'], two_layers['w1'] ], [two_layers['b0'], two_layers['b1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare for 3 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      " 18%|█▊        | 9/50 [00:00<00:00, 89.05it/s]\u001b[A\n",
      " 38%|███▊      | 19/50 [00:00<00:00, 90.29it/s]\u001b[A\n",
      " 58%|█████▊    | 29/50 [00:00<00:00, 90.56it/s]\u001b[A\n",
      " 76%|███████▌  | 38/50 [00:00<00:00, 89.08it/s]\u001b[A\n",
      " 96%|█████████▌| 48/50 [00:00<00:00, 90.29it/s]\u001b[A\n",
      "100%|██████████| 50/50 [00:00<00:00, 90.52it/s]\u001b[A\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/50 [00:29<23:42, 29.03s/it]\u001b[A\n",
      "  4%|▍         | 2/50 [00:57<23:05, 28.86s/it]\u001b[A\n",
      "  6%|▌         | 3/50 [01:26<22:30, 28.73s/it]\u001b[A\n",
      "  8%|▊         | 4/50 [01:54<21:57, 28.65s/it]\u001b[A\n",
      " 10%|█         | 5/50 [02:22<21:26, 28.59s/it]\u001b[A\n",
      " 12%|█▏        | 6/50 [02:51<20:56, 28.57s/it]\u001b[A\n",
      " 14%|█▍        | 7/50 [03:19<20:26, 28.53s/it]\u001b[A\n",
      " 16%|█▌        | 8/50 [03:48<20:00, 28.58s/it]\u001b[A\n",
      " 18%|█▊        | 9/50 [04:18<19:39, 28.77s/it]\u001b[A\n",
      " 20%|██        | 10/50 [04:47<19:11, 28.79s/it]\u001b[A\n",
      " 22%|██▏       | 11/50 [05:16<18:42, 28.77s/it]\u001b[A\n",
      " 24%|██▍       | 12/50 [05:45<18:14, 28.80s/it]\u001b[A\n",
      " 26%|██▌       | 13/50 [06:14<17:45, 28.80s/it]\u001b[A\n",
      " 28%|██▊       | 14/50 [06:43<17:17, 28.82s/it]\u001b[A\n",
      " 30%|███       | 15/50 [07:12<16:48, 28.81s/it]\u001b[A\n",
      " 32%|███▏      | 16/50 [07:40<16:19, 28.80s/it]\u001b[A\n",
      " 34%|███▍      | 17/50 [08:09<15:50, 28.80s/it]\u001b[A\n",
      " 36%|███▌      | 18/50 [08:38<15:22, 28.83s/it]\u001b[A\n",
      " 38%|███▊      | 19/50 [09:05<14:49, 28.71s/it]\u001b[A\n",
      " 40%|████      | 20/50 [09:34<14:21, 28.71s/it]\u001b[A\n",
      " 42%|████▏     | 21/50 [10:01<13:50, 28.64s/it]\u001b[A\n",
      " 44%|████▍     | 22/50 [10:28<13:19, 28.57s/it]\u001b[A\n",
      " 46%|████▌     | 23/50 [10:54<12:48, 28.47s/it]\u001b[A\n",
      " 48%|████▊     | 24/50 [11:21<12:18, 28.39s/it]\u001b[A\n",
      " 50%|█████     | 25/50 [11:47<11:47, 28.30s/it]\u001b[A\n",
      " 52%|█████▏    | 26/50 [12:13<11:17, 28.22s/it]\u001b[A\n",
      " 54%|█████▍    | 27/50 [12:40<10:47, 28.16s/it]\u001b[A\n",
      " 56%|█████▌    | 28/50 [13:06<10:18, 28.10s/it]\u001b[A\n",
      " 58%|█████▊    | 29/50 [13:33<09:49, 28.05s/it]\u001b[A\n",
      " 60%|██████    | 30/50 [14:00<09:20, 28.03s/it]\u001b[A\n",
      " 62%|██████▏   | 31/50 [14:27<08:51, 27.98s/it]\u001b[A\n",
      " 64%|██████▍   | 32/50 [14:54<08:22, 27.94s/it]\u001b[A\n",
      " 66%|██████▌   | 33/50 [15:20<07:54, 27.90s/it]\u001b[A\n",
      " 68%|██████▊   | 34/50 [15:47<07:25, 27.85s/it]\u001b[A\n",
      " 70%|███████   | 35/50 [16:13<06:57, 27.81s/it]\u001b[A\n",
      " 72%|███████▏  | 36/50 [16:39<06:28, 27.76s/it]\u001b[A\n",
      " 74%|███████▍  | 37/50 [17:05<06:00, 27.72s/it]\u001b[A\n",
      " 76%|███████▌  | 38/50 [17:31<05:32, 27.68s/it]\u001b[A\n",
      " 78%|███████▊  | 39/50 [17:58<05:04, 27.64s/it]\u001b[A\n",
      " 80%|████████  | 40/50 [18:24<04:36, 27.61s/it]\u001b[A\n",
      " 82%|████████▏ | 41/50 [18:50<04:08, 27.58s/it]\u001b[A\n",
      " 84%|████████▍ | 42/50 [19:16<03:40, 27.55s/it]\u001b[A\n",
      " 86%|████████▌ | 43/50 [19:43<03:12, 27.52s/it]\u001b[A\n",
      " 88%|████████▊ | 44/50 [20:09<02:44, 27.49s/it]\u001b[A\n",
      " 90%|█████████ | 45/50 [20:35<02:17, 27.46s/it]\u001b[A\n",
      " 92%|█████████▏| 46/50 [21:01<01:49, 27.43s/it]\u001b[A\n",
      " 94%|█████████▍| 47/50 [21:27<01:22, 27.40s/it]\u001b[A\n",
      " 96%|█████████▌| 48/50 [21:55<00:54, 27.40s/it]\u001b[A\n",
      " 98%|█████████▊| 49/50 [22:22<00:27, 27.39s/it]\u001b[A\n",
      "100%|██████████| 50/50 [22:48<00:00, 27.37s/it]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      " 37%|███▋      | 11/30 [00:00<00:00, 106.77it/s]\u001b[A\n",
      " 77%|███████▋  | 23/30 [00:00<00:00, 112.16it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:00<00:00, 111.57it/s]\u001b[A\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/30 [00:00<00:12,  2.38it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [00:00<00:11,  2.38it/s]\u001b[A\n",
      " 10%|█         | 3/30 [00:01<00:11,  2.37it/s]\u001b[A\n",
      " 13%|█▎        | 4/30 [00:01<00:10,  2.37it/s]\u001b[A\n",
      " 17%|█▋        | 5/30 [00:02<00:10,  2.38it/s]\u001b[A\n",
      " 20%|██        | 6/30 [00:02<00:10,  2.37it/s]\u001b[A\n",
      " 23%|██▎       | 7/30 [00:02<00:09,  2.36it/s]\u001b[A\n",
      " 27%|██▋       | 8/30 [00:03<00:09,  2.36it/s]\u001b[A\n",
      " 30%|███       | 9/30 [00:03<00:08,  2.36it/s]\u001b[A\n",
      " 33%|███▎      | 10/30 [00:04<00:08,  2.37it/s]\u001b[A\n",
      " 37%|███▋      | 11/30 [00:04<00:08,  2.37it/s]\u001b[A\n",
      " 40%|████      | 12/30 [00:05<00:07,  2.37it/s]\u001b[A\n",
      " 43%|████▎     | 13/30 [00:05<00:07,  2.37it/s]\u001b[A\n",
      " 47%|████▋     | 14/30 [00:05<00:06,  2.36it/s]\u001b[A\n",
      " 50%|█████     | 15/30 [00:06<00:06,  2.37it/s]\u001b[A\n",
      " 53%|█████▎    | 16/30 [00:06<00:05,  2.37it/s]\u001b[A\n",
      " 57%|█████▋    | 17/30 [00:07<00:05,  2.36it/s]\u001b[A\n",
      " 60%|██████    | 18/30 [00:07<00:05,  2.36it/s]\u001b[A\n",
      " 63%|██████▎   | 19/30 [00:08<00:04,  2.36it/s]\u001b[A\n",
      " 67%|██████▋   | 20/30 [00:08<00:04,  2.36it/s]\u001b[A\n",
      " 70%|███████   | 21/30 [00:08<00:03,  2.36it/s]\u001b[A\n",
      " 73%|███████▎  | 22/30 [00:09<00:03,  2.36it/s]\u001b[A\n",
      " 77%|███████▋  | 23/30 [00:09<00:02,  2.36it/s]\u001b[A\n",
      " 80%|████████  | 24/30 [00:10<00:02,  2.36it/s]\u001b[A\n",
      " 83%|████████▎ | 25/30 [00:10<00:02,  2.36it/s]\u001b[A\n",
      " 87%|████████▋ | 26/30 [00:11<00:01,  2.36it/s]\u001b[A\n",
      " 90%|█████████ | 27/30 [00:11<00:01,  2.36it/s]\u001b[A\n",
      " 93%|█████████▎| 28/30 [00:11<00:00,  2.36it/s]\u001b[A\n",
      " 97%|█████████▋| 29/30 [00:12<00:00,  2.36it/s]\u001b[A\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:00<00:00, 108.97it/s]\u001b[A\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:02,  3.69it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:02,  3.69it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:00<00:01,  3.66it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:01<00:01,  3.71it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:01<00:01,  3.75it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:01<00:01,  3.78it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:01<00:00,  3.79it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:02<00:00,  3.80it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:02<00:00,  3.79it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.80it/s]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "w3_num, b3_num = ComputeGradsNumSlowBatchNorm(X_training_1[:, :2], Y_training_1[:, :2], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('3_layers_num', w0=w3_num[0], w1=w3_num[1], w2=w3_num[2], b0=b3_num[0], b1=b3_num[1], b2=b3_num[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "p, batch_normalization_activations, batch_normalization_outputs, intermediate_outputs, means, variances = ForwardPassBatchNormalization(X_training_1[:, :2], weights, biases)\n",
    "weights_3, biases_3= BackwardPassBatchNormalization(X_training_1[:, :2], Y_training_1[:, :2], weights, biases, p, batch_normalization_outputs, batch_normalization_activations, intermediate_outputs, means, variances, regularization_term=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 1.4574774508711658e-05\n",
      "Deviation on bias vector: 0.020078843569924645\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 0.0006714264687067433\n",
      "Deviation on bias vector: 0.1015625\n",
      "-----------------\n",
      "Layer no. 3:\n",
      "Deviation on weight matrix: 1.0708749294639795e-11\n",
      "Deviation on bias vector: 1.2260625257832792e-11\n"
     ]
    }
   ],
   "source": [
    "check_similarity(weights_3, biases_3, w3_num, b3_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare for 4 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Layer no. 1:\n",
      "Deviation on weight matrix: 2.894663197138072e-09\n",
      "Deviation on bias vector: 0.16003125\n",
      "-----------------\n",
      "Layer no. 2:\n",
      "Deviation on weight matrix: 4.848126914126361e-10\n",
      "Deviation on bias vector: 0.0738525390625\n",
      "-----------------\n",
      "Layer no. 3:\n",
      "Deviation on weight matrix: 4.899817892918272e-10\n",
      "Deviation on bias vector: 0.11653645833333333\n",
      "-----------------\n",
      "Layer no. 4:\n",
      "Deviation on weight matrix: 9.78062348082e-10\n",
      "Deviation on bias vector: 1.7724027351071386e-09\n"
     ]
    }
   ],
   "source": [
    "weights, biases = initialize_weights([[50, 3072], [20, 50], [15, 20], [10, 15]])\n",
    "\n",
    "p, batch_normalization_activations, batch_normalization_outputs, intermediate_outputs, means, variances = ForwardPassBatchNormalization(X_training_1[:, :4], weights, biases)\n",
    "weights_4, biases_4 = BackwardPassBatchNormalization(X_training_1[:, :4], Y_training_1[:, :4], weights, biases, p, batch_normalization_outputs, batch_normalization_activations, intermediate_outputs, means, variances, regularization_term=0)\n",
    "\n",
    "four_layers = np.load('4_layers.npz')\n",
    "\n",
    "check_similarity(weights_4, biases_4, [four_layers['w0'], four_layers['w1'], four_layers['w2'], four_layers['w3']], [four_layers['b0'], four_layers['b1'], four_layers['b2'], four_layers['b3']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfiiting in a small subset of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity():\n",
    "\n",
    "    weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "    GD_params = [100, 0.05, 200]\n",
    "\n",
    "    best_weights, best_biases, training_loss, validation_loss, exponentials = MiniBatchGDBatchNormalization(X_training_1[:, :1000], Y_training_1[:, :1000], X_training_2[:, :1000], Y_training_2[:, :1000], y_training_2[:1000], GD_params, weights, biases, regularization_term=1e-6)\n",
    "\n",
    "    visualize_costs(training_loss, validation_loss, display=True, title='Overfitting at a small dataset')\n",
    "    print(f'Training set accuracy: {ComputeAccuracyBatchNormalization(X_training_1[:, :100], y_training_1[:100], best_weights, best_biases)}')\n",
    "    print(f'Validation set accuracy: {ComputeAccuracyBatchNormalization(X_training_2[:, :100], y_training_2[:100], best_weights, best_biases)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 100.0\n",
      "Validation set accuracy: 34.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sanity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random search for good ranges of $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search():\n",
    "\n",
    "    for eta in [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]:\n",
    "        print('-----------------------')\n",
    "        print('eta: ', eta)\n",
    "\n",
    "        GD_params = [100, eta, 5]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization( X_training_1,\n",
    "                                                                                                 Y_training_1,\n",
    "                                                                                                 X_training_2,\n",
    "                                                                                                 Y_training_2,\n",
    "                                                                                                 y_training_2,\n",
    "                                                                                                 GD_params,\n",
    "                                                                                                 weights,\n",
    "                                                                                                 biases,\n",
    "                                                                                                 regularization_term=1e-6)\n",
    "\n",
    "        print()\n",
    "        for epoch, loss in enumerate(cost):\n",
    "            print(f'Cross-entropy loss at epoch no.{epoch+1}: {loss}')\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "        print(f'Validation set accuracy: {ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights, best_biases, exponentials)}%')\n",
    "\n",
    "    for eta in np.arange(0.15, 1.05, 0.05):\n",
    "\n",
    "        print('-----------------------')\n",
    "        print('eta: ', eta)\n",
    "\n",
    "        GD_params = [100, eta, 5]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization(    X_training_1,\n",
    "                                                                                                    Y_training_1,\n",
    "                                                                                                    X_training_2,\n",
    "                                                                                                    Y_training_2,\n",
    "                                                                                                    y_training_2,\n",
    "                                                                                                    GD_params,\n",
    "                                                                                                    weights,\n",
    "                                                                                                    biases,\n",
    "                                                                                                    regularization_term=0.000001)\n",
    "        print()\n",
    "        for epoch, loss in enumerate(cost):\n",
    "            print(f'Cross-entropy loss at epoch no.{epoch+1}: {loss}')\n",
    "        print()\n",
    "\n",
    "        print(f'Validation set accuracy: {ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights, best_biases, exponentials)}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "eta:  1e-05\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3024004969856926\n",
      "Cross-entropy loss at epoch no.2: 2.30141070312675\n",
      "Cross-entropy loss at epoch no.3: 2.3003529146639976\n",
      "Cross-entropy loss at epoch no.4: 2.299162285333397\n",
      "Cross-entropy loss at epoch no.5: 2.2978090525437764\n",
      "Cross-entropy loss at epoch no.6: 2.2963016696759535\n",
      "\n",
      "Validation set accuracy: 24.55%\n",
      "-----------------------\n",
      "eta:  5e-05\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3023449026664626\n",
      "Cross-entropy loss at epoch no.2: 2.2962484257373554\n",
      "Cross-entropy loss at epoch no.3: 2.2837450514713153\n",
      "Cross-entropy loss at epoch no.4: 2.267923037061595\n",
      "Cross-entropy loss at epoch no.5: 2.2515390217750118\n",
      "Cross-entropy loss at epoch no.6: 2.2351971685492837\n",
      "\n",
      "Validation set accuracy: 26.49%\n",
      "-----------------------\n",
      "eta:  0.0001\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3024923089332536\n",
      "Cross-entropy loss at epoch no.2: 2.287360237749634\n",
      "Cross-entropy loss at epoch no.3: 2.2534343154472865\n",
      "Cross-entropy loss at epoch no.4: 2.2180942054958597\n",
      "Cross-entropy loss at epoch no.5: 2.1836510214681075\n",
      "Cross-entropy loss at epoch no.6: 2.150214719435292\n",
      "\n",
      "Validation set accuracy: 31.01%\n",
      "-----------------------\n",
      "eta:  0.0005\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3024537479653575\n",
      "Cross-entropy loss at epoch no.2: 2.18148112488407\n",
      "Cross-entropy loss at epoch no.3: 2.0740170702723435\n",
      "Cross-entropy loss at epoch no.4: 1.9928453514007505\n",
      "Cross-entropy loss at epoch no.5: 1.9311525329029882\n",
      "Cross-entropy loss at epoch no.6: 1.88428988390866\n",
      "\n",
      "Validation set accuracy: 28.39%\n",
      "-----------------------\n",
      "eta:  0.001\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302553511106812\n",
      "Cross-entropy loss at epoch no.2: 2.098444407648277\n",
      "Cross-entropy loss at epoch no.3: 1.9488704079265569\n",
      "Cross-entropy loss at epoch no.4: 1.8487733167061755\n",
      "Cross-entropy loss at epoch no.5: 1.7778954271729677\n",
      "Cross-entropy loss at epoch no.6: 1.7135213980858373\n",
      "\n",
      "Validation set accuracy: 35.09%\n",
      "-----------------------\n",
      "eta:  0.005\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302712665909097\n",
      "Cross-entropy loss at epoch no.2: 1.9236974215776514\n",
      "Cross-entropy loss at epoch no.3: 1.7673588318422346\n",
      "Cross-entropy loss at epoch no.4: 1.647997655422567\n",
      "Cross-entropy loss at epoch no.5: 1.5640835798006774\n",
      "Cross-entropy loss at epoch no.6: 1.5055670842082851\n",
      "\n",
      "Validation set accuracy: 38.23%\n",
      "-----------------------\n",
      "eta:  0.01\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302338078426188\n",
      "Cross-entropy loss at epoch no.2: 1.781888012202992\n",
      "Cross-entropy loss at epoch no.3: 1.6282770680172884\n",
      "Cross-entropy loss at epoch no.4: 1.5304304535744546\n",
      "Cross-entropy loss at epoch no.5: 1.4731686750144093\n",
      "Cross-entropy loss at epoch no.6: 1.417296187858367\n",
      "\n",
      "Validation set accuracy: 39.83%\n",
      "-----------------------\n",
      "eta:  0.05\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302576564212157\n",
      "Cross-entropy loss at epoch no.2: 1.7088201074387817\n",
      "Cross-entropy loss at epoch no.3: 1.5773013537061449\n",
      "Cross-entropy loss at epoch no.4: 1.5012581996638794\n",
      "Cross-entropy loss at epoch no.5: 1.4367107668808252\n",
      "Cross-entropy loss at epoch no.6: 1.3803265798138757\n",
      "\n",
      "Validation set accuracy: 41.14%\n",
      "-----------------------\n",
      "eta:  0.1\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3027557020166367\n",
      "Cross-entropy loss at epoch no.2: 1.7378882934312532\n",
      "Cross-entropy loss at epoch no.3: 1.6175616440282257\n",
      "Cross-entropy loss at epoch no.4: 1.5188822989195039\n",
      "Cross-entropy loss at epoch no.5: 1.4579369002232232\n",
      "Cross-entropy loss at epoch no.6: 1.4013333331106381\n",
      "\n",
      "Validation set accuracy: 41.0%\n",
      "-----------------------\n",
      "eta:  0.15\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302605034937475\n",
      "Cross-entropy loss at epoch no.2: 1.7379354061187524\n",
      "Cross-entropy loss at epoch no.3: 1.5967540891129026\n",
      "Cross-entropy loss at epoch no.4: 1.512448190032505\n",
      "Cross-entropy loss at epoch no.5: 1.4616873136944541\n",
      "Cross-entropy loss at epoch no.6: 1.404839720434373\n",
      "\n",
      "Validation set accuracy: 40.43%\n",
      "-----------------------\n",
      "eta:  0.2\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302519516272677\n",
      "Cross-entropy loss at epoch no.2: 1.7567800991706388\n",
      "Cross-entropy loss at epoch no.3: 1.602425483967389\n",
      "Cross-entropy loss at epoch no.4: 1.500783370337282\n",
      "Cross-entropy loss at epoch no.5: 1.433957766530211\n",
      "Cross-entropy loss at epoch no.6: 1.3975971619105556\n",
      "\n",
      "Validation set accuracy: 39.42%\n",
      "-----------------------\n",
      "eta:  0.25\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3027161259448126\n",
      "Cross-entropy loss at epoch no.2: 1.738169947839216\n",
      "Cross-entropy loss at epoch no.3: 1.6108298324142574\n",
      "Cross-entropy loss at epoch no.4: 1.5397433905778042\n",
      "Cross-entropy loss at epoch no.5: 1.4718329399407717\n",
      "Cross-entropy loss at epoch no.6: 1.4357947598185952\n",
      "\n",
      "Validation set accuracy: 38.85%\n",
      "-----------------------\n",
      "eta:  0.30000000000000004\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3024307041194265\n",
      "Cross-entropy loss at epoch no.2: 1.7746764574226646\n",
      "Cross-entropy loss at epoch no.3: 1.6625182477905998\n",
      "Cross-entropy loss at epoch no.4: 1.6003346572027697\n",
      "Cross-entropy loss at epoch no.5: 1.5444468363508685\n",
      "Cross-entropy loss at epoch no.6: 1.5350960494127912\n",
      "\n",
      "Validation set accuracy: 36.05%\n",
      "-----------------------\n",
      "eta:  0.3500000000000001\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302495706648735\n",
      "Cross-entropy loss at epoch no.2: 1.8123992777187585\n",
      "Cross-entropy loss at epoch no.3: 1.685129211809253\n",
      "Cross-entropy loss at epoch no.4: 1.5903087449350048\n",
      "Cross-entropy loss at epoch no.5: 1.5501318711800778\n",
      "Cross-entropy loss at epoch no.6: 1.4763281098783858\n",
      "\n",
      "Validation set accuracy: 37.41%\n",
      "-----------------------\n",
      "eta:  0.40000000000000013\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302750678987683\n",
      "Cross-entropy loss at epoch no.2: 1.842786392491129\n",
      "Cross-entropy loss at epoch no.3: 1.7222150032718295\n",
      "Cross-entropy loss at epoch no.4: 1.6297700547809395\n",
      "Cross-entropy loss at epoch no.5: 1.5684014333104057\n",
      "Cross-entropy loss at epoch no.6: 1.5187063200215793\n",
      "\n",
      "Validation set accuracy: 37.53%\n",
      "-----------------------\n",
      "eta:  0.45000000000000007\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302427737526021\n",
      "Cross-entropy loss at epoch no.2: 1.7474067246993172\n",
      "Cross-entropy loss at epoch no.3: 1.677431725088296\n",
      "Cross-entropy loss at epoch no.4: 1.6347133578282291\n",
      "Cross-entropy loss at epoch no.5: 1.5887591686286422\n",
      "Cross-entropy loss at epoch no.6: 1.5749127232454967\n",
      "\n",
      "Validation set accuracy: 34.57%\n",
      "-----------------------\n",
      "eta:  0.5000000000000001\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302633305206161\n",
      "Cross-entropy loss at epoch no.2: 1.7898746905999745\n",
      "Cross-entropy loss at epoch no.3: 1.7175883499821831\n",
      "Cross-entropy loss at epoch no.4: 1.6567102794483701\n",
      "Cross-entropy loss at epoch no.5: 1.6134754127877968\n",
      "Cross-entropy loss at epoch no.6: 1.5926513208485409\n",
      "\n",
      "Validation set accuracy: 36.61%\n",
      "-----------------------\n",
      "eta:  0.5500000000000002\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.30262172228019\n",
      "Cross-entropy loss at epoch no.2: 1.808349205460883\n",
      "Cross-entropy loss at epoch no.3: 1.7302946233256493\n",
      "Cross-entropy loss at epoch no.4: 1.704672134157193\n",
      "Cross-entropy loss at epoch no.5: 1.6700920813232625\n",
      "Cross-entropy loss at epoch no.6: 1.60569917109046\n",
      "\n",
      "Validation set accuracy: 35.88%\n",
      "-----------------------\n",
      "eta:  0.6000000000000002\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302920487652482\n",
      "Cross-entropy loss at epoch no.2: 1.9674389994763177\n",
      "Cross-entropy loss at epoch no.3: 1.8023778766168335\n",
      "Cross-entropy loss at epoch no.4: 1.682547061465735\n",
      "Cross-entropy loss at epoch no.5: 1.6005156882847296\n",
      "Cross-entropy loss at epoch no.6: 1.5837989382636712\n",
      "\n",
      "Validation set accuracy: 35.16%\n",
      "-----------------------\n",
      "eta:  0.6500000000000002\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3023088651005996\n",
      "Cross-entropy loss at epoch no.2: 1.8736743288175264\n",
      "Cross-entropy loss at epoch no.3: 1.7584584201579259\n",
      "Cross-entropy loss at epoch no.4: 1.6550551702483849\n",
      "Cross-entropy loss at epoch no.5: 1.6201613978347689\n",
      "Cross-entropy loss at epoch no.6: 1.589558055817523\n",
      "\n",
      "Validation set accuracy: 35.71%\n",
      "-----------------------\n",
      "eta:  0.7000000000000002\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3022441524309727\n",
      "Cross-entropy loss at epoch no.2: 1.9159245460744347\n",
      "Cross-entropy loss at epoch no.3: 1.7889421171434534\n",
      "Cross-entropy loss at epoch no.4: 1.7324442839939995\n",
      "Cross-entropy loss at epoch no.5: 1.6937085859970114\n",
      "Cross-entropy loss at epoch no.6: 1.6863027959543824\n",
      "\n",
      "Validation set accuracy: 35.14%\n",
      "-----------------------\n",
      "eta:  0.7500000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302718603574153\n",
      "Cross-entropy loss at epoch no.2: 1.8421188610053985\n",
      "Cross-entropy loss at epoch no.3: 1.8730222913914507\n",
      "Cross-entropy loss at epoch no.4: 1.771837393063317\n",
      "Cross-entropy loss at epoch no.5: 1.7143272634204816\n",
      "Cross-entropy loss at epoch no.6: 1.6310248843778243\n",
      "\n",
      "Validation set accuracy: 35.7%\n",
      "-----------------------\n",
      "eta:  0.8000000000000003\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.30242225407954\n",
      "Cross-entropy loss at epoch no.2: 1.8992315621545701\n",
      "Cross-entropy loss at epoch no.3: 1.7852927313772806\n",
      "Cross-entropy loss at epoch no.4: 1.7586761820883994\n",
      "Cross-entropy loss at epoch no.5: 1.7088413294724207\n",
      "Cross-entropy loss at epoch no.6: 1.6704572146728551\n",
      "\n",
      "Validation set accuracy: 35.05%\n",
      "-----------------------\n",
      "eta:  0.8500000000000002\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3029103929135735\n",
      "Cross-entropy loss at epoch no.2: 1.8969333111551514\n",
      "Cross-entropy loss at epoch no.3: 1.8883063460307858\n",
      "Cross-entropy loss at epoch no.4: 1.8387276987596117\n",
      "Cross-entropy loss at epoch no.5: 1.8101466577828467\n",
      "Cross-entropy loss at epoch no.6: 1.796504833133501\n",
      "\n",
      "Validation set accuracy: 35.2%\n",
      "-----------------------\n",
      "eta:  0.9000000000000002\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3024698870147335\n",
      "Cross-entropy loss at epoch no.2: 2.113849466167961\n",
      "Cross-entropy loss at epoch no.3: 2.057418410716646\n",
      "Cross-entropy loss at epoch no.4: 2.0508420819501514\n",
      "Cross-entropy loss at epoch no.5: 2.0035628537849797\n",
      "Cross-entropy loss at epoch no.6: 1.9955156876934947\n",
      "\n",
      "Validation set accuracy: 31.28%\n",
      "-----------------------\n",
      "eta:  0.9500000000000003\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.3022066775447434\n",
      "Cross-entropy loss at epoch no.2: 1.9749869281824242\n",
      "Cross-entropy loss at epoch no.3: 1.874489787273532\n",
      "Cross-entropy loss at epoch no.4: 1.7859366332159703\n",
      "Cross-entropy loss at epoch no.5: 1.8001769827955854\n",
      "Cross-entropy loss at epoch no.6: 1.7672956493017289\n",
      "\n",
      "Validation set accuracy: 33.8%\n",
      "-----------------------\n",
      "eta:  1.0000000000000002\n",
      "\n",
      "Cross-entropy loss at epoch no.1: 2.302318661719054\n",
      "Cross-entropy loss at epoch no.2: 2.0268974891032916\n",
      "Cross-entropy loss at epoch no.3: 1.8435548843098952\n",
      "Cross-entropy loss at epoch no.4: 1.9239469251331243\n",
      "Cross-entropy loss at epoch no.5: 1.9209081824057785\n",
      "Cross-entropy loss at epoch no.6: 1.9493873760207268\n",
      "\n",
      "Validation set accuracy: 32.35%\n"
     ]
    }
   ],
   "source": [
    "random_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse-to-Fine search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing a random search on values for $\\eta$ with no regularization, the intervals between [0.01, 0.1] and [0.6, 1] were selected for a more thorough investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_search():\n",
    "    \"\"\"\n",
    "    First step of coarse search, where good values for eta derived from random_search are tested along\n",
    "    with many tries for the amount of regularization.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    accuracies = []\n",
    "    etas = []\n",
    "    lambdas = []\n",
    "\n",
    "    for regularization_term in [1e-6, 1e-5, 1e-4, 1e-3]:\n",
    "\n",
    "        e_min = np.log(0.005)\n",
    "        e_max = np.log(0.15)\n",
    "\n",
    "        for _ in range(50):\n",
    "\n",
    "            eta_term = np.random.rand(1, 1).flatten()[0]\n",
    "            e = e_min + (e_max - e_min) * eta_term\n",
    "            eta = np.exp(e)\n",
    "            etas.append(eta)\n",
    "\n",
    "            lambdas.append(regularization_term)\n",
    "\n",
    "            GD_params = [100, eta, 10]\n",
    "\n",
    "            weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "            best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization(\n",
    "                X_training_1,\n",
    "                Y_training_1,\n",
    "                X_training_2,\n",
    "                Y_training_2,\n",
    "                y_training_2,\n",
    "                GD_params,\n",
    "                weights,\n",
    "                biases,\n",
    "                regularization_term,\n",
    "                False)\n",
    "            \n",
    "            print('---------------------------------')\n",
    "            print('Learning rate: ' + str(eta) + ', amount of regularization term: ' + str(regularization_term))\n",
    "            accuracy_on_validation_set = ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights, best_biases, exponentials)\n",
    "            accuracies.append(accuracy_on_validation_set)\n",
    "            print('Accuracy performance on the validation set: ', accuracy_on_validation_set)\n",
    "\n",
    "    sort_them_all = sorted(zip(accuracies, etas, lambdas))\n",
    "\n",
    "    best_accuracies = [x for x, _, _ in sort_them_all]\n",
    "    best_etas = [y for _, y, _ in sort_them_all]\n",
    "    best_lambdas = [z for _, _, z in sort_them_all]\n",
    "\n",
    "    print('---------------------------------')\n",
    "    print('BEST PERFORMANCE: ', str(best_accuracies[-1]))\n",
    "    print('Best eta: ', best_etas[-1])\n",
    "    print('Best lambda: ', best_lambdas[-1])\n",
    "\n",
    "    print('---------------------------------')\n",
    "    print('SECOND BEST PERFORMANCE: ', str(best_accuracies[-2]))\n",
    "    print('Second best eta: ', best_etas[-2])\n",
    "    print('Second best lambda: ', best_lambdas[-2])\n",
    "\n",
    "    print('---------------------------------')\n",
    "    print('THIRD BEST PERFORMANCE: ', str(best_accuracies[-3]))\n",
    "    print('Third best eta: ', best_etas[-3])\n",
    "    print('Third best lambda: ', best_lambdas[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Learning rate: 0.04578466336957471, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.79\n",
      "---------------------------------\n",
      "Learning rate: 0.11920790455976901, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.45\n",
      "---------------------------------\n",
      "Learning rate: 0.031558683059269926, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.22\n",
      "---------------------------------\n",
      "Learning rate: 0.13843571173329083, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.33\n",
      "---------------------------------\n",
      "Learning rate: 0.05308802864211276, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.05\n",
      "---------------------------------\n",
      "Learning rate: 0.012518634645423575, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.78\n",
      "---------------------------------\n",
      "Learning rate: 0.011625103395214718, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.78\n",
      "---------------------------------\n",
      "Learning rate: 0.10288465796217207, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.12\n",
      "---------------------------------\n",
      "Learning rate: 0.010386107748862934, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.5\n",
      "---------------------------------\n",
      "Learning rate: 0.10746681721265434, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.06\n",
      "---------------------------------\n",
      "Learning rate: 0.01702396263496846, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.31\n",
      "---------------------------------\n",
      "Learning rate: 0.012229274377376866, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.01\n",
      "---------------------------------\n",
      "Learning rate: 0.07131953592219513, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.95\n",
      "---------------------------------\n",
      "Learning rate: 0.045414111987364725, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.26\n",
      "---------------------------------\n",
      "Learning rate: 0.09829915041929584, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.09\n",
      "---------------------------------\n",
      "Learning rate: 0.006742806498325037, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.66\n",
      "---------------------------------\n",
      "Learning rate: 0.012307648450423942, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.11\n",
      "---------------------------------\n",
      "Learning rate: 0.013016618397599905, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.77\n",
      "---------------------------------\n",
      "Learning rate: 0.11719784848775054, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.95\n",
      "---------------------------------\n",
      "Learning rate: 0.0201983921731667, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.3\n",
      "---------------------------------\n",
      "Learning rate: 0.08110698837773762, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.72\n",
      "---------------------------------\n",
      "Learning rate: 0.018791948840046896, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.38\n",
      "---------------------------------\n",
      "Learning rate: 0.07760842056478896, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.43\n",
      "---------------------------------\n",
      "Learning rate: 0.05757025356063453, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.47\n",
      "---------------------------------\n",
      "Learning rate: 0.04325933843246685, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.73\n",
      "---------------------------------\n",
      "Learning rate: 0.014833685115708026, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.72\n",
      "---------------------------------\n",
      "Learning rate: 0.04734281568894979, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.08\n",
      "---------------------------------\n",
      "Learning rate: 0.13811919747131823, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.46\n",
      "---------------------------------\n",
      "Learning rate: 0.05192821702979388, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.58\n",
      "---------------------------------\n",
      "Learning rate: 0.10572684988868448, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.06\n",
      "---------------------------------\n",
      "Learning rate: 0.03833853223489769, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.27\n",
      "---------------------------------\n",
      "Learning rate: 0.009066257788173517, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.75\n",
      "---------------------------------\n",
      "Learning rate: 0.04449645221375528, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.86\n",
      "---------------------------------\n",
      "Learning rate: 0.13523147489119855, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.05\n",
      "---------------------------------\n",
      "Learning rate: 0.09952167682326298, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.92\n",
      "---------------------------------\n",
      "Learning rate: 0.0059830880010558725, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.59\n",
      "---------------------------------\n",
      "Learning rate: 0.005076352876566289, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.19\n",
      "---------------------------------\n",
      "Learning rate: 0.13319908763922625, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.56\n",
      "---------------------------------\n",
      "Learning rate: 0.08286852852564823, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.99\n",
      "---------------------------------\n",
      "Learning rate: 0.05140871674450086, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.98\n",
      "---------------------------------\n",
      "Learning rate: 0.008742077346265377, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.3\n",
      "---------------------------------\n",
      "Learning rate: 0.02477805467629233, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.7\n",
      "---------------------------------\n",
      "Learning rate: 0.007544604210334126, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  39.76\n",
      "---------------------------------\n",
      "Learning rate: 0.058631735725105535, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.1\n",
      "---------------------------------\n",
      "Learning rate: 0.011748954013850343, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.74\n",
      "---------------------------------\n",
      "Learning rate: 0.020959272886574964, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.64\n",
      "---------------------------------\n",
      "Learning rate: 0.07624339252567566, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.4\n",
      "---------------------------------\n",
      "Learning rate: 0.04803740105655433, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.92\n",
      "---------------------------------\n",
      "Learning rate: 0.0052471544793727155, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.63\n",
      "---------------------------------\n",
      "Learning rate: 0.034523447573427825, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.21\n",
      "---------------------------------\n",
      "Learning rate: 0.008973954215889935, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.97\n",
      "---------------------------------\n",
      "Learning rate: 0.02619808778825631, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Learning rate: 0.10879332627717456, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.33\n",
      "---------------------------------\n",
      "Learning rate: 0.058387892860951536, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.16\n",
      "---------------------------------\n",
      "Learning rate: 0.01170735533235222, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.78\n",
      "---------------------------------\n",
      "Learning rate: 0.02566745907043904, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.55\n",
      "---------------------------------\n",
      "Learning rate: 0.02745729071403371, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.12\n",
      "---------------------------------\n",
      "Learning rate: 0.00755395671949037, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.71\n",
      "---------------------------------\n",
      "Learning rate: 0.005567723132864187, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.55\n",
      "---------------------------------\n",
      "Learning rate: 0.011075146001937375, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.28\n",
      "---------------------------------\n",
      "Learning rate: 0.019735589923390078, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.85\n",
      "---------------------------------\n",
      "Learning rate: 0.05998617441650027, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.48\n",
      "---------------------------------\n",
      "Learning rate: 0.07014069814088782, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.57\n",
      "---------------------------------\n",
      "Learning rate: 0.04879233254241313, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.61\n",
      "---------------------------------\n",
      "Learning rate: 0.1164035374275931, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.54\n",
      "---------------------------------\n",
      "Learning rate: 0.06927564335106597, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.36\n",
      "---------------------------------\n",
      "Learning rate: 0.04526958820084906, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.4\n",
      "---------------------------------\n",
      "Learning rate: 0.11727722350659371, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.77\n",
      "---------------------------------\n",
      "Learning rate: 0.05085010300951201, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  42.01\n",
      "---------------------------------\n",
      "Learning rate: 0.02643362336485303, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.69\n",
      "---------------------------------\n",
      "Learning rate: 0.05624764911757438, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  39.44\n",
      "---------------------------------\n",
      "Learning rate: 0.09147957982611303, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.1\n",
      "---------------------------------\n",
      "Learning rate: 0.08778478746527789, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  39.94\n",
      "---------------------------------\n",
      "Learning rate: 0.017952442438516108, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.1\n",
      "---------------------------------\n",
      "Learning rate: 0.06895493963321621, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.28\n",
      "---------------------------------\n",
      "Learning rate: 0.06430040839123341, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.85\n",
      "---------------------------------\n",
      "Learning rate: 0.06613627767811096, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.61\n",
      "---------------------------------\n",
      "Learning rate: 0.006123407425388391, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.21\n",
      "---------------------------------\n",
      "Learning rate: 0.07753432042729447, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  42.16\n",
      "---------------------------------\n",
      "Learning rate: 0.09518437824268237, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.02\n",
      "---------------------------------\n",
      "Learning rate: 0.023750465173888024, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.27\n",
      "---------------------------------\n",
      "Learning rate: 0.011095913961925071, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.16\n",
      "---------------------------------\n",
      "Learning rate: 0.03305037522755727, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.39\n",
      "---------------------------------\n",
      "Learning rate: 0.13963677352202597, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  39.73\n",
      "---------------------------------\n",
      "Learning rate: 0.14970045932957596, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  39.24\n",
      "---------------------------------\n",
      "Learning rate: 0.005355100264776308, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.7\n",
      "---------------------------------\n",
      "Learning rate: 0.11932265749406629, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.53\n",
      "---------------------------------\n",
      "Learning rate: 0.007256589782052057, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.58\n",
      "---------------------------------\n",
      "Learning rate: 0.0346511731635329, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.29\n",
      "---------------------------------\n",
      "Learning rate: 0.01883104246205048, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.4\n",
      "---------------------------------\n",
      "Learning rate: 0.009401230207523072, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.92\n",
      "---------------------------------\n",
      "Learning rate: 0.14823911500764855, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.12\n",
      "---------------------------------\n",
      "Learning rate: 0.1493746510410106, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.56\n",
      "---------------------------------\n",
      "Learning rate: 0.012520529491015652, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.58\n",
      "---------------------------------\n",
      "Learning rate: 0.07748753568183334, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.2\n",
      "---------------------------------\n",
      "Learning rate: 0.007773014094476239, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.25\n",
      "---------------------------------\n",
      "Learning rate: 0.10018337805232955, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.13\n",
      "---------------------------------\n",
      "Learning rate: 0.008010060323284673, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.68\n",
      "---------------------------------\n",
      "Learning rate: 0.005580657833953645, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.08\n",
      "---------------------------------\n",
      "Learning rate: 0.08078450806955613, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.03\n",
      "---------------------------------\n",
      "Learning rate: 0.05290871918530189, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.27\n",
      "---------------------------------\n",
      "Learning rate: 0.06616267837578531, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.99\n",
      "---------------------------------\n",
      "Learning rate: 0.10779098546140994, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  39.78\n",
      "---------------------------------\n",
      "Learning rate: 0.01150602853791146, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Learning rate: 0.05038170433374072, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.42\n",
      "---------------------------------\n",
      "Learning rate: 0.005078669888458132, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.71\n",
      "---------------------------------\n",
      "Learning rate: 0.016339866276654423, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.88\n",
      "---------------------------------\n",
      "Learning rate: 0.10027525930005092, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.67\n",
      "---------------------------------\n",
      "Learning rate: 0.09982040292425287, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  39.01\n",
      "---------------------------------\n",
      "Learning rate: 0.020193135410301696, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.95\n",
      "---------------------------------\n",
      "Learning rate: 0.015620195850487463, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.69\n",
      "---------------------------------\n",
      "Learning rate: 0.007830770810164351, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.02\n",
      "---------------------------------\n",
      "Learning rate: 0.006746601237978861, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.8\n",
      "---------------------------------\n",
      "Learning rate: 0.007614232163301885, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.16\n",
      "---------------------------------\n",
      "Learning rate: 0.005925336453517064, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.09\n",
      "---------------------------------\n",
      "Learning rate: 0.00714980717448613, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.89\n",
      "---------------------------------\n",
      "Learning rate: 0.06740203406246156, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.47\n",
      "---------------------------------\n",
      "Learning rate: 0.009379842336460782, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.73\n",
      "---------------------------------\n",
      "Learning rate: 0.14262650820768918, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.19\n",
      "---------------------------------\n",
      "Learning rate: 0.012758333647821824, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.63\n",
      "---------------------------------\n",
      "Learning rate: 0.007846579155040412, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.5\n",
      "---------------------------------\n",
      "Learning rate: 0.005375229165420262, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.22\n",
      "---------------------------------\n",
      "Learning rate: 0.12232932734596834, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.1\n",
      "---------------------------------\n",
      "Learning rate: 0.024888017226248664, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.52\n",
      "---------------------------------\n",
      "Learning rate: 0.017103756039242086, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.25\n",
      "---------------------------------\n",
      "Learning rate: 0.03482243438810072, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  39.87\n",
      "---------------------------------\n",
      "Learning rate: 0.035628347194210645, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  39.81\n",
      "---------------------------------\n",
      "Learning rate: 0.10332096316215308, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.02\n",
      "---------------------------------\n",
      "Learning rate: 0.007551993028199698, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.57\n",
      "---------------------------------\n",
      "Learning rate: 0.012313531423361912, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.41\n",
      "---------------------------------\n",
      "Learning rate: 0.0065024479733519274, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  42.22\n",
      "---------------------------------\n",
      "Learning rate: 0.056082446003286705, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  39.48\n",
      "---------------------------------\n",
      "Learning rate: 0.030555220591374974, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.91\n",
      "---------------------------------\n",
      "Learning rate: 0.008589824664586495, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  42.13\n",
      "---------------------------------\n",
      "Learning rate: 0.02335861331862238, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.24\n",
      "---------------------------------\n",
      "Learning rate: 0.10914017485245114, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.04\n",
      "---------------------------------\n",
      "Learning rate: 0.03518316322685768, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.5\n",
      "---------------------------------\n",
      "Learning rate: 0.006183877991878418, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.77\n",
      "---------------------------------\n",
      "Learning rate: 0.03121447485393369, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.75\n",
      "---------------------------------\n",
      "Learning rate: 0.005479111246716131, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.04\n",
      "---------------------------------\n",
      "Learning rate: 0.07792357527200804, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.65\n",
      "---------------------------------\n",
      "Learning rate: 0.03457533642036077, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.69\n",
      "---------------------------------\n",
      "Learning rate: 0.08026874326730203, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.34\n",
      "---------------------------------\n",
      "Learning rate: 0.008565424575705048, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.23\n",
      "---------------------------------\n",
      "Learning rate: 0.012853096356364347, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.12\n",
      "---------------------------------\n",
      "Learning rate: 0.037321133663423135, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.86\n",
      "---------------------------------\n",
      "Learning rate: 0.07660093722150772, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.96\n",
      "---------------------------------\n",
      "Learning rate: 0.06401605723682044, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.04\n",
      "---------------------------------\n",
      "Learning rate: 0.01615598078569927, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  39.69\n",
      "---------------------------------\n",
      "Learning rate: 0.03556802112049856, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  42.07\n",
      "---------------------------------\n",
      "Learning rate: 0.08171647581500474, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.15\n",
      "---------------------------------\n",
      "Learning rate: 0.05525363539032448, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.23\n",
      "---------------------------------\n",
      "Learning rate: 0.036459765909208235, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.49\n",
      "---------------------------------\n",
      "Learning rate: 0.039037693351486255, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.23\n",
      "---------------------------------\n",
      "Learning rate: 0.011283283200445634, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.1\n",
      "---------------------------------\n",
      "Learning rate: 0.0056495080500158605, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Learning rate: 0.1295029258729205, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.07\n",
      "---------------------------------\n",
      "Learning rate: 0.005697896537461265, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.92\n",
      "---------------------------------\n",
      "Learning rate: 0.0052412107050517414, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.08\n",
      "---------------------------------\n",
      "Learning rate: 0.016238179952563346, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  38.81\n",
      "---------------------------------\n",
      "Learning rate: 0.07945997562325698, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.95\n",
      "---------------------------------\n",
      "Learning rate: 0.05800803416280395, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.84\n",
      "---------------------------------\n",
      "Learning rate: 0.01025502756818525, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.36\n",
      "---------------------------------\n",
      "Learning rate: 0.010242494339792033, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.32\n",
      "---------------------------------\n",
      "Learning rate: 0.13142239605934786, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.21\n",
      "---------------------------------\n",
      "Learning rate: 0.027007875489597603, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.37\n",
      "---------------------------------\n",
      "Learning rate: 0.02130409704576346, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.51\n",
      "---------------------------------\n",
      "Learning rate: 0.014370349589350233, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.23\n",
      "---------------------------------\n",
      "Learning rate: 0.028468861272678034, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.6\n",
      "---------------------------------\n",
      "Learning rate: 0.0084370787336951, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.37\n",
      "---------------------------------\n",
      "Learning rate: 0.04720340089435045, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.6\n",
      "---------------------------------\n",
      "Learning rate: 0.010410054480851052, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.33\n",
      "---------------------------------\n",
      "Learning rate: 0.03605809163323723, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.58\n",
      "---------------------------------\n",
      "Learning rate: 0.0365414101899311, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.62\n",
      "---------------------------------\n",
      "Learning rate: 0.09260307909890753, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  37.97\n",
      "---------------------------------\n",
      "Learning rate: 0.005055542645689451, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.52\n",
      "---------------------------------\n",
      "Learning rate: 0.01513781617739773, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.05\n",
      "---------------------------------\n",
      "Learning rate: 0.040208956847494176, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.19\n",
      "---------------------------------\n",
      "Learning rate: 0.019854520870066856, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.16\n",
      "---------------------------------\n",
      "Learning rate: 0.0426627837693365, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.3\n",
      "---------------------------------\n",
      "Learning rate: 0.06203806320315104, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.33\n",
      "---------------------------------\n",
      "Learning rate: 0.02975762234652748, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.04\n",
      "---------------------------------\n",
      "Learning rate: 0.08492767152181034, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.61\n",
      "---------------------------------\n",
      "Learning rate: 0.01361105036247775, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.62\n",
      "---------------------------------\n",
      "Learning rate: 0.020957286263538318, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.2\n",
      "---------------------------------\n",
      "Learning rate: 0.007206664440179861, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.99\n",
      "---------------------------------\n",
      "Learning rate: 0.031241996539494, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.4\n",
      "---------------------------------\n",
      "Learning rate: 0.14885482580360726, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.04\n",
      "---------------------------------\n",
      "Learning rate: 0.02040652920271722, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.85\n",
      "---------------------------------\n",
      "Learning rate: 0.07815073407539704, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.04\n",
      "---------------------------------\n",
      "Learning rate: 0.040153435883639996, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.3\n",
      "---------------------------------\n",
      "Learning rate: 0.016452235249870113, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.08\n",
      "---------------------------------\n",
      "Learning rate: 0.029563195924339803, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.27\n",
      "---------------------------------\n",
      "Learning rate: 0.07660720504575663, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.59\n",
      "---------------------------------\n",
      "Learning rate: 0.04147171168630576, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.46\n",
      "---------------------------------\n",
      "Learning rate: 0.022599133495597678, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.66\n",
      "---------------------------------\n",
      "Learning rate: 0.016808035167520136, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.23\n",
      "---------------------------------\n",
      "Learning rate: 0.06249427689780213, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.87\n",
      "---------------------------------\n",
      "Learning rate: 0.06267724372545284, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.44\n",
      "---------------------------------\n",
      "Learning rate: 0.013312232093839084, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.37\n",
      "---------------------------------\n",
      "BEST PERFORMANCE:  42.22\n",
      "Best eta:  0.0065024479733519274\n",
      "Best lambda:  0.0001\n",
      "---------------------------------\n",
      "SECOND BEST PERFORMANCE:  42.16\n",
      "Second best eta:  0.07753432042729447\n",
      "Second best lambda:  1e-05\n",
      "---------------------------------\n",
      "THIRD BEST PERFORMANCE:  42.13\n",
      "Third best eta:  0.008589824664586495\n",
      "Third best lambda:  0.0001\n"
     ]
    }
   ],
   "source": [
    "coarse_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations that can be made for each regularization setting:\n",
    "\n",
    "i) For $\\lambda$ = $10^{-6}$ accuracy exceed 42% for $\\eta$ in [0.027, 0.06]  \n",
    "ii) For $\\lambda$ = $10^{-5}$ accuracy reaches high valued for $\\eta$'s close to 0.01 and 0.065, and no similar tests were ran, so we can test this direction too  \n",
    "iii) For $\\lambda$ = $10^{-4}$ the best space to search for good values is [0.01, 0.02]  \n",
    "iv) For a value of $\\lambda$ as high as $10^{-3}$ we can find good validation set accuracy performance in both close to 0.01 and 0.1 for $\\eta$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_search():\n",
    "    \"\"\"\n",
    "    Fine search based on the results of coarse search\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracies = []\n",
    "    etas = []\n",
    "    lambdas = []\n",
    "\n",
    "    # Lambda = 10^(-6)\n",
    "    regularization_term = 1e-6\n",
    "\n",
    "    e_min = np.log(0.027)\n",
    "    e_max = np.log(0.06)\n",
    "\n",
    "    for _ in range(20):\n",
    "        eta_term = np.random.rand(1, 1).flatten()[0]\n",
    "        e = e_min + (e_max - e_min) * eta_term\n",
    "        eta = np.exp(e)\n",
    "        etas.append(eta)\n",
    "\n",
    "        lambdas.append(regularization_term)\n",
    "\n",
    "        GD_params = [100, eta, 10]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization(\n",
    "            X_training_1,\n",
    "            Y_training_1,\n",
    "            X_training_2,\n",
    "            Y_training_2,\n",
    "            y_training_2,\n",
    "            GD_params,\n",
    "            weights,\n",
    "            biases,\n",
    "            regularization_term)\n",
    "\n",
    "        print('---------------------------------')\n",
    "        print('Learning rate: ' + str(eta) + ', amount of regularization term: ' + str(regularization_term))\n",
    "        accuracy_on_validation_set = ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights,\n",
    "                                                                       best_biases,\n",
    "                                                                       exponentials)\n",
    "        accuracies.append(accuracy_on_validation_set)\n",
    "        print('Accuracy performance on the validation set: ', accuracy_on_validation_set)\n",
    "\n",
    "    # Lambda = 10^(-5)\n",
    "    regularization_term = 1e-5\n",
    "\n",
    "    e_min = np.log(0.005)\n",
    "    e_max = np.log(0.015)\n",
    "\n",
    "    for _ in range(20):\n",
    "        eta_term = np.random.rand(1, 1).flatten()[0]\n",
    "        e = e_min + (e_max - e_min) * eta_term\n",
    "        eta = np.exp(e)\n",
    "        etas.append(eta)\n",
    "\n",
    "        lambdas.append(regularization_term)\n",
    "\n",
    "        GD_params = [100, eta, 10]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization(\n",
    "            X_training_1,\n",
    "            Y_training_1,\n",
    "            X_training_2,\n",
    "            Y_training_2,\n",
    "            y_training_2,\n",
    "            GD_params,\n",
    "            weights,\n",
    "            biases,\n",
    "            regularization_term)\n",
    "\n",
    "        print('---------------------------------')\n",
    "        print('Learning rate: ' + str(eta) + ', amount of regularization term: ' + str(regularization_term))\n",
    "        accuracy_on_validation_set = ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights,\n",
    "                                                                       best_biases,\n",
    "                                                                       exponentials)\n",
    "        accuracies.append(accuracy_on_validation_set)\n",
    "        print('Accuracy performance on the validation set: ', accuracy_on_validation_set)\n",
    "\n",
    "    e_min = np.log(0.06)\n",
    "    e_max = np.log(0.07)\n",
    "\n",
    "    for _ in range(20):\n",
    "        eta_term = np.random.rand(1, 1).flatten()[0]\n",
    "        e = e_min + (e_max - e_min) * eta_term\n",
    "        eta = np.exp(e)\n",
    "        etas.append(eta)\n",
    "\n",
    "        lambdas.append(regularization_term)\n",
    "\n",
    "        GD_params = [100, eta, 10]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization(\n",
    "            X_training_1,\n",
    "            Y_training_1,\n",
    "            X_training_2,\n",
    "            Y_training_2,\n",
    "            y_training_2,\n",
    "            GD_params,\n",
    "            weights,\n",
    "            biases,\n",
    "            regularization_term)\n",
    "\n",
    "        print('---------------------------------')\n",
    "        print('Learning rate: ' + str(eta) + ', amount of regularization term: ' + str(regularization_term))\n",
    "        accuracy_on_validation_set = ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights,\n",
    "                                                                       best_biases,\n",
    "                                                                       exponentials)\n",
    "        accuracies.append(accuracy_on_validation_set)\n",
    "        print('Accuracy performance on the validation set: ', accuracy_on_validation_set)\n",
    "\n",
    "    # Lambda = 10^(-4)\n",
    "    regularization_term = 1e-4\n",
    "\n",
    "    e_min = np.log(0.01)\n",
    "    e_max = np.log(0.02)\n",
    "\n",
    "    for _ in range(10):\n",
    "        eta_term = np.random.rand(1, 1).flatten()[0]\n",
    "        e = e_min + (e_max - e_min) * eta_term\n",
    "        eta = np.exp(e)\n",
    "        etas.append(eta)\n",
    "\n",
    "        lambdas.append(regularization_term)\n",
    "\n",
    "        GD_params = [100, eta, 10]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization(\n",
    "            X_training_1,\n",
    "            Y_training_1,\n",
    "            X_training_2,\n",
    "            Y_training_2,\n",
    "            y_training_2,\n",
    "            GD_params,\n",
    "            weights,\n",
    "            biases,\n",
    "            regularization_term)\n",
    "\n",
    "        print('---------------------------------')\n",
    "        print('Learning rate: ' + str(eta) + ', amount of regularization term: ' + str(regularization_term))\n",
    "        accuracy_on_validation_set = ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights,\n",
    "                                                                       best_biases,\n",
    "                                                                       exponentials)\n",
    "        accuracies.append(accuracy_on_validation_set)\n",
    "        print('Accuracy performance on the validation set: ', accuracy_on_validation_set)\n",
    "\n",
    "    # Lambda = 10^(-3)\n",
    "    regularization_term = 1e-3\n",
    "\n",
    "    e_min = np.log(0.005)\n",
    "    e_max = np.log(0.015)\n",
    "\n",
    "    for _ in range(10):\n",
    "        eta_term = np.random.rand(1, 1).flatten()[0]\n",
    "        e = e_min + (e_max - e_min) * eta_term\n",
    "        eta = np.exp(e)\n",
    "        etas.append(eta)\n",
    "\n",
    "        lambdas.append(regularization_term)\n",
    "\n",
    "        GD_params = [100, eta, 10]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials = MiniBatchGDBatchNormalization(\n",
    "            X_training_1,\n",
    "            Y_training_1,\n",
    "            X_training_2,\n",
    "            Y_training_2,\n",
    "            y_training_2,\n",
    "            GD_params,\n",
    "            weights,\n",
    "            biases,\n",
    "            regularization_term)\n",
    "\n",
    "        print('---------------------------------')\n",
    "        print('Learning rate: ' + str(eta) + ', amount of regularization term: ' + str(regularization_term))\n",
    "        accuracy_on_validation_set = ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights,\n",
    "                                                                       best_biases,\n",
    "                                                                       exponentials)\n",
    "        accuracies.append(accuracy_on_validation_set)\n",
    "        print('Accuracy performance on the validation set: ', accuracy_on_validation_set)\n",
    "\n",
    "    e_min = np.log(0.09)\n",
    "    e_max = np.log(0.11)\n",
    "\n",
    "    for _ in range(10):\n",
    "        eta_term = np.random.rand(1, 1).flatten()[0]\n",
    "        e = e_min + (e_max - e_min) * eta_term\n",
    "        eta = np.exp(e)\n",
    "        etas.append(eta)\n",
    "\n",
    "        lambdas.append(regularization_term)\n",
    "\n",
    "        GD_params = [100, eta, 10]\n",
    "\n",
    "        weights, biases = initialize_weights([[50, 3072], [30, 50], [10, 30]])\n",
    "\n",
    "        best_weights, best_biases, cost, val_cost, exponentials, exponential_variances = MiniBatchGDBatchNormalization(\n",
    "            X_training_1,\n",
    "            Y_training_1,\n",
    "            X_training_2,\n",
    "            Y_training_2,\n",
    "            y_training_2,\n",
    "            GD_params,\n",
    "            weights,\n",
    "            biases,\n",
    "            regularization_term)\n",
    "\n",
    "        print('---------------------------------')\n",
    "        print('Learning rate: ' + str(eta) + ', amount of regularization term: ' + str(regularization_term))\n",
    "        accuracy_on_validation_set = ComputeAccuracyBatchNormalization(X_training_2, y_training_2, best_weights,\n",
    "                                                                       best_biases,\n",
    "                                                                       exponentials)\n",
    "        accuracies.append(accuracy_on_validation_set)\n",
    "        print('Accuracy performance on the validation set: ', accuracy_on_validation_set)\n",
    "\n",
    "    sort_them_all = sorted(zip(accuracies, etas, lambdas))\n",
    "\n",
    "    best_accuracies = [x for x, _, _ in sort_them_all]\n",
    "    best_etas = [y for _, y, _ in sort_them_all]\n",
    "    best_lambdas = [z for _, _, z in sort_them_all]\n",
    "\n",
    "    print('---------------------------------')\n",
    "    print('BEST PERFORMANCE: ', str(best_accuracies[-1]))\n",
    "    print('Best eta: ', best_etas[-1])\n",
    "    print('Best lambda: ', best_lambdas[-1])\n",
    "\n",
    "    print('---------------------------------')\n",
    "    print('SECOND BEST PERFORMANCE: ', str(best_accuracies[-2]))\n",
    "    print('Second best eta: ', best_etas[-2])\n",
    "    print('Second best lambda: ', best_lambdas[-2])\n",
    "\n",
    "    print('---------------------------------')\n",
    "    print('THIRD BEST PERFORMANCE: ', str(best_accuracies[-3]))\n",
    "    print('Third best eta: ', best_etas[-3])\n",
    "    print('Third best lambda: ', best_lambdas[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Learning rate: 0.02905109667305325, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.78\n",
      "---------------------------------\n",
      "Learning rate: 0.030459438029090218, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.23\n",
      "---------------------------------\n",
      "Learning rate: 0.05358893887759502, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.77\n",
      "---------------------------------\n",
      "Learning rate: 0.045296741753274035, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.04\n",
      "---------------------------------\n",
      "Learning rate: 0.035532596573735024, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.98\n",
      "---------------------------------\n",
      "Learning rate: 0.03666996262672731, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.11\n",
      "---------------------------------\n",
      "Learning rate: 0.035214091197526935, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.5\n",
      "---------------------------------\n",
      "Learning rate: 0.03257765067921279, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.3\n",
      "---------------------------------\n",
      "Learning rate: 0.03621662266384831, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.3\n",
      "---------------------------------\n",
      "Learning rate: 0.046791410754014974, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.51\n",
      "---------------------------------\n",
      "Learning rate: 0.03798554708278429, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.93\n",
      "---------------------------------\n",
      "Learning rate: 0.05371887136264754, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.98\n",
      "---------------------------------\n",
      "Learning rate: 0.0500555391084002, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.41\n",
      "---------------------------------\n",
      "Learning rate: 0.03539748962730703, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.73\n",
      "---------------------------------\n",
      "Learning rate: 0.04722170533365713, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.48\n",
      "---------------------------------\n",
      "Learning rate: 0.047508402353001064, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.9\n",
      "---------------------------------\n",
      "Learning rate: 0.03115182478668483, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.52\n",
      "---------------------------------\n",
      "Learning rate: 0.03833169049793067, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.13\n",
      "---------------------------------\n",
      "Learning rate: 0.035200119194530255, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  41.37\n",
      "---------------------------------\n",
      "Learning rate: 0.037922930770868016, amount of regularization term: 1e-06\n",
      "Accuracy performance on the validation set:  40.9\n",
      "---------------------------------\n",
      "Learning rate: 0.0053290494707075526, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.57\n",
      "---------------------------------\n",
      "Learning rate: 0.00502651961826622, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.53\n",
      "---------------------------------\n",
      "Learning rate: 0.013353636460377169, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.81\n",
      "---------------------------------\n",
      "Learning rate: 0.009882677981820187, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.6\n",
      "---------------------------------\n",
      "Learning rate: 0.007154956762931363, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.49\n",
      "---------------------------------\n",
      "Learning rate: 0.005481723138966167, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.82\n",
      "---------------------------------\n",
      "Learning rate: 0.01063618000318649, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.37\n",
      "---------------------------------\n",
      "Learning rate: 0.006830139977124169, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.45\n",
      "---------------------------------\n",
      "Learning rate: 0.012760217171381715, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.76\n",
      "---------------------------------\n",
      "Learning rate: 0.005497446470902127, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.14\n",
      "---------------------------------\n",
      "Learning rate: 0.009110242222696232, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.32\n",
      "---------------------------------\n",
      "Learning rate: 0.008340048396463979, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.06\n",
      "---------------------------------\n",
      "Learning rate: 0.01309291206731049, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.44\n",
      "---------------------------------\n",
      "Learning rate: 0.012296451719307391, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  42.12\n",
      "---------------------------------\n",
      "Learning rate: 0.00518596430283456, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.41\n",
      "---------------------------------\n",
      "Learning rate: 0.006644436158482235, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.42\n",
      "---------------------------------\n",
      "Learning rate: 0.009897997000568644, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.36\n",
      "---------------------------------\n",
      "Learning rate: 0.005298158646821741, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.57\n",
      "---------------------------------\n",
      "Learning rate: 0.009475629894980908, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.44\n",
      "---------------------------------\n",
      "Learning rate: 0.0071515864537435685, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.37\n",
      "---------------------------------\n",
      "Learning rate: 0.06342325717807851, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.16\n",
      "---------------------------------\n",
      "Learning rate: 0.0619126526392173, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.46\n",
      "---------------------------------\n",
      "Learning rate: 0.06908106885453122, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.48\n",
      "---------------------------------\n",
      "Learning rate: 0.06424089240357207, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.38\n",
      "---------------------------------\n",
      "Learning rate: 0.06210133102865957, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.94\n",
      "---------------------------------\n",
      "Learning rate: 0.06443068105371681, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.18\n",
      "---------------------------------\n",
      "Learning rate: 0.06043196460761161, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  39.46\n",
      "---------------------------------\n",
      "Learning rate: 0.0617583632216385, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  39.57\n",
      "---------------------------------\n",
      "Learning rate: 0.06537496324562872, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.65\n",
      "---------------------------------\n",
      "Learning rate: 0.06167755763303073, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.3\n",
      "---------------------------------\n",
      "Learning rate: 0.06480414407796874, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.65\n",
      "---------------------------------\n",
      "Learning rate: 0.06488337694881885, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Learning rate: 0.06575783995307094, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  41.14\n",
      "---------------------------------\n",
      "Learning rate: 0.06922001552108539, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.39\n",
      "---------------------------------\n",
      "Learning rate: 0.06271246993639171, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.22\n",
      "---------------------------------\n",
      "Learning rate: 0.06514258680934436, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.92\n",
      "---------------------------------\n",
      "Learning rate: 0.06837906428004828, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.82\n",
      "---------------------------------\n",
      "Learning rate: 0.06314887808555977, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.1\n",
      "---------------------------------\n",
      "Learning rate: 0.06270881320690751, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  39.38\n",
      "---------------------------------\n",
      "Learning rate: 0.06740922475664951, amount of regularization term: 1e-05\n",
      "Accuracy performance on the validation set:  40.58\n",
      "---------------------------------\n",
      "Learning rate: 0.013758750694416276, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.26\n",
      "---------------------------------\n",
      "Learning rate: 0.013569489016264742, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.47\n",
      "---------------------------------\n",
      "Learning rate: 0.017034282503044118, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.88\n",
      "---------------------------------\n",
      "Learning rate: 0.017083872247713322, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.96\n",
      "---------------------------------\n",
      "Learning rate: 0.01728551329854948, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  38.74\n",
      "---------------------------------\n",
      "Learning rate: 0.0100213291100385, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.91\n",
      "---------------------------------\n",
      "Learning rate: 0.0130411855450307, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.25\n",
      "---------------------------------\n",
      "Learning rate: 0.012577139495007961, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.81\n",
      "---------------------------------\n",
      "Learning rate: 0.012273141760187208, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  40.52\n",
      "---------------------------------\n",
      "Learning rate: 0.019463065669909563, amount of regularization term: 0.0001\n",
      "Accuracy performance on the validation set:  41.07\n",
      "---------------------------------\n",
      "Learning rate: 0.013395406201749277, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.77\n",
      "---------------------------------\n",
      "Learning rate: 0.006012247233446521, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  39.64\n",
      "---------------------------------\n",
      "Learning rate: 0.007043821106546183, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.42\n",
      "---------------------------------\n",
      "Learning rate: 0.006088047127016916, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.1\n",
      "---------------------------------\n",
      "Learning rate: 0.005371034059493141, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.16\n",
      "---------------------------------\n",
      "Learning rate: 0.006040982784183693, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.82\n",
      "---------------------------------\n",
      "Learning rate: 0.006129223899931321, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  41.48\n",
      "---------------------------------\n",
      "Learning rate: 0.006602056499168694, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.89\n",
      "---------------------------------\n",
      "Learning rate: 0.005950213904240823, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.41\n",
      "---------------------------------\n",
      "Learning rate: 0.0057283614515107485, amount of regularization term: 0.001\n",
      "Accuracy performance on the validation set:  40.4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-2799a94a687b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-3fb6797e7db3>\u001b[0m in \u001b[0;36mfine_search\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             regularization_term)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 5)"
     ]
    }
   ],
   "source": [
    "fine_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
