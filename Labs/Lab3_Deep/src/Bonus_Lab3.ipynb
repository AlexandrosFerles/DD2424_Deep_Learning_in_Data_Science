{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import everything you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical as make_class_categorical\n",
    "import _pickle as pickle\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ring a bell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inform_exit():\n",
    "    \n",
    "    os.system('say \"Training process is completed\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBatch(filename):\n",
    "    \"\"\"\n",
    "    Loads batch based on the given filename and produces the X, Y, and y arrays\n",
    "\n",
    "    :param filename: Path of the file\n",
    "    :return: X, Y and y arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # borrowed from https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    dictionary = unpickle(filename)\n",
    "\n",
    "    # borrowed from https://stackoverflow.com/questions/16977385/extract-the-nth-key-in-a-python-dictionary?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "    def ix(dic, n):  # don't use dict as  a variable name\n",
    "        try:\n",
    "            return list(dic)[n]  # or sorted(dic)[n] if you want the keys to be sorted\n",
    "        except IndexError:\n",
    "            print('not enough keys')\n",
    "\n",
    "    garbage = ix(dictionary, 1)\n",
    "    y = dictionary[garbage]\n",
    "    Y = np.transpose(make_class_categorical(y, 10))\n",
    "    garbage = ix(dictionary, 2)\n",
    "    X = np.transpose(dictionary[garbage]) / 255\n",
    "\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shapes_list, std=0.001):\n",
    "    \"\"\"\n",
    "    Initializes the weight and bias arrays for the 2 layers of the network\n",
    "\n",
    "    :param shapes_list: List that contains the shapes of the weight matrices of each layer. The number of layers can be found through\n",
    "                        estimating the length of this list.\n",
    "    :param variance (optional): The variance of the normal distribution that will be used for the initialization of the weights\n",
    "\n",
    "    :return: Weights and bias arrays for each layer of the network stored in lists\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(400)\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for shape in shapes_list:\n",
    "\n",
    "        W = np.random.normal(0, std, size=(shape[0], shape[1]))\n",
    "        b = np.zeros(shape=(shape[0], 1))\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization_k_layers(shapes_list):\n",
    "    \"\"\"\n",
    "    He initialization on the weight matrices.\n",
    "\n",
    "    :param shapes_list: List that contains the dimensions of each layer of the network.\n",
    "\n",
    "    :return: Initialized weight and bias matrices based on He initialization of the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for pair in shapes_list:\n",
    "\n",
    "        weights.append(np.random.randn(pair[0], pair[1]) * np.sqrt(2 / float(pair[0])))\n",
    "        biases.append(np.zeros(shape=(pair[0], 1)))\n",
    "\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function\n",
    "\n",
    "    :param x: Input to the function\n",
    "\n",
    "    :return: Output of ReLU(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    :param X: ND-Array. Probably should be floats.\n",
    "    :param theta: float parameter, used as a multiplier prior to exponentiation. Default = 1.0\n",
    "    :param axis (optional): axis to compute values along. Default is the first non-singleton axis.\n",
    "\n",
    "    :return: An array the same size as X. The result will sum to 1 along the specified axis\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClasses(p):\n",
    "    \"\"\"\n",
    "    Predicts classes based on the softmax output of the network\n",
    "\n",
    "    :param p: Softmax output of the network\n",
    "    :return: Predicted classes\n",
    "    \"\"\"\n",
    "\n",
    "    return np.argmax(p, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormalize(s, mean_s, var_s, epsilon=1e-20):\n",
    "    \"\"\"\n",
    "    Normalizes the scores of a batch based on their mean and variance.\n",
    "\n",
    "    :param s: Scores evaluated as output of a layer of the network.\n",
    "    :param mean_s: Mean of the scores.\n",
    "    :param var_s: Variance of the scores.\n",
    "    :param epsilon: A small number that is present to ensure that no division by zero will be performed.\n",
    "\n",
    "    :return: The normalized scores,\n",
    "    \"\"\"\n",
    "\n",
    "    diff = s - mean_s\n",
    "\n",
    "    return diff / (np.sqrt(var_s + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPassBatchNormalization(X, weights, biases, exponentials= None):\n",
    "    \"\"\"\n",
    "    Evaluates the forward pass result of the classifier network using batch normalization.\n",
    "\n",
    "    :param X: Input data.\n",
    "    :param weights: Weight arrays of the k-layer network.\n",
    "    :param biases: Bias vectors of the k-layer network.\n",
    "\n",
    "    :return: Softmax probabilities (predictions) of the true labels of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    s = np.dot(weights[0], X) + biases[0]\n",
    "\n",
    "    intermediate_outputs = [s]\n",
    "\n",
    "    if exponentials is not None:\n",
    "\n",
    "        exponential_means = exponentials[0]\n",
    "        exponential_variances = exponentials[1]\n",
    "\n",
    "        mean_s = exponential_means[0]\n",
    "        var_s = exponential_variances[0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        mean_s = s.mean(axis=1).reshape(s.shape[0], 1)\n",
    "        var_s = s.var(axis=1).reshape(s.shape[0], 1)\n",
    "\n",
    "        means = [mean_s]\n",
    "        variances = [var_s]\n",
    "\n",
    "    normalized_score = BatchNormalize(s, mean_s, var_s)\n",
    "\n",
    "    batch_normalization_outputs = [normalized_score]\n",
    "    batch_normalization_activations = [ReLU(normalized_score)]\n",
    "\n",
    "    for index in range(1, len(weights) - 1):\n",
    "\n",
    "        s = np.dot(weights[index], batch_normalization_activations[-1]) + biases[index]\n",
    "\n",
    "        intermediate_outputs.append(s)\n",
    "\n",
    "        if exponentials is None:\n",
    "            mean_s = s.mean(axis=1).reshape(s.shape[0], 1)\n",
    "            var_s = s.var(axis=1).reshape(s.shape[0], 1)\n",
    "\n",
    "            means.append(mean_s)\n",
    "            variances.append(var_s)\n",
    "\n",
    "        else:\n",
    "\n",
    "            mean_s = exponential_means[index]\n",
    "            var_s = exponential_variances[index]\n",
    "\n",
    "        normalized_score = BatchNormalize(s, mean_s, var_s)\n",
    "\n",
    "        batch_normalization_outputs.append(normalized_score)\n",
    "        batch_normalization_activations.append(ReLU(normalized_score))\n",
    "\n",
    "    s = np.dot(weights[-1], batch_normalization_activations[-1]) + biases[-1]\n",
    "\n",
    "    p = softmax(s, axis=0)\n",
    "\n",
    "    if exponentials is not None:\n",
    "        return p\n",
    "    else:\n",
    "        return p, batch_normalization_activations, batch_normalization_outputs, intermediate_outputs, means, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracyBatchNormalization(X, y, weights, biases, exponentials = None):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the feed-forward k-layer network\n",
    "\n",
    "    :param X: Input data\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param exponentials: Contains the exponential means and variances computed, they are used in call after training.\n",
    "\n",
    "    :return: Accuracy performance of the neural network.\n",
    "    \"\"\"\n",
    "    if exponentials is not None:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)\n",
    "    else:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)[0]\n",
    "    predictions = predictClasses(p)\n",
    "\n",
    "    accuracy = round(np.sum(np.where(predictions - y == 0, 1, 0)) * 100 / len(y), 2)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCostBatchNormalization(X, Y, weights, biases, regularization_term, exponentials=None):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss on a batch of data.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param weights: Weights arrays of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "    :param exponentials: (Optional) Contains the exponential means and variances computed, they are used in call after training.\n",
    "\n",
    "    :return: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    if exponentials is not None:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)\n",
    "    else:\n",
    "        p = ForwardPassBatchNormalization(X, weights, biases, exponentials)[0]\n",
    "\n",
    "    cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum() / float(X.shape[1])\n",
    "\n",
    "    weight_sum = 0\n",
    "    for weight in weights:\n",
    "\n",
    "        weight_sum += np.power(weight, 2).sum()\n",
    "\n",
    "    return cross_entropy_loss + regularization_term * weight_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_momentum(arrays):\n",
    "    \"\"\"\n",
    "    Initializes the momentum arrays to zero numpy arrays.\n",
    "    \n",
    "    :param matrices: Weights or bias that need corresponding momentum arrays.\n",
    "    :return: Numpy zeros for each layer of the same shape\n",
    "    \"\"\"\n",
    "    momentum_matrices = []\n",
    "    for elem in arrays:\n",
    "        momentum_matrices.append(np.zeros(elem.shape))\n",
    "    return momentum_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_momentum(weights, grad_weights, momentum_weights, biases, grad_biases, momentum_biases, eta, momentum_term):\n",
    "    \"\"\"\n",
    "    Add momentum to an array (weight or bias) of the network.\n",
    "\n",
    "    :param weights: The weight matrices of the k layers.\n",
    "    :param grad_weights: The gradient updatea of the weights.\n",
    "    :param momentum_weights: Momentum arrays (v) of the weights.\n",
    "    :param biases: The bias vector of the k layers.\n",
    "    :param grad_biases: The gradient updates for the biases.\n",
    "    :param momentum_biases: Momentum vectors (v) of the weights.\n",
    "    :param eta: Learning rate of the network.\n",
    "    :param momentum_term: Amount of momentum to be taken into account in the updates.\n",
    "\n",
    "    :return: Updated weights and biases of the network with momentum contribution, updated momentumm arrays for the\n",
    "             weights and biases of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    updated_weights = []\n",
    "    updated_biases = []\n",
    "\n",
    "    for index in range(len(weights)):\n",
    "\n",
    "        new_momentum_weight = momentum_term * momentum_weights[index] + eta * grad_weights[index]\n",
    "        momentum_weights[index] = new_momentum_weight\n",
    "        updated_weights.append(weights[index] - new_momentum_weight)\n",
    "\n",
    "        new_momentum_bias = momentum_term * momentum_biases[index] + eta * grad_biases[index]\n",
    "        momentum_biases[index] = new_momentum_bias\n",
    "        updated_biases.append(biases[index] - new_momentum_bias)\n",
    "\n",
    "\n",
    "    return updated_weights, updated_biases, momentum_weights, momentum_biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass and compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchNormBackPass(g, s, mean_s, var_s, epsilon=1e-20):\n",
    "\n",
    "    # First part of the gradient:\n",
    "    V_b = (var_s+ epsilon) ** (-0.5)\n",
    "    part_1 = g * V_b\n",
    "\n",
    "    # Second part pf the gradient\n",
    "    diff = s - mean_s\n",
    "    grad_J_vb = -0.5 * np.sum(g * (var_s+epsilon) ** (-1.5) * diff, axis=1)\n",
    "    grad_J_vb = np.expand_dims(grad_J_vb, axis=1)\n",
    "    part_2 = (2/float(s.shape[1])) * grad_J_vb * diff\n",
    "\n",
    "    # Third part of the gradient\n",
    "    grad_J_mb = -np.sum(g * V_b, axis=1)\n",
    "    grad_J_mb = np.expand_dims(grad_J_mb, axis=1)\n",
    "    part_3 = grad_J_mb / float(s.shape[1])\n",
    "\n",
    "    return part_1 + part_2 + part_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackwardPassBatchNormalization(X, Y, weights, biases, p, bn_outputs, bn_activations, intermediate_outputs, means, variances, regularization_term):\n",
    "\n",
    "    # Back-propagate output layer at first\n",
    "\n",
    "    g = p - Y\n",
    "\n",
    "    bias_updates = [g.sum(axis=1).reshape(biases[-1].shape)]\n",
    "    weight_updates = [np.dot(g, bn_activations[-1].T)]\n",
    "\n",
    "    g = np.dot(g.T, weights[-1])\n",
    "    ind = 1 * (bn_outputs[-1] > 0)\n",
    "    g = g.T * ind\n",
    "\n",
    "    for i in reversed(range(len(weights) -1)):\n",
    "    # Back-propagate the gradient vector g to the layer before\n",
    "\n",
    "        g = BatchNormBackPass(g, intermediate_outputs[i], means[i], variances[i])\n",
    "\n",
    "        if i == 0:\n",
    "            weight_updates.append(np.dot(g, X.T))\n",
    "            bias_updates.append(np.sum(g, axis=1).reshape(biases[i].shape))\n",
    "            break\n",
    "        else:\n",
    "            weight_updates.append(np.dot(g, bn_activations[i-1].T))\n",
    "            bias_updates.append(np.sum(g, axis=1).reshape(biases[i].shape))\n",
    "\n",
    "        g = np.dot(g.T, weights[i])\n",
    "        ind = 1 * (bn_outputs[i-1] > 0)\n",
    "        g = g.T * ind\n",
    "\n",
    "\n",
    "    for elem in weight_updates:\n",
    "        elem /= X.shape[1]\n",
    "\n",
    "    for elem in bias_updates:\n",
    "        elem /= X.shape[1]\n",
    "\n",
    "    # Reverse the updates to match the order of the layers\n",
    "    weight_updates = list(reversed(weight_updates)).copy()\n",
    "    bias_updates = list(reversed(bias_updates)).copy()\n",
    "\n",
    "    for index in range(len(weight_updates)):\n",
    "        weight_updates[index] += 2*regularization_term * weights[index]\n",
    "\n",
    "    return weight_updates, bias_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExponentialMovingAverage(means, exponential_means, variances, exponential_variances, a=0.99):\n",
    "\n",
    "    for index, elem in enumerate(exponential_means):\n",
    "\n",
    "        exponential_means[index] = a * elem + (1-a) * means[index]\n",
    "        exponential_variances[index] = a * exponential_variances[index] + (1-a) * variances[index]\n",
    "\n",
    "    return exponential_means, exponential_variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_plots(train, validation, display=False, title=None, save_name=None, save_path='../figures/'):\n",
    "    \"\"\"\n",
    "    Visualization and saving plots (losses and accuracies) of the network.\n",
    "\n",
    "    :param train: Loss of accuracy of the training data.\n",
    "    :param validation: Loss of accuracy of the validation data.\n",
    "    :param display: (Optional) Boolean, set to True for displaying the loss evolution plot.\n",
    "    :param title: (Optional) Title of the plot.\n",
    "    :param save_name: (Optional) name of the file to save the plot.\n",
    "    :param save_path: (Optional) Path of the folder to save the plot in your local computer.\n",
    "\n",
    "    :return: None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.plot(train, 'g', label='Training set ')\n",
    "    plt.plot(validation, 'r', label='Validation set')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    if save_name is not None:\n",
    "        if save_path[-1] != '/':\n",
    "            save_path += '/'\n",
    "        plt.savefig(save_path + save_name)\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MiniBatch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchGDBatchNormalization(training_set, validation_set, GDparams, weights, biases, momentum_term=0.9):\n",
    "    \"\"\"\n",
    "    Performs mini batch-gradient descent computations with batch normalization.\n",
    "\n",
    "    :param training_set: Training data.\n",
    "    :param validation_set: Validation data.\n",
    "    :param GDparams: Gradient descent parameters (number of mini batches to construct, learning rate, epochs, amount of regularization to be applied)\n",
    "    :param weights: Weight matrices of the k layers\n",
    "    :param biases: Bias vectors of the k layers\n",
    "\n",
    "    :return: The weight and bias matrices learnt (trained) from the training process,\n",
    "             loss in training and validation set, accuracy evolution in training and validation set.\n",
    "    \"\"\"\n",
    "    [number_of_mini_batches, eta, epoches, regularization_term] = GDparams\n",
    "\n",
    "    [X, Y, y], [X_validation, Y_validation, y_validation] = training_set, validation_set\n",
    "\n",
    "    train_loss_evolution, validation_loss_evolution = [], []\n",
    "    train_accuracy_evolution, validation_accuracy_evolution = [], []\n",
    "\n",
    "    momentum_weights, momentum_biases = initialize_momentum(weights), initialize_momentum(biases)\n",
    "\n",
    "    original_training_cost = ComputeCost(X, Y, weights, biases, regularization_term)\n",
    "\n",
    "    best_weights, best_biases, best_validation_set_accuracy = weights, biases, 0\n",
    "    exponentials, best_exponentials = [], []\n",
    "\n",
    "    for epoch in tqdm(range(epoches)):\n",
    "#     for epoch in range(epoches):\n",
    "\n",
    "        for batch in range(1, int(X.shape[1] / number_of_mini_batches)):\n",
    "            start = (batch - 1) * number_of_mini_batches\n",
    "            end = min(batch * number_of_mini_batches + int(X.shape[1] / number_of_mini_batches), X.shape[1] )\n",
    "\n",
    "            p, batch_norm_activations, batch_norm_outputs, intermediate_outputs, means, variances = ForwardPassBatchNormalization(X[:, start:end], weights, biases)\n",
    "\n",
    "            grad_weights, grad_biases = BackwardPassBatchNormalization(X[:, start:end], Y[:, start:end], weights, biases, p, batch_norm_outputs, batch_norm_activations, intermediate_outputs, means, variances, regularization_term)\n",
    "\n",
    "            weights, biases, momentum_weights, momentum_biases = add_momentum(weights, grad_weights, momentum_weights, biases, grad_biases, momentum_biases, eta, momentum_term)\n",
    "\n",
    "            if epoch == 0 and start == 0:\n",
    "                exponential_means = means.copy()\n",
    "                exponential_variances = variances.copy()\n",
    "                exponentials, best_exponentials = [exponential_means, exponential_variances], [exponential_means, exponential_variances]\n",
    "            else:\n",
    "                exponentials = ExponentialMovingAverage(means, exponentials[0], variances, exponentials[1])\n",
    "\n",
    "        epoch_cost = ComputeCostBatchNormalization(X, Y, weights, biases, regularization_term, exponentials)\n",
    "        if epoch_cost > 3 * original_training_cost:\n",
    "            break\n",
    "        val_epoch_cost = ComputeCostBatchNormalization(X_validation, Y_validation, weights, biases, regularization_term, exponentials)\n",
    "\n",
    "        train_loss_evolution.append(epoch_cost)\n",
    "        validation_loss_evolution.append(val_epoch_cost)\n",
    "\n",
    "        train_accuracy_evolution.append(ComputeAccuracyBatchNormalization(X, y, weights, biases, exponentials))\n",
    "        validation_accuracy_evolution.append(ComputeAccuracyBatchNormalization(X_validation, y_validation, weights, biases, exponentials))\n",
    "\n",
    "        if validation_accuracy_evolution[-1] > best_validation_set_accuracy:\n",
    "\n",
    "            best_weights, best_biases, best_validation_set_accuracy = weights, biases, validation_accuracy_evolution[-1]\n",
    "            best_exponentials = exponentials\n",
    "\n",
    "        # Decay the learning rate\n",
    "        eta *= 0.95\n",
    "\n",
    "    return best_weights, best_biases, [train_loss_evolution, validation_loss_evolution], [train_accuracy_evolution, validation_accuracy_evolution], best_exponentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all data in the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets():\n",
    "    \"\"\"\n",
    "    Creates the full dataset, containing all the available data for training except 1000 images\n",
    "    used for the validation set.\n",
    "\n",
    "    :return: Training, validation and test sets (features, ground-truth labels, and their one-hot representation\n",
    "    \"\"\"\n",
    "\n",
    "    X_training_1, Y_training_1, y_training_1 = LoadBatch('../../cifar-10-batches-py/data_batch_1')\n",
    "    X_training_2, Y_training_2, y_training_2 = LoadBatch('../../cifar-10-batches-py/data_batch_2')\n",
    "    X_training_3, Y_training_3, y_training_3 = LoadBatch('../../cifar-10-batches-py/data_batch_3')\n",
    "    X_training_4, Y_training_4, y_training_4 = LoadBatch('../../cifar-10-batches-py/data_batch_4')\n",
    "    X_training_5, Y_training_5, y_training_5 = LoadBatch('../../cifar-10-batches-py/data_batch_5')\n",
    "\n",
    "    X_training = np.concatenate((X_training_1, X_training_3), axis=1)\n",
    "    X_training = np.copy(np.concatenate((X_training, X_training_4), axis=1))\n",
    "    X_training = np.copy(np.concatenate((X_training, X_training_5), axis=1))\n",
    "\n",
    "    X_training = np.concatenate((X_training, X_training_2[:, :9000]), axis=1)\n",
    "\n",
    "    Y_training = np.concatenate((Y_training_1, Y_training_3), axis=1)\n",
    "    Y_training = np.copy(np.concatenate((Y_training, Y_training_4), axis=1))\n",
    "    Y_training = np.copy(np.concatenate((Y_training, Y_training_5), axis=1))\n",
    "\n",
    "    Y_training = np.concatenate((Y_training, Y_training_2[:, :9000]), axis=1)\n",
    "\n",
    "    y_training = y_training_1 + y_training_3 + y_training_4 + y_training_5 + y_training_2[:9000]\n",
    "\n",
    "    X_validation = np.copy(X_training_2[:, 9000:])\n",
    "    Y_validation = np.copy(Y_training_2[:, 9000:])\n",
    "    y_validation = y_training_2[9000:]\n",
    "\n",
    "    X_test, _, y_test = LoadBatch('../../cifar-10-batches-py/test_batch')\n",
    "\n",
    "    mean = np.mean(X_training)\n",
    "    X_training -= mean\n",
    "    X_validation -= mean\n",
    "    X_test -= mean\n",
    "\n",
    "    return [X_training, Y_training, y_training], [X_validation, Y_validation, y_validation], [X_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS EXERCISE 1: Try improvements on your k-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement 1: Try to perform batch normalisation after the activation function is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS EXERCISE 2: Try a different activation function than ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
