{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import to_categorical as make_class_categorical\n",
    "import _pickle as pickle\n",
    "from tqdm import tqdm\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(d, m, K, std=0.001):\n",
    "    \"\"\"\n",
    "    Initializes the weight and bias arrays for the 2 layers of the network\n",
    "\n",
    "    :param d: Dimensionality of the input data\n",
    "    :param m: Number of nodes in the first layer\n",
    "    :param K: Number of different classes (K=10 for the CIFAR-10 dataset)\n",
    "    :param variance (optional): The variance of the normal distribution that will be used for the initialization of the weights\n",
    "\n",
    "    :return: Weights and bias arrays for the first and second layer of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(400)\n",
    "\n",
    "    W1 = np.random.normal(0, std, size=(m, d))\n",
    "    b1 = np.zeros(shape=(m, 1))\n",
    "\n",
    "    W2 = np.random.normal(0, std, size=(K, m))\n",
    "    b2 = np.zeros(shape=(K, 1))\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBatch(filename):\n",
    "    \"\"\"\n",
    "    Loads batch based on the given filename and produces the X, Y, and y arrays\n",
    "\n",
    "    :param filename: Path of the file\n",
    "    :return: X, Y and y arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # borrowed from https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    dictionary = unpickle(filename)\n",
    "\n",
    "    # borrowed from https://stackoverflow.com/questions/16977385/extract-the-nth-key-in-a-python-dictionary?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "    def ix(dic, n):  # don't use dict as  a variable name\n",
    "        try:\n",
    "            return list(dic)[n]  # or sorted(dic)[n] if you want the keys to be sorted\n",
    "        except IndexError:\n",
    "            print('not enough keys')\n",
    "\n",
    "    garbage = ix(dictionary, 1)\n",
    "    y = dictionary[garbage]\n",
    "    Y = np.transpose(make_class_categorical(y, 10))\n",
    "    garbage = ix(dictionary, 2)\n",
    "    X = np.transpose(dictionary[garbage]) / 255\n",
    "\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function\n",
    "\n",
    "    :param x: Input to the function\n",
    "\n",
    "    :return: Output of ReLU(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "\n",
    "    # Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    # Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    # Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateClassifier(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Computes the Softmax output of the 2 layer network, based on input data X and trained weight and bias arrays\n",
    "\n",
    "    :param X: Input data\n",
    "    :param W1: Weight array of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight array of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "\n",
    "    :return: Softmax output of the trained network\n",
    "    \"\"\"\n",
    "\n",
    "    s1 = np.dot(W1, X) + b1\n",
    "    h = ReLU(s1)\n",
    "    s = np.dot(W2, h) + b2\n",
    "    p = softmax(s, axis=0)\n",
    "\n",
    "    return p, h, s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClasses(p):\n",
    "    \"\"\"\n",
    "    Predicts classes based on the softmax output of the network\n",
    "\n",
    "    :param p: Softmax output of the network\n",
    "    :return: Predicted classes\n",
    "    \"\"\"\n",
    "\n",
    "    return np.argmax(p, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(X, y, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the feed-forward 2-layer network\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "\n",
    "    :return: Accuracy metric of the neural network.\n",
    "    \"\"\"\n",
    "    p, _, _ = EvaluateClassifier(X=X, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "    predictions = predictClasses(p)\n",
    "\n",
    "    accuracy = np.sum(np.where(predictions - y == 0, 1, 0))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCost(X, Y, W1, W2, b1, b2, regularization_term= 0):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss on a batch of data.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "\n",
    "    :return: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "\n",
    "    p, _, _ = EvaluateClassifier(X=X, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "    cross_entropy_loss = np.sum(-np.log(np.dot(Y.T, p)), axis=1).sum() / float(X.shape[0])\n",
    "\n",
    "    weight_sum = np.power(W1, 2).sum() + np.power(W2, 2).sum()\n",
    "\n",
    "    return cross_entropy_loss + regularization_term * weight_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNum(X, Y, W1, b1, W2, b2, regularization_term, h=1e-5):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data with numerical computations.\n",
    "    Contributed by Josephine Sullivan for educational purposes for the DD2424 Deep Learning in Data Science course.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network computed with numerical computations\n",
    "    \"\"\"\n",
    "\n",
    "    grad_W1= np.zeros((W1.shape[0], W1.shape[1]))\n",
    "    grad_b1= np.zeros((W1.shape[0], 1))\n",
    "    grad_W2= np.zeros((W2.shape[0], W2.shape[1]))\n",
    "    grad_b2= np.zeros((W2.shape[0], 1))\n",
    "    \n",
    "    c = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "    \n",
    "    for i in range(b1.shape[0]):\n",
    "        b1_try = np.copy(b1)\n",
    "        b1_try[i, 0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1_try, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "        grad_b1[i,0] = (c2-c) / h\n",
    "\n",
    "    for i in range(b2.shape[0]):\n",
    "        b2_try= np.copy(b2)\n",
    "        b2_try[i, 0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2_try, regularization_term=regularization_term)\n",
    "        grad_b2[i,0] = (c2-c) / h\n",
    "        \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            \n",
    "            W1_try= np.copy(W1)\n",
    "            W1_try[i,j] += h\n",
    "            c2= ComputeCost(X=X, Y=Y, W1=W1_try, b1=b1, W2=W2, regularization_term=regularization_term)\n",
    "            \n",
    "            grad_W1[i,j] = (c2-c) / h\n",
    "\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2_try = np.copy(W2)\n",
    "            W2_try[i, j] += h\n",
    "            c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2_try, regularization_term=regularization_term)\n",
    "\n",
    "            grad_W2[i, j] = (c2 - c) / h\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, W1, b1, W2, b2, regularization_term, h=1e-5):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data with numerical computations of great precision, thus slower computations.\n",
    "    Contributed by Josephine Sullivan for educational purposes for the DD2424 Deep Learning in Data Science course.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network computed with numerical computations with high precision.\n",
    "    \"\"\"\n",
    "\n",
    "    grad_W1= np.zeros((W1.shape[0], W1.shape[1]))\n",
    "    grad_b1= np.zeros((b1.shape[0], 1))\n",
    "    grad_W2= np.zeros((W2.shape[0], W2.shape[1]))\n",
    "    grad_b2= np.zeros((b2.shape[0], 1))\n",
    "    \n",
    "    for i in tqdm(range(b1.shape[0])):\n",
    "\n",
    "        b1_try = np.copy(b1)\n",
    "        b1_try[i,0] -= h\n",
    "        c1 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1_try, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "        b1_try = np.copy(b1)\n",
    "        b1_try[i,0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1_try, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "        grad_b1[i,0] = (c2-c1)/(2*h)\n",
    "\n",
    "    for i in tqdm(range(b2.shape[0])):\n",
    "        b2_try = np.copy(b2)\n",
    "        b2_try[i, 0] -= h\n",
    "        c1 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2_try, regularization_term=regularization_term)\n",
    "\n",
    "        b2_try = np.copy(b2)\n",
    "        b2_try[i, 0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2_try, regularization_term=regularization_term)\n",
    "        grad_b2[i, 0] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    for i in tqdm(range(W1.shape[0])):\n",
    "        for j in tqdm(range(W1.shape[1])):\n",
    "\n",
    "            W1_try = np.copy(W1)\n",
    "            W1_try[i, j] -= h\n",
    "            c1 = ComputeCost(X=X, Y=Y, W1=W1_try, b1=b1, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            W1_try = np.copy(W1)\n",
    "            W1_try[i, j] += h\n",
    "            c2 = ComputeCost(X=X, Y=Y, W1=W1_try, b1=b1, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            grad_W1[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    for i in tqdm(range(W2.shape[0])):\n",
    "        for j in tqdm(range(W2.shape[1])):\n",
    "\n",
    "            W2_try = np.copy(W2)\n",
    "            W2_try[i, j] -= h\n",
    "            c1 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2_try, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            W2_try = np.copy(W2)\n",
    "            W2_try[i, j] += h\n",
    "            c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2_try, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            grad_W2[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradients(X, Y, W1, b1, W2, b2, regularization_term= 0):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate the classifier to the batch\n",
    "    p, h, s1 = EvaluateClassifier(X=X, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "    # Back-propagate second layer at first\n",
    "\n",
    "    g = p - Y\n",
    "    grad_b2 = g.sum(axis=1).reshape(b2.shape)\n",
    "    grad_W2 = np.dot(g, h.T)\n",
    "\n",
    "    # Back-propagate the gradient vector g to the first layer\n",
    "    g = np.dot(g.T, W2)\n",
    "    ind = 1 * (s1 > 0)\n",
    "    g = g.T * ind\n",
    "\n",
    "    grad_b1 = np.sum(g, axis=1).reshape(b1.shape)\n",
    "    grad_W1 = np.dot(g, X.T)\n",
    "\n",
    "    grad_W1 /= X.shape[1]\n",
    "    grad_b1 /= X.shape[1]\n",
    "    grad_W2 /= X.shape[1]\n",
    "    grad_b2 /= X.shape[1]\n",
    "\n",
    "    # Add regularizers\n",
    "    grad_W1 = grad_W1 + 2 * regularization_term * W1\n",
    "    grad_W2 = grad_W2 +2 * regularization_term * W2\n",
    "\n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(gradW1, gradb1, gradW2, gradb2, gradW1_num, gradb1_num, gradW2_num, gradb2_num, threshold = 1e-4):\n",
    "\n",
    "    \"\"\"\n",
    "    Compares the gradients of both the analytical and numerical method and prints out a message of result \n",
    "    or failure, depending on how close these gradients are between each other.\n",
    "    \n",
    "    :param gradW1: Gradient of W1, analytically computed\n",
    "    :param gradb1: Gradient of b1, analytically computed\n",
    "    :param gradW2: Gradient of W2, analytically computed\n",
    "    :param gradb2: Gradient of b2, analytically computed\n",
    "    :param gradW1_num: Gradient of W1, numerically computed\n",
    "    :param gradb1_num: Gradient of b1, numerically computed\n",
    "    :param gradW2_num: Gradient of W2, numerically computed\n",
    "    :param gradb2_num: Gradient of b2, numerically computed\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    W1_abs = np.abs(gradW1 - gradW1_num)\n",
    "    b1_abs = np.abs(gradb1 - gradb1_num)\n",
    "    \n",
    "    W2_abs = np.abs(gradW2 - gradW2_num)\n",
    "    b2_abs = np.abs(gradb2 - gradb2_num)\n",
    "    \n",
    "    W1_nominator = np.average(W1_abs)\n",
    "    b1_nominator = np.average(b1_abs)\n",
    "    \n",
    "    W2_nominator = np.average(W2_abs)\n",
    "    b2_nominator = np.average(b2_abs)\n",
    "\n",
    "\n",
    "    gradW1_abs = np.absolute(gradW1)\n",
    "    gradW1_num_abs = np.absolute(gradW1_num)\n",
    "    \n",
    "    gradW2_abs = np.absolute(gradW2)\n",
    "    gradW2_num_abs = np.absolute(gradW2_num)\n",
    "\n",
    "    gradb1_abs = np.absolute(gradb1)\n",
    "    gradb1_num_abs = np.absolute(gradb1)\n",
    "    \n",
    "    gradb2_abs = np.absolute(gradb2)\n",
    "    gradb2_num_abs = np.absolute(gradb2)\n",
    "\n",
    "    sum_W1 = gradW1_abs + gradW1_num_abs\n",
    "    sum_W2 = gradW2_abs + gradW2_num_abs\n",
    "    sum_b1 = gradb1_abs + gradb1_num_abs\n",
    "    sum_b2 = gradb2_abs + gradb2_num_abs\n",
    "\n",
    "    check_W1 = W1_nominator / np.amax(sum_W1)\n",
    "    check_b1 = b1_nominator / np.amax(sum_b1)\n",
    "\n",
    "    check_W2 = W2_nominator / np.amax(sum_W2)\n",
    "    check_b2 = b2_nominator / np.amax(sum_b2)\n",
    "\n",
    "\n",
    "    if check_W1 < threshold and check_b1 < threshold and check_W2 < threshold and check_b2 < threshold:\n",
    "        print( \"Success!!\")\n",
    "        print(\"Average error on weights of first layer= \", check_W1)\n",
    "        print(\"Average error on bias of first layer=\", check_b1)\n",
    "        print(\"Average error on weights of second layer= \", check_W2)\n",
    "        print(\"Average error on bias of second layer= \", check_b2)\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"Average error on weights of first layer= \", check_W1)\n",
    "        print(\"Average error on bias of first layer=\", check_b1)\n",
    "        print(\"Average error on weights of second layer= \", check_W2)\n",
    "        print(\"Average error on bias of second layer= \", check_b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGNMENT 2 EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Exercise 1: Read in the data & initialize the parameters ofthe network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_1, Y_training_1, y_training_1 = LoadBatch('../../cifar-10-batches-py/data_batch_1')\n",
    "X_training_2, Y_training_2, y_training_2 = LoadBatch('../../cifar-10-batches-py/data_batch_2')\n",
    "X_test, _, y_test = LoadBatch('../../cifar-10-batches-py/test_batch')\n",
    "\n",
    "mean = np.mean(X_training_1)\n",
    "X_training_1 -= mean\n",
    "X_training_2 -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = initialize_weights(d=X_training_1.shape[0], m=50, K=Y_training_1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Compute the gradients for the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W1, grad_b1, grad_W2, grad_b2 = ComputeGradients(X_training_1[:,0:2], Y_training_1[:,0:2], W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test no.1: Compare with numerically computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W1_num = np.load('grad_W1_num.npy')\n",
    "grad_b1_num = np.load('grad_b1_num.npy')\n",
    "grad_W2_num = np.load('grad_W2_num.npy')\n",
    "grad_b2_num = np.load('grad_b2_num.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!!\n",
      "Average error on weights of first layer=  7.52283628908819e-09\n",
      "Average error on bias of first layer= 4.602426280088195e-09\n",
      "Average error on weights of second layer=  3.0747216543846533e-10\n",
      "Average error on bias of second layer=  1.193638211533644e-11\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_W1, grad_b1, grad_W2, grad_b2, grad_W1_num, grad_b1_num, grad_W2_num, grad_b2_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Add momentum to your update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
