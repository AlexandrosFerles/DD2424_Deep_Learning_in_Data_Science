{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import to_categorical as make_class_categorical\n",
    "import _pickle as pickle\n",
    "from tqdm import tqdm\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(d, m, K, std=0.001):\n",
    "    \"\"\"\n",
    "    Initializes the weight and bias arrays for the 2 layers of the network\n",
    "\n",
    "    :param d: Dimensionality of the input data\n",
    "    :param m: Number of nodes in the first layer\n",
    "    :param K: Number of different classes (K=10 for the CIFAR-10 dataset)\n",
    "    :param variance (optional): The variance of the normal distribution that will be used for the initialization of the weights\n",
    "\n",
    "    :return: Weights and bias arrays for the first and second layer of the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(400)\n",
    "\n",
    "    W1 = np.random.normal(0, std, size=(m, d))\n",
    "    b1 = np.zeros(shape=(m, 1))\n",
    "\n",
    "    W2 = np.random.normal(0, std, size=(K, m))\n",
    "    b2 = np.zeros(shape=(K, 1))\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBatch(filename):\n",
    "    \"\"\"\n",
    "    Loads batch based on the given filename and produces the X, Y, and y arrays\n",
    "\n",
    "    :param filename: Path of the file\n",
    "    :return: X, Y and y arrays\n",
    "    \"\"\"\n",
    "\n",
    "    # borrowed from https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    dictionary = unpickle(filename)\n",
    "\n",
    "    # borrowed from https://stackoverflow.com/questions/16977385/extract-the-nth-key-in-a-python-dictionary?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "    def ix(dic, n):  # don't use dict as  a variable name\n",
    "        try:\n",
    "            return list(dic)[n]  # or sorted(dic)[n] if you want the keys to be sorted\n",
    "        except IndexError:\n",
    "            print('not enough keys')\n",
    "\n",
    "    garbage = ix(dictionary, 1)\n",
    "    y = dictionary[garbage]\n",
    "    Y = np.transpose(make_class_categorical(y, 10))\n",
    "    garbage = ix(dictionary, 2)\n",
    "    X = np.transpose(dictionary[garbage]) / 255\n",
    "\n",
    "    return X, Y, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit function\n",
    "\n",
    "    :param x: Input to the function\n",
    "\n",
    "    :return: Output of ReLU(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "\n",
    "    # Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    # Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    # Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateClassifier(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Computes the Softmax output of the 2 layer network, based on input data X and trained weight and bias arrays\n",
    "\n",
    "    :param X: Input data\n",
    "    :param W1: Weight array of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight array of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "\n",
    "    :return: Softmax output of the trained network\n",
    "    \"\"\"\n",
    "    s1 = np.dot(W1, X) + b1\n",
    "    h = ReLU(s1)\n",
    "    s = np.dot(W2, h) + b2\n",
    "    p = softmax(s, axis=0)\n",
    "\n",
    "    return p, h, s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClasses(p):\n",
    "    \"\"\"\n",
    "    Predicts classes based on the softmax output of the network\n",
    "\n",
    "    :param p: Softmax output of the network\n",
    "    :return: Predicted classes\n",
    "    \"\"\"\n",
    "\n",
    "    return np.argmax(p, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeAccuracy(X, y, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the feed-forward 2-layer network\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "\n",
    "    :return: Accuracy metric of the neural network.\n",
    "    \"\"\"\n",
    "    p, _, _ = EvaluateClassifier(X=X, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "    predictions = predictClasses(p)\n",
    "\n",
    "    accuracy = np.sum(np.where(predictions - y == 0, 1, 0))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCost(X, Y, W1, W2, b1, b2, regularization_term= 0):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss on a batch of data.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param y: Labels of the ground truth\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "\n",
    "    :return: Cross-entropy loss.\n",
    "    \"\"\"\n",
    "    p, _, _ = EvaluateClassifier(X=X, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "    cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum() / float(X.shape[1])\n",
    "\n",
    "    weight_sum = np.power(W1, 2).sum() + np.power(W2, 2).sum()\n",
    "\n",
    "    return cross_entropy_loss + regularization_term * weight_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNum(X, Y, W1, b1, W2, b2, regularization_term, h=1e-5):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data with numerical computations.\n",
    "    Contributed by Josephine Sullivan for educational purposes for the DD2424 Deep Learning in Data Science course.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network computed with numerical computations\n",
    "    \"\"\"\n",
    "\n",
    "    grad_W1= np.zeros((W1.shape[0], W1.shape[1]))\n",
    "    grad_b1= np.zeros((W1.shape[0], 1))\n",
    "    grad_W2= np.zeros((W2.shape[0], W2.shape[1]))\n",
    "    grad_b2= np.zeros((W2.shape[0], 1))\n",
    "    \n",
    "    c = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "    \n",
    "    for i in range(b1.shape[0]):\n",
    "        b1_try = np.copy(b1)\n",
    "        b1_try[i, 0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1_try, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "        grad_b1[i,0] = (c2-c) / h\n",
    "\n",
    "    for i in range(b2.shape[0]):\n",
    "        b2_try= np.copy(b2)\n",
    "        b2_try[i, 0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2_try, regularization_term=regularization_term)\n",
    "        grad_b2[i,0] = (c2-c) / h\n",
    "        \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            \n",
    "            W1_try= np.copy(W1)\n",
    "            W1_try[i,j] += h\n",
    "            c2= ComputeCost(X=X, Y=Y, W1=W1_try, b1=b1, W2=W2, regularization_term=regularization_term)\n",
    "            \n",
    "            grad_W1[i,j] = (c2-c) / h\n",
    "\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2_try = np.copy(W2)\n",
    "            W2_try[i, j] += h\n",
    "            c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2_try, regularization_term=regularization_term)\n",
    "\n",
    "            grad_W2[i, j] = (c2 - c) / h\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, W1, b1, W2, b2, regularization_term, h=1e-5):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data with numerical computations of great precision, thus slower computations.\n",
    "    Contributed by Josephine Sullivan for educational purposes for the DD2424 Deep Learning in Data Science course.\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network computed with numerical computations with high precision.\n",
    "    \"\"\"\n",
    "\n",
    "    grad_W1= np.zeros((W1.shape[0], W1.shape[1]))\n",
    "    grad_b1= np.zeros((b1.shape[0], 1))\n",
    "    grad_W2= np.zeros((W2.shape[0], W2.shape[1]))\n",
    "    grad_b2= np.zeros((b2.shape[0], 1))\n",
    "    \n",
    "    for i in tqdm(range(b1.shape[0])):\n",
    "\n",
    "        b1_try = np.copy(b1)\n",
    "        b1_try[i,0] -= h\n",
    "        c1 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1_try, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "        b1_try = np.copy(b1)\n",
    "        b1_try[i,0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1_try, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "        grad_b1[i,0] = (c2-c1)/(2*h)\n",
    "\n",
    "    for i in tqdm(range(b2.shape[0])):\n",
    "        b2_try = np.copy(b2)\n",
    "        b2_try[i, 0] -= h\n",
    "        c1 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2_try, regularization_term=regularization_term)\n",
    "\n",
    "        b2_try = np.copy(b2)\n",
    "        b2_try[i, 0] += h\n",
    "        c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2, b2=b2_try, regularization_term=regularization_term)\n",
    "        grad_b2[i, 0] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    for i in tqdm(range(W1.shape[0])):\n",
    "        for j in tqdm(range(W1.shape[1])):\n",
    "\n",
    "            W1_try = np.copy(W1)\n",
    "            W1_try[i, j] -= h\n",
    "            c1 = ComputeCost(X=X, Y=Y, W1=W1_try, b1=b1, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            W1_try = np.copy(W1)\n",
    "            W1_try[i, j] += h\n",
    "            c2 = ComputeCost(X=X, Y=Y, W1=W1_try, b1=b1, W2=W2, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            grad_W1[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    for i in tqdm(range(W2.shape[0])):\n",
    "        for j in tqdm(range(W2.shape[1])):\n",
    "\n",
    "            W2_try = np.copy(W2)\n",
    "            W2_try[i, j] -= h\n",
    "            c1 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2_try, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            W2_try = np.copy(W2)\n",
    "            W2_try[i, j] += h\n",
    "            c2 = ComputeCost(X=X, Y=Y, W1=W1, b1=b1, W2=W2_try, b2=b2, regularization_term=regularization_term)\n",
    "\n",
    "            grad_W2[i, j] = (c2 - c1) / (2 * h)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradients(X, Y, W1, b1, W2, b2, p, h, s1, regularization_term= 0):\n",
    "    \"\"\"\n",
    "    Computes gradient descent updates on a batch of data\n",
    "\n",
    "    :param X: Input data\n",
    "    :param Y: One-hot representation of the true labels of input data X\n",
    "    :param W1: Weight matrix of the first layer\n",
    "    :param b1: Bias vector of the first layer\n",
    "    :param W2: Weight matrix of the second layer\n",
    "    :param b2: Bias vector of the second layer\n",
    "    :param p: Softmax probabilities (predictions) of the network over classes.\n",
    "    :param h: ReLU activations of the network.\n",
    "    :param s1: True outout of the first layer of the network.\n",
    "    :param regularization_term: Contribution of the regularization in the weight updates\n",
    "\n",
    "    :return: Weight and bias updates of the first and second layer of our network\n",
    "    \"\"\"\n",
    "\n",
    "    # Back-propagate second layer at first\n",
    "\n",
    "    g = p - Y\n",
    "    grad_b2 = g.sum(axis=1).reshape(b2.shape)\n",
    "    grad_W2 = np.dot(g, h.T)\n",
    "\n",
    "    # Back-propagate the gradient vector g to the first layer\n",
    "    g = np.dot(g.T, W2)\n",
    "    ind = 1 * (s1 > 0)\n",
    "    g = g.T * ind\n",
    "\n",
    "    grad_b1 = np.sum(g, axis=1).reshape(b1.shape)\n",
    "    grad_W1 = np.dot(g, X.T)\n",
    "\n",
    "    grad_W1 /= X.shape[1]\n",
    "    grad_b1 /= X.shape[1]\n",
    "    grad_W2 /= X.shape[1]\n",
    "    grad_b2 /= X.shape[1]\n",
    "\n",
    "    # Add regularizers\n",
    "    grad_W1 = grad_W1 + 2 * regularization_term * W1\n",
    "    grad_W2 = grad_W2 +2 * regularization_term * W2\n",
    "\n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(gradW1, gradb1, gradW2, gradb2, gradW1_num, gradb1_num, gradW2_num, gradb2_num, threshold = 1e-4):\n",
    "\n",
    "    \"\"\"\n",
    "    Compares the gradients of both the analytical and numerical method and prints out a message of result \n",
    "    or failure, depending on how close these gradients are between each other.\n",
    "    \n",
    "    :param gradW1: Gradient of W1, analytically computed\n",
    "    :param gradb1: Gradient of b1, analytically computed\n",
    "    :param gradW2: Gradient of W2, analytically computed\n",
    "    :param gradb2: Gradient of b2, analytically computed\n",
    "    :param gradW1_num: Gradient of W1, numerically computed\n",
    "    :param gradb1_num: Gradient of b1, numerically computed\n",
    "    :param gradW2_num: Gradient of W2, numerically computed\n",
    "    :param gradb2_num: Gradient of b2, numerically computed\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    W1_abs = np.abs(gradW1 - gradW1_num)\n",
    "    b1_abs = np.abs(gradb1 - gradb1_num)\n",
    "    \n",
    "    W2_abs = np.abs(gradW2 - gradW2_num)\n",
    "    b2_abs = np.abs(gradb2 - gradb2_num)\n",
    "    \n",
    "    W1_nominator = np.average(W1_abs)\n",
    "    b1_nominator = np.average(b1_abs)\n",
    "    \n",
    "    W2_nominator = np.average(W2_abs)\n",
    "    b2_nominator = np.average(b2_abs)\n",
    "\n",
    "\n",
    "    gradW1_abs = np.absolute(gradW1)\n",
    "    gradW1_num_abs = np.absolute(gradW1_num)\n",
    "    \n",
    "    gradW2_abs = np.absolute(gradW2)\n",
    "    gradW2_num_abs = np.absolute(gradW2_num)\n",
    "\n",
    "    gradb1_abs = np.absolute(gradb1)\n",
    "    gradb1_num_abs = np.absolute(gradb1)\n",
    "    \n",
    "    gradb2_abs = np.absolute(gradb2)\n",
    "    gradb2_num_abs = np.absolute(gradb2)\n",
    "\n",
    "    sum_W1 = gradW1_abs + gradW1_num_abs\n",
    "    sum_W2 = gradW2_abs + gradW2_num_abs\n",
    "    sum_b1 = gradb1_abs + gradb1_num_abs\n",
    "    sum_b2 = gradb2_abs + gradb2_num_abs\n",
    "\n",
    "    check_W1 = W1_nominator / np.amax(sum_W1)\n",
    "    check_b1 = b1_nominator / np.amax(sum_b1)\n",
    "\n",
    "    check_W2 = W2_nominator / np.amax(sum_W2)\n",
    "    check_b2 = b2_nominator / np.amax(sum_b2)\n",
    "\n",
    "\n",
    "    if check_W1 < threshold and check_b1 < threshold and check_W2 < threshold and check_b2 < threshold:\n",
    "        print( \"Success!!\")\n",
    "        print(\"Average error on weights of first layer= \", check_W1)\n",
    "        print(\"Average error on bias of first layer=\", check_b1)\n",
    "        print(\"Average error on weights of second layer= \", check_W2)\n",
    "        print(\"Average error on bias of second layer= \", check_b2)\n",
    "    else:\n",
    "        print(\"Failure\")\n",
    "        print(\"Average error on weights of first layer= \", check_W1)\n",
    "        print(\"Average error on bias of first layer=\", check_b1)\n",
    "        print(\"Average error on weights of second layer= \", check_W2)\n",
    "        print(\"Average error on bias of second layer= \", check_b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchGD(X, Y, X_validation, Y_validation, y, y_validation, GDparams, W1, b1, W2, b2, regularization_term = 0):\n",
    "    \"\"\"\n",
    "    Performs mini batch-gradient descent computations.\n",
    "\n",
    "    :param X: Input batch of data\n",
    "    :param Y: One-hot representation of the true labels of the data.\n",
    "    :param X_validation: Input batch of validation data.\n",
    "    :param Y_validation: One-hot representation of the true labels of the validation data.\n",
    "    :param y: True labels of the data.\n",
    "    :param y_validation: True labels of the validation data.\n",
    "    :param GDparams: Gradient descent parameters (number of mini batches to construct, learning rate, epochs)\n",
    "    :param W1: Weight matrix of the first layer of the network.\n",
    "    :param b1: Bias vector of the first layer of the network.\n",
    "    :param W2: Weight matrix of the second layer of the network.\n",
    "    :param b2: Bias vector of the second layer of the network.\n",
    "    :param regularization_term: Amount of regularization applied.\n",
    "\n",
    "    :return: The weight and bias matrices learnt (trained) from the training process, loss in training and validation set.\n",
    "    \"\"\"\n",
    "    number_of_mini_batches = GDparams[0]\n",
    "    eta = GDparams[1]\n",
    "    epoches = GDparams[2]\n",
    "\n",
    "    cost = []\n",
    "    val_cost = []\n",
    "\n",
    "    for _ in tqdm(range(epoches)):\n",
    "\n",
    "        for batch in range(1, int(X.shape[1] // number_of_mini_batches)):\n",
    "            start = (batch - 1) * number_of_mini_batches + 1\n",
    "            end = batch * number_of_mini_batches + 1\n",
    "\n",
    "            p, h, s1 = EvaluateClassifier(X[:,start:end], W1, b1, W2, b2)\n",
    "\n",
    "            grad_W1, grad_b1, grad_W2, grad_b2 = ComputeGradients(X[:,start:end], Y[:,start:end], W1, b1, W2, b2, p, h ,s1)\n",
    "\n",
    "            W1 -= eta * grad_W1\n",
    "            b1 -= eta * grad_b1\n",
    "            W2 -= eta * grad_W2\n",
    "            b2 -= eta * grad_b2\n",
    "\n",
    "        epoch_cost = ComputeCost(X, Y, W1, W2, b1, b2)\n",
    "        val_epoch_cost = ComputeCost(X_validation, Y_validation, W1, W2, b1, b2)\n",
    "\n",
    "        cost.append(epoch_cost)\n",
    "        val_cost.append(val_epoch_cost)\n",
    "\n",
    "    return W1, b1, W2, b2, cost, val_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_costs(loss, val_loss, display= False, title = None, save_name= None, save_path='./'):\n",
    "    \"\"\"\n",
    "    Visualization and saving the loss of the network.\n",
    "\n",
    "    :param loss: Loss of the network.\n",
    "    :param display: (Optional) Boolean, set to True for displaying the loss evolution plot.\n",
    "    :param title: (Optional) Title of the plot.\n",
    "    :param save_name: (Optional) name of the file to save the plot.\n",
    "    :param save_path: (Optional) Path of the folder to save the plot in your local computer.\n",
    "\n",
    "    :return: None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.plot(loss, 'g', label='Training set ')\n",
    "    plt.plot(val_loss, 'r', label='Validation set')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "    if save_name is not None:\n",
    "        if save_path[-1] !='/':\n",
    "            save_path+='/'\n",
    "        plt.savefig(save_path + save_name)\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGNMENT 2 EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Exercise 1: Read in the data & initialize the parameters ofthe network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_1, Y_training_1, y_training_1 = LoadBatch('../../cifar-10-batches-py/data_batch_1')\n",
    "X_training_2, Y_training_2, y_training_2 = LoadBatch('../../cifar-10-batches-py/data_batch_2')\n",
    "X_test, _, y_test = LoadBatch('../../cifar-10-batches-py/test_batch')\n",
    "\n",
    "mean = np.mean(X_training_1)\n",
    "X_training_1 -= mean\n",
    "X_training_2 -= mean\n",
    "X_test -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = initialize_weights(d=X_training_1.shape[0], m=50, K=Y_training_1.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Compute the gradients for the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, h, s1 = EvaluateClassifier(X_training_1[:,0:2], W1, b1, W2, b2)\n",
    "grad_W1, grad_b1, grad_W2, grad_b2 = ComputeGradients(X_training_1[:,0:2], Y_training_1[:,0:2], W1, b1, W2, b2, p, h, s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test no.1: Compare with numerically computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W1_num = np.load('grad_W1_num.npy')\n",
    "grad_b1_num = np.load('grad_b1_num.npy')\n",
    "grad_W2_num = np.load('grad_W2_num.npy')\n",
    "grad_b2_num = np.load('grad_b2_num.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!!\n",
      "Average error on weights of first layer=  7.52283628908819e-09\n",
      "Average error on bias of first layer= 4.602426280088195e-09\n",
      "Average error on weights of second layer=  3.0747216543846533e-10\n",
      "Average error on bias of second layer=  1.193638211533644e-11\n"
     ]
    }
   ],
   "source": [
    "check_similarity(grad_W1, grad_b1, grad_W2, grad_b2, grad_W1_num, grad_b1_num, grad_W2_num, grad_b2_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test no.2: Overfit on a small subset of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 21.13it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W1, b1, W2, b2 = initialize_weights(d=X_training_1.shape[0], m=50, K=Y_training_1.shape[0])\n",
    "GD_params = [100, 0.05, 200]\n",
    "\n",
    "W1, b1, W2, b2, training_set_loss, validation_set_loss = MiniBatchGD(   X_training_1[:, :1000],\n",
    "                                                                        Y_training_1[:, :1000],\n",
    "                                                                        X_training_2[:, :1000],\n",
    "                                                                        Y_training_2[:, :1000],\n",
    "                                                                        [], [],\n",
    "                                                                        GD_params,\n",
    "                                                                        W1, b1, W2, b2)\n",
    "\n",
    "visualize_costs(training_set_loss, validation_set_loss, display=True, title='Cross Entropy Loss Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe overfitting in the training data, as the loss on the training set gets pretty small values, while the loss on the validation set starts increasing after a few number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Add momentum to your update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
