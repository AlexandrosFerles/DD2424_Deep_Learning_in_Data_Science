{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import everything you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ring a bell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inform_exit():\n",
    "    \n",
    "    os.system('say \"Training process is completed\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Text_Data(file_path='../goblet_book.txt'):\n",
    "    \"\"\"\n",
    "    Reads the input data.\n",
    "\n",
    "    :param file_path: (Optional) Position of the txt file in the local system.\n",
    "\n",
    "    :return: book_data: all input characters and unique_characters: unique single characters of the input data.\n",
    "    \"\"\"\n",
    "    book_data = open(file_path, 'r').read()\n",
    "    unique_characters = list(set(book_data))\n",
    "\n",
    "    return book_data, unique_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters to Index transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Char_to_Ind(chars_list, unique_chars):\n",
    "    \"\"\"\n",
    "    Maps the original characters to integers.\n",
    "\n",
    "    :param chars_list: The list of characters to be encoded into integers.\n",
    "    :param unique_chars: The set of unique characters available.\n",
    "\n",
    "    :return: A list of integers that correspond to the characters.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_characters = []\n",
    "\n",
    "    for char in chars_list:\n",
    "        for index, letter in enumerate(unique_chars):\n",
    "\n",
    "            if char == letter:\n",
    "\n",
    "                encoded_characters.append(index)\n",
    "\n",
    "    return encoded_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index to Char transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ind_to_Char(one_hot_representation, unique_chars_list):\n",
    "    \"\"\"\n",
    "    Maps a list of integers to their corresponding characters,\n",
    "\n",
    "    :param one_hot_representation: A list of one_hot representations to be transformed to their correspoding characters.\n",
    "    :param unique_chars_list: The list of unique characters.\n",
    "\n",
    "    :return: The actual character sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    actual_character_sequence = []\n",
    "\n",
    "    for i in range(one_hot_representation.shape[1]):\n",
    "\n",
    "        letter_pos = np.where(one_hot_representation[:,i] == 1.0)[0][0]\n",
    "        actual_character_sequence.append(unique_chars_list[letter_pos])\n",
    "\n",
    "    return actual_character_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_endoding(x, K):\n",
    "    \"\"\"\n",
    "    Creates the one hot encoding representation of an array.\n",
    "\n",
    "\n",
    "    :param x: The array that we wish to map in an one-hot representation.\n",
    "    :param K: The number of distinct classes.\n",
    "\n",
    "    :return: One hot representation of this number.\n",
    "    \"\"\"\n",
    "\n",
    "    x_encoded = np.zeros((K, len(x)))\n",
    "    for index, elem in enumerate(x):\n",
    "\n",
    "        x_encoded[elem, index] = 1.0\n",
    "\n",
    "    return x_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    :param X: ND-Array. Probably should be floats.\n",
    "    :param theta: float parameter, used as a multiplier prior to exponentiation. Default = 1.0\n",
    "    :param axis (optional): axis to compute values along. Default is the first non-singleton axis.\n",
    "\n",
    "    :return: An array the same size as X. The result will sum to 1 along the specified axis\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_smoothed_loss(smoothed_loss, display=False, title=None, save_name=None, save_path='../figures/'):\n",
    "    \"\"\"\n",
    "        Visualization and saving the loss of the network.\n",
    "\n",
    "        :param smoothed_loss: The smooth loss of the RNN network.\n",
    "        :param display: (Optional) Boolean, set to True for displaying the loss evolution plot.\n",
    "        :param title: (Optional) Title of the plot.\n",
    "        :param save_name: (Optional) name of the file to save the plot.\n",
    "        :param save_path: (Optional) Path of the folder to save the plot in your local computer.\n",
    "\n",
    "        :return: None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.plot(smoothed_loss)\n",
    "\n",
    "    if save_name is not None:\n",
    "        if save_path[-1] != '/':\n",
    "            save_path += '/'\n",
    "        plt.savefig(save_path + save_name + '.png')\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Recurrent Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m, K, eta, seq_length, std, epsilon=1e-10):\n",
    "        \"\"\"\n",
    "        Initial setting of the RNN.\n",
    "\n",
    "        :param m: Dimensionality of the hidden state.\n",
    "        :param K: Number of unique classes to identify.\n",
    "        :param eta: The learning rate of the training process.\n",
    "        :param seq_length: The length of the input sequence.\n",
    "        :param std: the variance of the normal distribution that initializes the weight matrices.\n",
    "        :param epsilon: epsilon parameter of ada-grad.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m = m\n",
    "        self.K = K\n",
    "        self.eta = eta\n",
    "        self.seq_length = seq_length\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and bias matrices\n",
    "        \"\"\"\n",
    "\n",
    "        U = np.random.normal(0, self.std, size=(self.m, self.K))\n",
    "        W = np.random.normal(0, self.std, size=(self.m, self.m))\n",
    "        V = np.random.normal(0, self.std, size=(self.K, self.m))\n",
    "\n",
    "        b = np.zeros((self.m, 1))\n",
    "        c = np.zeros((self.K, 1))\n",
    "\n",
    "        return [W, U, b, V, c]\n",
    "\n",
    "    def synthesize_sequence(self, h0, x0, weight_parameters, text_length):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters under the RNN values.\n",
    "\n",
    "        :param self: The RNN.\n",
    "        :param h0: Hidden state at time 0.\n",
    "        :param x0: First (dummy) input vector of the RNN.\n",
    "        :param weight_parameters: The weighst and biases of the RNN, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "        :param text_length: The length of the text you wish to generate\n",
    "\n",
    "        :return: Synthesized text through.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "        Y = np.zeros(shape=(x0.shape[0], text_length))\n",
    "\n",
    "        alpha = np.dot(W, h0) + np.dot(U, np.expand_dims(x0[:,0], axis=1)) + b\n",
    "        h = np.tanh(alpha)\n",
    "        o = np.dot(V, h) + c\n",
    "        p = softmax(o)\n",
    "\n",
    "        # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "        cumulative_sum = np.cumsum(p)\n",
    "        draw_number = np.random.sample()\n",
    "\n",
    "        # Find the element that corresponds to this random sample\n",
    "        pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "        # Create one-hot representation of the found position\n",
    "        Y[pos, 0] = 1.0\n",
    "\n",
    "        h0 = np.copy(h)\n",
    "        x0 = np.expand_dims(np.copy(Y[:,0]), axis=1)\n",
    "\n",
    "        for index in range(1, text_length):\n",
    "\n",
    "            alpha = np.dot(W, h0) + np.dot(U, x0) + b\n",
    "            h = np.tanh(alpha)\n",
    "            o = np.dot(V, h) + c\n",
    "            p = softmax(o)\n",
    "\n",
    "            # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "            cumulative_sum = np.cumsum(p)\n",
    "            draw_number = np.random.sample()\n",
    "\n",
    "            # Find the element that corresponds to this random sample\n",
    "            pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "            # Create one-hot representation of the found position\n",
    "            Y[pos, index] = 1.0\n",
    "\n",
    "            h0 = np.copy(h)\n",
    "            x0 = np.expand_dims(np.copy(Y[:, index]), axis=1)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def ComputeLoss(self, input_sequence, Y, weight_parameters):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss of the RNN.\n",
    "\n",
    "        :param input_sequence: The input sequence.\n",
    "        :param weight_parameters: Weights and matrices of the RNN, which in particularly are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "\n",
    "        :return: Cross entropy loss (divergence between the predictions and the true output)\n",
    "        \"\"\"\n",
    "\n",
    "        p = self.ForwardPass(input_sequence, weight_parameters)[2]\n",
    "\n",
    "        cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum()\n",
    "\n",
    "        return cross_entropy_loss\n",
    "\n",
    "    def ForwardPass(self, input_sequence, weight_parameters, h0 = None):\n",
    "        \"\"\"\n",
    "        Evaluates the predictions that the RNN does in an input character sequence.\n",
    "\n",
    "        :param input_sequence: The one-hot representation of the input sequence.\n",
    "        :param weight_parameters: The weights and biases of the network, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "        :param h0: Initial hidden state value.\n",
    "\n",
    "        :return: The predicted character sequence based on the input one.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "\n",
    "        p = np.zeros(input_sequence.shape)\n",
    "        if h0 is None:\n",
    "            h_list = [np.zeros((self.m, 1))]\n",
    "        else:\n",
    "            h_list = [h0]\n",
    "\n",
    "        a_list = []\n",
    "\n",
    "        for index in range(0, input_sequence.shape[1]):\n",
    "\n",
    "            alpha = np.dot(W, h_list[-1]) + np.dot(U, np.expand_dims(input_sequence[:,index], axis=1)) + b\n",
    "            a_list.append(alpha)\n",
    "            h = np.tanh(alpha)\n",
    "            h_list.append(h)\n",
    "            o = np.dot(V, h) + c\n",
    "            p[:, index] = softmax(o).reshape(p.shape[0],)\n",
    "\n",
    "        return a_list, h_list[1:], p\n",
    "\n",
    "    def BackwardPass(self, x, Y, p, W, V, a, h, with_clipping=True):\n",
    "        \"\"\"\n",
    "        Computes the gradient updates of the network's weight and bias matrices based on the divergence between the\n",
    "        prediction and the true output.\n",
    "\n",
    "        :param x: Input data to the network.\n",
    "        :param p: Predictions of the network.\n",
    "        :param Y: One-hot representation of the correct sequence.\n",
    "        :param W: Hidden-to-Hidden weight matrix.\n",
    "        :param V: Hidden-to-Output weight matrix.\n",
    "        :param a: Hidden states before non-linearity.\n",
    "        :param h: Hidden states of the network at each time step.\n",
    "        :param with_clipping: (Optional) Set to False for not clipping in he gradients\n",
    "\n",
    "        :return:  Gradient updates.\n",
    "        \"\"\"\n",
    "\n",
    "        # grad_W, grad_U, grad_b, grad_V, grad_c = \\\n",
    "        #     np.zeros(W.shape), np.zeros((W.shape[0], x.shape[1])), np.zeros((W.shape[0], 1)), np.zeros(V), np.zeros((x.shape[0], 1))\n",
    "\n",
    "        # Computing the gradients for the last time step\n",
    "\n",
    "        grad_c = np.expand_dims((p[:, x.shape[1]- 1] - Y[:, x.shape[1]- 1]).T, axis=1)\n",
    "        grad_V = np.dot(grad_c, h[-1].T)\n",
    "\n",
    "        grad_h = np.dot(V.T, grad_c)\n",
    "\n",
    "        grad_b = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[-1].reshape((a[-1].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "        grad_W = np.dot(grad_b, h[-2].T)\n",
    "        grad_U = np.dot(grad_b, np.expand_dims(x[:,-1], axis=0))\n",
    "\n",
    "        grad_a = grad_b\n",
    "\n",
    "        for time_step in reversed(range(x.shape[1]- 1)):\n",
    "\n",
    "            grad_o = np.expand_dims((p[:, time_step] - Y[:, time_step]).T, axis=1)\n",
    "            grad_V += np.dot(grad_o, np.transpose(h[time_step]))\n",
    "            grad_c += grad_o\n",
    "\n",
    "            grad_h = np.dot(V.T, grad_o) + np.dot(grad_a.T, W).T\n",
    "\n",
    "            grad_a = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[time_step].reshape((a[time_step].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "            grad_W += np.dot(grad_a, h[time_step-1].T)\n",
    "            grad_U += np.dot(grad_a, np.expand_dims(x[:,time_step], axis=1).T)\n",
    "\n",
    "            grad_b += grad_a\n",
    "\n",
    "        gradients = [grad_W, grad_U, grad_b, grad_V, grad_c]\n",
    "\n",
    "        if with_clipping:\n",
    "\n",
    "            for index, elem in enumerate(gradients):\n",
    "\n",
    "                gradients[index] = np.maximum(-5, np.minimum(elem, 5))\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def initialize_ada_grad(self, weight_parameters):\n",
    "        \"\"\"\n",
    "        Initializes the ada_grads of the weights parameters.\n",
    "\n",
    "        :param weight_parameters: The weights and biases of the RNN\n",
    "\n",
    "        :return: Initialized ada-grad parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        ada_grads = []\n",
    "\n",
    "        for elem in weight_parameters:\n",
    "\n",
    "            ada_grads.append(np.zeros(shape=elem.shape))\n",
    "\n",
    "        return ada_grads\n",
    "\n",
    "    def ada_grad_update(self, weight_parameters, ada_grads, gradients, eta):\n",
    "        \"\"\"\n",
    "        Conducts one update step of the ada-grants and weights parameters based on the currently estimated gradient updates.\n",
    "\n",
    "        :param weight_parameters: Weights and biases of the RNN network.\n",
    "        :param ada_grads: Ada grad parameters.\n",
    "        :param gradients: Gradient updates of a training step.\n",
    "        :param eta: Learning rate of the training process.\n",
    "\n",
    "        :return: Updated weight and ada_grad parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update ada-grads\n",
    "        for index, elem in enumerate(ada_grads):\n",
    "\n",
    "            elem += gradients[index] ** 2\n",
    "\n",
    "        for index, weight_elem in enumerate(weight_parameters):\n",
    "\n",
    "            weight_elem -= eta * gradients[index] / np.sqrt(ada_grads[index] + self.epsilon)\n",
    "\n",
    "        return weight_parameters, ada_grads\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, epoches, unique_characters, verbose=True):\n",
    "        \"\"\"\n",
    "        Comnducts the training pprocess of the RNN nad estimates the model.\n",
    "\n",
    "        :param X: Input data (one-hot representation).\n",
    "        :param Y: Treu labels (one-hot representation).\n",
    "        :param epoches: Number of training epochs.\n",
    "        :param unique_characters: The unique characters that can be generated from the training process.\n",
    "\n",
    "        :return: The trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        weight_parameters = self.init_weights()\n",
    "        gradient_object = Gradients(self)\n",
    "\n",
    "        # Number of distinct training sequences per epoch\n",
    "        training_sequences_per_epoch = X.shape[1] - self.seq_length\n",
    "\n",
    "        ada_grads = self.initialize_ada_grad(weight_parameters)\n",
    "\n",
    "        for epoch in range(epoches):\n",
    "\n",
    "            hprev = np.zeros(shape=(self.m, 1))\n",
    "            \n",
    "            if epoch == 0 and verbose:\n",
    "                \n",
    "                synthesized_text = Ind_to_Char(self.synthesize_sequence(h0=hprev, x0=X, weight_parameters=weight_parameters, text_length=200), unique_characters)\n",
    "                print('---------------------------------------------------------')\n",
    "                print(f'Synthesized before any update step:')\n",
    "                print(''.join(synthesized_text))\n",
    "\n",
    "            for e in range(training_sequences_per_epoch):\n",
    "                \n",
    "                current_update_step = epoch * training_sequences_per_epoch + e\n",
    "\n",
    "                x = X[:, e:e + self.seq_length]\n",
    "                y = Y[:, e + 1:e + self.seq_length + 1]\n",
    "\n",
    "                gradient_updates, hprev = gradient_object.ComputeGradients(x, y, weight_parameters, hprev)\n",
    "\n",
    "                weight_parameters, ada_grads = self.ada_grad_update(weight_parameters, ada_grads, gradient_updates, eta=self.eta)\n",
    "\n",
    "                if epoch ==0 and e ==0 :\n",
    "                    smooth_loss_evolution = [self.ComputeLoss(x, y, weight_parameters)]\n",
    "                    minimum_loss = smooth_loss_evolution[0]\n",
    "\n",
    "                else:\n",
    "                    current_loss = 0.999 * smooth_loss_evolution[-1] + 0.001 * self.ComputeLoss(x, y, weight_parameters)\n",
    "                    smooth_loss_evolution.append(current_loss)\n",
    "                    if current_loss < minimum_loss:\n",
    "                        best_weights = weight_parameters\n",
    "                    best_h_prev = hprev\n",
    "\n",
    "                if verbose:\n",
    "                    \n",
    "                    if len(smooth_loss_evolution) % 1000 == 0 and len(smooth_loss_evolution) > 0:\n",
    "                        print('---------------------------------------------------------')\n",
    "                        print(f'Smooth loss at update step no.{current_update_step}: {smooth_loss_evolution[-1]}')\n",
    "    \n",
    "                        # Also generate synthesized text if 500 updates steps have been conducted\n",
    "                        if len(smooth_loss_evolution) % 10000 == 0 and len(smooth_loss_evolution) > 0:\n",
    "    \n",
    "                            synthesized_text = Ind_to_Char(self.synthesize_sequence(h0=hprev, x0=X, weight_parameters=weight_parameters, text_length=200), unique_characters)\n",
    "                            print('---------------------------------------------------------')\n",
    "                            print(f'Synthesized text of update step no.{current_update_step}')\n",
    "                            print(''.join(synthesized_text))\n",
    "                    \n",
    "                if with_break and current_update_step == 100000:\n",
    "                    return best_weights, best_h_prev, smooth_loss_evolution                     \n",
    "\n",
    "        return best_weights, best_h_prev, smooth_loss_evolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradients:\n",
    "\n",
    "    def __init__(self, RNN):\n",
    "        self.RNN = RNN\n",
    "\n",
    "    def ComputeGradients(self, X, Y, weight_parameters, hprev, with_clipping=True):\n",
    "        \"\"\"\n",
    "        Computes the analytical gradient updates of the network.\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "        :param hprev: Initial hidden state to be used in the forward and backwward pass of the RNN.\n",
    "        :param with_clipping: (Optional) Set ot False if you don't wish to apply clipping in the gradients.\n",
    "\n",
    "        :return: Gradients updates.\n",
    "        \"\"\"\n",
    "\n",
    "        a_list, h_list, p = self.RNN.ForwardPass(X, weight_parameters, h0=hprev)\n",
    "        gradients = self.RNN.BackwardPass(X, Y, p, weight_parameters[0], weight_parameters[3], a_list, h_list, with_clipping)\n",
    "\n",
    "        return gradients, h_list[0]\n",
    "\n",
    "    def ComputeGradsNumSlow(self, X, Y, weight_parameters, h=1e-4):\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        all_grads_num = []\n",
    "\n",
    "        for index, elem in enumerate(weight_parameters):\n",
    "\n",
    "            grad_elem = np.zeros(elem.shape)\n",
    "            # h_prev = np.zeros((W.shape[1], 1))\n",
    "\n",
    "            for i in tqdm(range(elem.shape[0])):\n",
    "                for j in range(elem.shape[1]):\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] -= h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c1 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] += h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c2 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    grad_elem[i, j] = (c2-c1) / (2*h)\n",
    "\n",
    "            all_grads_num.append(grad_elem)\n",
    "\n",
    "        return all_grads_num\n",
    "\n",
    "    def check_similarity(self, X, Y, weight_parameters, with_cliping = False):\n",
    "        \"\"\"\n",
    "        Computes and compares the analytical and numerical gradients.\n",
    "\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "        :param with_cliping: (Optional) Set to True to apply clipping in the gradients\n",
    "\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "\n",
    "        analytical_gradients, _ = self.ComputeGradients(X, Y, weight_parameters, hprev=np.zeros(shape=(self.RNN.m, 1)))\n",
    "        numerical_gradients = self.ComputeGradsNumSlow(X, Y, weight_parameters)\n",
    "\n",
    "        for weight_index in range(len(analytical_gradients)):\n",
    "            print('-----------------')\n",
    "            print(f'Weight parameter no. {weight_index+1}:')\n",
    "\n",
    "            weight_abs = np.abs(analytical_gradients[weight_index] - numerical_gradients[weight_index])\n",
    "\n",
    "            weight_nominator = np.average(weight_abs)\n",
    "\n",
    "            grad_weight_abs = np.absolute(analytical_gradients[weight_index])\n",
    "            grad_weight_num_abs = np.absolute(numerical_gradients[weight_index])\n",
    "\n",
    "            sum_weight = grad_weight_abs + grad_weight_num_abs\n",
    "\n",
    "            print(f'Deviation between analytical and numerical gradients: {weight_nominator / np.amax(sum_weight)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data, unique_characters = Load_Text_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Set hyper-parameters & initialize the RNN’s parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.1)\n",
    "\n",
    "weight_parameters = rnn.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Synthesize text from your randomly initialized RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6}Cf}KL9D qSWzI;)Jv}J\"!'A\n"
     ]
    }
   ],
   "source": [
    "input_sequence = book_data[:rnn.seq_length]\n",
    "\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "test = rnn.synthesize_sequence(h0=np.zeros((rnn.m, 1)), x0=input_sequence_one_hot, weight_parameters=weight_parameters, text_length=rnn.seq_length)\n",
    "test2 = Ind_to_Char(test, unique_characters)\n",
    "print(''.join(test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Implement the forward and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vallidate the correct performance of the forward and backward pass by comparing with the numerical gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data, unique_characters = Load_Text_Data()\n",
    "\n",
    "rnn_object = RNN(m=5, K=len(unique_characters), eta=0.01, seq_length=25, std=0.01)\n",
    "gradient_object = Gradients(rnn_object)\n",
    "\n",
    "weight_parameters = rnn_object.init_weights()\n",
    "\n",
    "input_sequence = book_data[:rnn_object.seq_length]\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "output_sequence = book_data[1:1 + rnn_object.seq_length]\n",
    "output_encoding = Char_to_Ind(output_sequence, unique_characters)\n",
    "output_sequence_one_hot = create_one_hot_endoding(output_encoding, len(unique_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 79.17it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.70it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 398.28it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 102.19it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 508.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Weight parameter no. 1:\n",
      "Deviation between analytical and numerical gradients: 0.01530391399828311\n",
      "-----------------\n",
      "Weight parameter no. 2:\n",
      "Deviation between analytical and numerical gradients: 9.347829544732643e-11\n",
      "-----------------\n",
      "Weight parameter no. 3:\n",
      "Deviation between analytical and numerical gradients: 6.028087082473465e-10\n",
      "-----------------\n",
      "Weight parameter no. 4:\n",
      "Deviation between analytical and numerical gradients: 8.20341705132328e-10\n",
      "-----------------\n",
      "Weight parameter no. 5:\n",
      "Deviation between analytical and numerical gradients: 6.804348250238339e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gradient_object.check_similarity(input_sequence_one_hot, output_sequence_one_hot, weight_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Train your RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include a graph of the smooth loss function for a longish training run (at least 2 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.1)\n",
    "\n",
    "# Create one-hot data\n",
    "integer_encoding = Char_to_Ind(book_data, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "output_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "weight_parameters, smoothed_loss_evolution, h_prev = rnn.fit(X=input_sequence_one_hot, Y=output_sequence_one_hot, epoches=3, unique_characters=unique_characters, verbose=False)\n",
    "inform_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the smooth loss evolution for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_smoothed_loss(smoothed_loss_evolution, display=True, title='Smooth loss evolution', save_name='sm_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum loss achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.37968904174163"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(smoothed_loss_evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generated by the learnt weights parameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ermars tighe, ins belento the was iplowl the Harryon Yourbed modont \"y hise Dabllemelled Molfic artin, Indey pparee toryy's ofirg?\" shi ghefoin Mrtaco mone, snst\n",
      "her yo?  Harged fery,\"'stor, of oo tia\n"
     ]
    }
   ],
   "source": [
    "synthesized_text = Ind_to_Char(rnn.synthesize_sequence(np.zeros((rnn.m, 1)), input_sequence_one_hot, weight_parameters=weight_parameters, text_length=200), unique_characters)\n",
    "print(''.join(synthesized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Show the evolution of the text synthesized by your RNN during training by including a sample of synthesized text (200 characters long) before the first and before every 10,000th update steps when you train for 100,000 update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Synthesized before any update step:\n",
      "UWS/eg()B1c.iwlN-'rEgT;^6 3SXbXwN4aTVK.)YL'DTq)o_dFZu:p .SH\t )XXI;\"'-pJ)0JgWf-Hb)Nc')-EeL\"6•\n",
      "Mcn'EmQGqpIZKAd4dK'Bqak14b(F•l)IxVbQwXF7Vgu\n",
      "Qda69IzVUVNhIw)d_H^tLF!cihda:nB(X.rbCjanDWkU}^h9Xh67Pwn2qü4P3,N\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.999: 83.66097476020666\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.1999: 82.84507987344679\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.2999: 84.51008235555841\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.3999: 84.64622987842108\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.4999: 84.36886412990972\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.5999: 85.24579024098642\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.6999: 82.96009413788748\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.7999: 85.5126193876621\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.8999: 84.8840470477676\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.9999: 86.45902606321714\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.9999\n",
      "aids  hoccd lcoe oirg theek toe datousd afee the s is anoun  hork\n",
      "geiy hored thrts taedyith  hthaiQ\"\tile s oimn Bare, ohe leepe t srogc many th tcs ha kat rf se coandekorsedenans no tof tils brregbod \n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.10999: 86.43989930690758\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.11999: 85.68300057437753\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.12999: 83.47310848186181\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.13999: 83.9480466628181\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.14999: 84.20615499976303\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.15999: 82.38693734659442\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.16999: 82.73642597352237\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.17999: 79.59218368487292\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.18999: 76.50977956343634\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.19999: 75.61225219720365\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.19999\n",
      "oing baed blvnscond woe itadaithenaec noa theak acdks oled ius ilas  sas, Ansttsthe dilk dais thindinbes was au besasnkealidt ho the ed and  and danf ytag atlag piag, brlt ans  won gard maid thas aire\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.20999: 77.67297845306604\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.21999: 77.43086353030996\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.22999: 76.66571569238265\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.23999: 74.50189769931296\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.24999: 75.77121985428158\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.25999: 75.20064929070051\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.26999: 75.54817486849693\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.27999: 76.96339766963077\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.28999: 77.55956644819523\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.29999: 75.71868911382859\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.29999\n",
      " pre her mamd hge woflerth ng, baad aad  hat  aonemichee rllther )int ware; or aultswid Vardeirs thire Has-cad aonord any ourtes ist Badlsircrernm Vom Vfon?- Ye for thot ore w-on, hTwoy oac hou hin , \n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.30999: 74.34346666031642\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.31999: 76.83591983769738\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.32999: 76.96548060285181\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.33999: 79.01635771359217\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.34999: 77.60337432761973\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.35999: 75.36810194537743\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.36999: 72.12054333325001\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.37999: 72.1309662749947\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.38999: 74.55943338995503\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.39999: 76.52132444243982\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.39999\n",
      "ed Pitvor and, af -o fht -ain asos .o .N I Tho wayce  at wurass ser-o woupe ;ird w core ther anditiid, res che dousaefout  au hee.. RA\n",
      "\tr t ruNg.n ;har realb padpaced or tobemrewed wlhovllagd maerod  \n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.40999: 76.5116701914078\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.41999: 73.60141805868737\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.42999: 72.16694181334049\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.43999: 73.6512270342631\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.44999: 72.20974526273528\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.45999: 73.8748980837328\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.46999: 73.84627362061755\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.47999: 72.05502777240432\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.48999: 72.53678879044631\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.49999: 73.60884423301833\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.49999\n",
      "ornkanghe haldn rrnd rery cheathe foo hag lirpewagll Hack rouikrod seWmiterefuwache f th ingoate Bhe hat wod croregrtoags oh  Hooris and song totl lagos sisting.  Harne aRlllemun'y  wre hiUk le ticulb\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.50999: 72.87276911714424\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.51999: 73.72279389795021\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.52999: 71.30465069917669\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.53999: 70.4324328741515\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.54999: 70.79497714231157\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.55999: 71.85197296546541\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.56999: 71.22789984571736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.57999: 70.89914179206784\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.58999: 70.64531551641537\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.59999: 68.48476724967202\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.59999\n",
      "o anleofe leig loving o.  Hcuradl wor aladdey yet le tous mrerousd nas aachiit the atwushine temou, Bus her s, HaothevTun wy azidarup seeescath  blevc, Mls und uisivehe Modin's ser nt syrdwebeagimt.\n",
      "\n",
      "\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.60999: 69.10159357573067\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.61999: 67.30623036816868\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.62999: 69.5881716836414\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.63999: 70.12325312056244\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.64999: 72.67201023623777\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.65999: 70.78644028533806\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.66999: 70.02490498566546\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.67999: 68.3005250963309\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.68999: 67.29874600733842\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.69999: 66.92709288361775\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.69999\n",
      "e nnsttho Ugeryy Vevfhrucind borse. Hercey, l atrin sthe Parlen'st ing \"theain seos la, anwopUche dhan beyillikstMeg warl, hom nin's blo.l hareid lead fote tanthey sheuy nr ydalae hon\" has nt as in ou\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.70999: 66.75998236168661\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.71999: 66.51858826967171\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.72999: 66.2530506507692\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.73999: 67.9248430481796\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.74999: 65.28009802363992\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.75999: 68.24328717487472\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.76999: 66.77460304009041\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.77999: 66.02366814135455\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.78999: 66.55989250264344\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.79999: 67.4570987236666\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.79999\n",
      "ofber?\"hR lad on't, wasl. Radly Dued,\"\"Wey lons,\n",
      "Aul.\n",
      "Werdle \"e Hurt?\"\" Beruthe \"has wir, te. p(her tlotpe vousd \"asily on GyRe lemhind ings tre b uug. \"eth ser war\" Sangithe tI thong fowem, hasle lre\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.80999: 67.02343134197979\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.81999: 68.00747503303859\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.82999: 68.5500670663354\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.83999: 67.50855814500821\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.84999: 68.15906750237136\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.85999: 68.0986112641872\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.86999: 66.59044921566624\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.87999: 64.97486913001956\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.88999: 66.50489228687931\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.89999: 64.83449757209283\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.89999\n",
      "etiA s outt eo towadles lharsinghy pGared ghece cit -inj wa cocnd reverkep sHaughe (ferexseicwt an deot dove gors uplys thed th o .-.\"\n",
      "HsercanPwered bops outk nc,eshe cot hind tickyty Larrmy\" eurd tho\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.90999: 65.34728373695808\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.91999: 67.82076165971421\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.92999: 68.1806911466928\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.93999: 67.96988919615036\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.94999: 68.32639576887293\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.95999: 65.60711782517792\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.96999: 67.48079642336705\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.97999: 64.80433658867777\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.98999: 65.08213983403245\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.99999: 66.23851114199759\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.99999\n",
      "eskt che picly,w, wory nl?s Hint Macdyed aole kidathe the kangoon dtet, AMr cuctumyt at Wher bsaygne tol e tkink. \"He wassyt G,tor beocy sf said cag'tctn leun, Fnagd poug?got in eosidd akes ird s weal\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.1)\n",
    "\n",
    "# Create one-hot data\n",
    "integer_encoding = Char_to_Ind(book_data, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "output_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "weight_parameters, h_prev, smoothed_loss_evolution = rnn.fit(X=input_sequence_one_hot, Y=output_sequence_one_hot, epoches=3, unique_characters=unique_characters, with_break = True)\n",
    "inform_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothed loss evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_smoothed_loss(smoothed_loss_evolution, display=True, title='Smooth loss evolution', save_name='sm_loss_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum loss achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.549296697852895"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(smoothed_loss_evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesized text by best learnt parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oplly Haryeery at ye. \"Hlilint frkenmeded ougithenof Hasravich tiof.\n",
      "\"I ta tlis, the tasgonisal Ais Oue sosp qonyin te therimesryd kisconmist hamppos, ppare nnd ingbundtitklisronedFy serout e'ge s acling tore not enye tligh. \"Herrtredy an niy\" ond buvry,a himemengd nodt me d aic tarsing lure? tho w of rand Iht fo uor'ty fsrprytaocanghe to uasaGy stl trend Mrbla kaliveRind aiig \"ts pigetly..\" Oad .\"\"Woad  her tuwot  The ange ven tiok losgere pose cry. lirksidteas. MPrleaBy glatveaps. eniZ's yy't nep' germir fWyome sundanll, maof themenoleas erronorenin Fhpll seles id tufking thit oo mrf andtyy \"The rankiXg be ftnys relt waog mody. \"Whathinnde saic tont. sivevasr tari'g fur) boutdagrit Dor  as coot we beryer wesralingeun h topertidg shimt and, ourigl ybvyofud het nhd ou thFt oa seot and Donpis ter ehi?g Io yo thos  aeI\", Theckeprot aid oupkape alerey ny rred Me sleng  as ded fofr. blerot ahe. \" aor\"\n",
      "\"erild ted ererored af bots hesmigp.. Hand an  cass pofe therer..\"WSikl., Iso\n",
      " Dhed - sou\n"
     ]
    }
   ],
   "source": [
    "synthesized_text = Ind_to_Char(rnn.synthesize_sequence(h_prev, input_sequence_one_hot, weight_parameters=weight_parameters, text_length=1000), unique_characters)\n",
    "print(''.join(synthesized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
