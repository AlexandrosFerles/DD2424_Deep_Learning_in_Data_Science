{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import everything you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ring a bell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inform_exit():\n",
    "    \n",
    "    os.system('say \"Training process is completed\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Text_Data(file_path='../trump_tweet_data_archive'):\n",
    "    \"\"\"\n",
    "    Reads the input data.\n",
    "\n",
    "    :param file_path: (Optional) Position of the txt file in the local system.\n",
    "\n",
    "    :return: tweets: all input tweets and unique_characters: unique single characters of the input data.\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    for year in range(2009, 2018):\n",
    "        with open(f'../trump_tweet_data_archive/condensed_{year}.json', ) as condensed:\n",
    "            data = json.load(condensed)\n",
    "            for text in range(len(data)):\n",
    "                text = data[text]['text']\n",
    "                text += '±'\n",
    "                tweets.append( text )\n",
    "    all_chars = ''\n",
    "    for tweet in tweets:\n",
    "        all_chars += tweet\n",
    "    unique_characters = list(set(all_chars))\n",
    "    return tweets, unique_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters to Index transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Char_to_Ind(chars_list, unique_chars):\n",
    "    \"\"\"\n",
    "    Maps the original characters to integers.\n",
    "\n",
    "    :param chars_list: The list of characters to be encoded into integers.\n",
    "    :param unique_chars: The set of unique characters available.\n",
    "\n",
    "    :return: A list of integers that correspond to the characters.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_characters = []\n",
    "\n",
    "    for char in chars_list:\n",
    "        for index, letter in enumerate(unique_chars):\n",
    "\n",
    "            if char == letter:\n",
    "\n",
    "                encoded_characters.append(index)\n",
    "\n",
    "    return encoded_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index to Char transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ind_to_Char(one_hot_representation, unique_chars_list):\n",
    "    \"\"\"\n",
    "    Maps a list of integers to their corresponding characters,\n",
    "\n",
    "    :param one_hot_representation: A list of one_hot representations to be transformed to their correspoding characters.\n",
    "    :param unique_chars_list: The list of unique characters.\n",
    "\n",
    "    :return: The actual character sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    actual_character_sequence = []\n",
    "\n",
    "    for i in range(one_hot_representation.shape[1]):\n",
    "\n",
    "        letter_pos = np.where(one_hot_representation[:,i] == 1.0)[0][0]\n",
    "        actual_character_sequence.append(unique_chars_list[letter_pos])\n",
    "\n",
    "    return actual_character_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_endoding(x, K):\n",
    "    \"\"\"\n",
    "    Creates the one hot encoding representation of an array.\n",
    "\n",
    "\n",
    "    :param x: The array that we wish to map in an one-hot representation.\n",
    "    :param K: The number of distinct classes.\n",
    "\n",
    "    :return: One hot representation of this number.\n",
    "    \"\"\"\n",
    "\n",
    "    x_encoded = np.zeros((K, len(x)))\n",
    "    for index, elem in enumerate(x):\n",
    "\n",
    "        x_encoded[elem, index] = 1.0\n",
    "\n",
    "    return x_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    :param X: ND-Array. Probably should be floats.\n",
    "    :param theta: float parameter, used as a multiplier prior to exponentiation. Default = 1.0\n",
    "    :param axis (optional): axis to compute values along. Default is the first non-singleton axis.\n",
    "\n",
    "    :return: An array the same size as X. The result will sum to 1 along the specified axis\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_smoothed_loss(smoothed_loss, display=False, title=None, save_name=None, save_path='../figures/'):\n",
    "    \"\"\"\n",
    "        Visualization and saving the loss of the network.\n",
    "\n",
    "        :param smoothed_loss: The smooth loss of the RNN network.\n",
    "        :param display: (Optional) Boolean, set to True for displaying the loss evolution plot.\n",
    "        :param title: (Optional) Title of the plot.\n",
    "        :param save_name: (Optional) name of the file to save the plot.\n",
    "        :param save_path: (Optional) Path of the folder to save the plot in your local computer.\n",
    "\n",
    "        :return: None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.plot(smoothed_loss)\n",
    "\n",
    "    if save_name is not None:\n",
    "        if save_path[-1] != '/':\n",
    "            save_path += '/'\n",
    "        plt.savefig(save_path + save_name + '.png')\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Recurrent Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m, K, eta, seq_length, std, epsilon=1e-10):\n",
    "        \"\"\"\n",
    "        Initial setting of the RNN.\n",
    "\n",
    "        :param m: Dimensionality of the hidden state.\n",
    "        :param K: Number of unique classes to identify.\n",
    "        :param eta: The learning rate of the training process.\n",
    "        :param seq_length: The length of the input sequence.\n",
    "        :param std: the variance of the normal distribution that initializes the weight matrices.\n",
    "        :param epsilon: epsilon parameter of ada-grad.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m = m\n",
    "        self.K = K\n",
    "        self.eta = eta\n",
    "        self.seq_length = seq_length\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and bias matrices\n",
    "        \"\"\"\n",
    "\n",
    "        U = np.random.normal(0, self.std, size=(self.m, self.K))\n",
    "        W = np.random.normal(0, self.std, size=(self.m, self.m))\n",
    "        V = np.random.normal(0, self.std, size=(self.K, self.m))\n",
    "\n",
    "        b = np.zeros((self.m, 1))\n",
    "        c = np.zeros((self.K, 1))\n",
    "\n",
    "        return [W, U, b, V, c]\n",
    "\n",
    "    def synthesize_sequence(self, h0, x0, weight_parameters, text_length):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters under the RNN values.\n",
    "\n",
    "        :param self: The RNN.\n",
    "        :param h0: Hidden state at time 0.\n",
    "        :param x0: First (dummy) input vector of the RNN.\n",
    "        :param weight_parameters: The weighst and biases of the RNN, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "        :param text_length: The length of the text you wish to generate\n",
    "\n",
    "        :return: Synthesized text through.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "        Y = np.zeros(shape=(x0.shape[0], text_length))\n",
    "\n",
    "        alpha = np.dot(W, h0) + np.dot(U, np.expand_dims(x0[:,0], axis=1)) + b\n",
    "        h = np.tanh(alpha)\n",
    "        o = np.dot(V, h) + c\n",
    "        p = softmax(o)\n",
    "\n",
    "        # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "        cumulative_sum = np.cumsum(p)\n",
    "        draw_number = np.random.sample()\n",
    "\n",
    "        # Find the element that corresponds to this random sample\n",
    "        pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "        # Create one-hot representation of the found position\n",
    "        Y[pos, 0] = 1.0\n",
    "\n",
    "        h0 = np.copy(h)\n",
    "        x0 = np.expand_dims(np.copy(Y[:,0]), axis=1)\n",
    "\n",
    "        for index in range(1, text_length):\n",
    "\n",
    "            alpha = np.dot(W, h0) + np.dot(U, x0) + b\n",
    "            h = np.tanh(alpha)\n",
    "            o = np.dot(V, h) + c\n",
    "            p = softmax(o)\n",
    "\n",
    "            # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "            cumulative_sum = np.cumsum(p)\n",
    "            draw_number = np.random.sample()\n",
    "\n",
    "            # Find the element that corresponds to this random sample\n",
    "            pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "            # Create one-hot representation of the found position\n",
    "            Y[pos, index] = 1.0\n",
    "\n",
    "            h0 = np.copy(h)\n",
    "            x0 = np.expand_dims(np.copy(Y[:, index]), axis=1)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def ComputeLoss(self, input_sequence, Y, weight_parameters):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss of the RNN.\n",
    "\n",
    "        :param input_sequence: The input sequence.\n",
    "        :param weight_parameters: Weights and matrices of the RNN, which in particularly are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "\n",
    "        :return: Cross entropy loss (divergence between the predictions and the true output)\n",
    "        \"\"\"\n",
    "\n",
    "        p = self.ForwardPass(input_sequence, weight_parameters)[2]\n",
    "\n",
    "        cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum()\n",
    "\n",
    "        return cross_entropy_loss\n",
    "\n",
    "    def ForwardPass(self, input_sequence, weight_parameters, h0 = None):\n",
    "        \"\"\"\n",
    "        Evaluates the predictions that the RNN does in an input character sequence.\n",
    "\n",
    "        :param input_sequence: The one-hot representation of the input sequence.\n",
    "        :param weight_parameters: The weights and biases of the network, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "        :param h0: Initial hidden state value.\n",
    "\n",
    "        :return: The predicted character sequence based on the input one.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "\n",
    "        p = np.zeros(input_sequence.shape)\n",
    "        if h0 is None:\n",
    "            h_list = [np.zeros((self.m, 1))]\n",
    "        else:\n",
    "            h_list = [h0]\n",
    "\n",
    "        a_list = []\n",
    "\n",
    "        for index in range(0, input_sequence.shape[1]):\n",
    "\n",
    "            alpha = np.dot(W, h_list[-1]) + np.dot(U, np.expand_dims(input_sequence[:,index], axis=1)) + b\n",
    "            a_list.append(alpha)\n",
    "            h = np.tanh(alpha)\n",
    "            h_list.append(h)\n",
    "            o = np.dot(V, h) + c\n",
    "            p[:, index] = softmax(o).reshape(p.shape[0],)\n",
    "\n",
    "        return a_list, h_list[1:], p\n",
    "\n",
    "    def BackwardPass(self, x, Y, p, W, V, a, h, with_clipping=True):\n",
    "        \"\"\"\n",
    "        Computes the gradient updates of the network's weight and bias matrices based on the divergence between the\n",
    "        prediction and the true output.\n",
    "\n",
    "        :param x: Input data to the network.\n",
    "        :param p: Predictions of the network.\n",
    "        :param Y: One-hot representation of the correct sequence.\n",
    "        :param W: Hidden-to-Hidden weight matrix.\n",
    "        :param V: Hidden-to-Output weight matrix.\n",
    "        :param a: Hidden states before non-linearity.\n",
    "        :param h: Hidden states of the network at each time step.\n",
    "        :param with_clipping: (Optional) Set to False for not clipping in he gradients\n",
    "\n",
    "        :return:  Gradient updates.\n",
    "        \"\"\"\n",
    "\n",
    "        # grad_W, grad_U, grad_b, grad_V, grad_c = \\\n",
    "        #     np.zeros(W.shape), np.zeros((W.shape[0], x.shape[1])), np.zeros((W.shape[0], 1)), np.zeros(V), np.zeros((x.shape[0], 1))\n",
    "\n",
    "        # Computing the gradients for the last time step\n",
    "\n",
    "        grad_c = np.expand_dims((p[:, x.shape[1]- 1] - Y[:, x.shape[1]- 1]).T, axis=1)\n",
    "        grad_V = np.dot(grad_c, h[-1].T)\n",
    "\n",
    "        grad_h = np.dot(V.T, grad_c)\n",
    "\n",
    "        grad_b = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[-1].reshape((a[-1].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "        grad_W = np.dot(grad_b, h[-2].T)\n",
    "        grad_U = np.dot(grad_b, np.expand_dims(x[:,-1], axis=0))\n",
    "\n",
    "        grad_a = grad_b\n",
    "\n",
    "        for time_step in reversed(range(x.shape[1]- 1)):\n",
    "\n",
    "            grad_o = np.expand_dims((p[:, time_step] - Y[:, time_step]).T, axis=1)\n",
    "            grad_V += np.dot(grad_o, np.transpose(h[time_step]))\n",
    "            grad_c += grad_o\n",
    "\n",
    "            grad_h = np.dot(V.T, grad_o) + np.dot(grad_a.T, W).T\n",
    "\n",
    "            grad_a = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[time_step].reshape((a[time_step].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "            grad_W += np.dot(grad_a, h[time_step-1].T)\n",
    "            grad_U += np.dot(grad_a, np.expand_dims(x[:,time_step], axis=1).T)\n",
    "\n",
    "            grad_b += grad_a\n",
    "\n",
    "        gradients = [grad_W, grad_U, grad_b, grad_V, grad_c]\n",
    "\n",
    "        if with_clipping:\n",
    "\n",
    "            for index, elem in enumerate(gradients):\n",
    "\n",
    "                gradients[index] = np.maximum(-5, np.minimum(elem, 5))\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def initialize_ada_grad(self, weight_parameters):\n",
    "        \"\"\"\n",
    "        Initializes the ada_grads of the weights parameters.\n",
    "\n",
    "        :param weight_parameters: The weights and biases of the RNN\n",
    "\n",
    "        :return: Initialized ada-grad parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        ada_grads = []\n",
    "\n",
    "        for elem in weight_parameters:\n",
    "\n",
    "            ada_grads.append(np.zeros(shape=elem.shape))\n",
    "\n",
    "        return ada_grads\n",
    "\n",
    "    def ada_grad_update(self, weight_parameters, ada_grads, gradients, eta):\n",
    "        \"\"\"\n",
    "        Conducts one update step of the ada-grants and weights parameters based on the currently estimated gradient updates.\n",
    "\n",
    "        :param weight_parameters: Weights and biases of the RNN network.\n",
    "        :param ada_grads: Ada grad parameters.\n",
    "        :param gradients: Gradient updates of a training step.\n",
    "        :param eta: Learning rate of the training process.\n",
    "\n",
    "        :return: Updated weight and ada_grad parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update ada-grads\n",
    "        for index, elem in enumerate(ada_grads):\n",
    "\n",
    "            elem += gradients[index] ** 2\n",
    "\n",
    "        for index, weight_elem in enumerate(weight_parameters):\n",
    "\n",
    "            weight_elem -= eta * gradients[index] / np.sqrt(ada_grads[index] + self.epsilon)\n",
    "\n",
    "        return weight_parameters, ada_grads\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, epoches, unique_characters, verbose=True, with_break=False):\n",
    "        \"\"\"\n",
    "        Comnducts the training pprocess of the RNN nad estimates the model.\n",
    "\n",
    "        :param X: Input data (one-hot representation).\n",
    "        :param Y: Treu labels (one-hot representation).\n",
    "        :param epoches: Number of training epochs.\n",
    "        :param unique_characters: The unique characters that can be generated from the training process.\n",
    "        :param verbose: (Optional) Set to False if you do not wish to print synthesized texts during training.\n",
    "        :param with_break: (Optional) Set to True if you want to break after 100 000 update steps.\n",
    "\n",
    "        :return: The trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        weight_parameters = self.init_weights()\n",
    "        gradient_object = Gradients(self)\n",
    "\n",
    "        # Number of distinct training sequences per epoch\n",
    "        training_sequences_per_epoch = X.shape[1] - self.seq_length\n",
    "\n",
    "        ada_grads = self.initialize_ada_grad(weight_parameters)\n",
    "\n",
    "        for epoch in range(epoches):\n",
    "\n",
    "            hprev = np.zeros(shape=(self.m, 1))\n",
    "\n",
    "            if epoch == 0 and verbose:\n",
    "                synthesized_text = Ind_to_Char(\n",
    "                    self.synthesize_sequence(h0=hprev, x0=X, weight_parameters=weight_parameters, text_length=200),\n",
    "                    unique_characters)\n",
    "                print('---------------------------------------------------------')\n",
    "                print(f'Synthesized before any update step:')\n",
    "                print(''.join(synthesized_text))\n",
    "\n",
    "            for e in range(training_sequences_per_epoch):\n",
    "\n",
    "                current_update_step = epoch * training_sequences_per_epoch + e\n",
    "\n",
    "                x = X[:, e:e + self.seq_length]\n",
    "                y = Y[:, e + 1:e + self.seq_length + 1]\n",
    "\n",
    "                gradient_updates, hprev = gradient_object.ComputeGradients(x, y, weight_parameters, hprev)\n",
    "\n",
    "                weight_parameters, ada_grads = self.ada_grad_update(weight_parameters, ada_grads, gradient_updates,\n",
    "                                                                    eta=self.eta)\n",
    "\n",
    "                if epoch == 0 and e == 0:\n",
    "                    smooth_loss_evolution = [self.ComputeLoss(x, y, weight_parameters)]\n",
    "                    minimum_loss = smooth_loss_evolution[0]\n",
    "\n",
    "                else:\n",
    "                    current_loss = 0.999 * smooth_loss_evolution[-1] + 0.001 * self.ComputeLoss(x, y, weight_parameters)\n",
    "                    smooth_loss_evolution.append(current_loss)\n",
    "                    if current_loss < minimum_loss:\n",
    "                        best_weights = weight_parameters\n",
    "                    best_h_prev = hprev\n",
    "\n",
    "                if verbose:\n",
    "\n",
    "                    if len(smooth_loss_evolution) % 1000 == 0 and len(smooth_loss_evolution) > 0:\n",
    "                        print('---------------------------------------------------------')\n",
    "                        print(f'Smooth loss at update step no.{current_update_step}: {smooth_loss_evolution[-1]}')\n",
    "\n",
    "                        # Also generate synthesized text if 500 updates steps have been conducted\n",
    "                        if len(smooth_loss_evolution) % 10000 == 0 and len(smooth_loss_evolution) > 0:\n",
    "                            synthesized_text = Ind_to_Char(\n",
    "                                self.synthesize_sequence(h0=hprev, x0=X, weight_parameters=weight_parameters,\n",
    "                                                         text_length=200), unique_characters)\n",
    "                            print('---------------------------------------------------------')\n",
    "                            print(f'Synthesized text of update step no.{current_update_step}')\n",
    "                            print(''.join(synthesized_text))\n",
    "\n",
    "                if with_break and current_update_step == 100000:\n",
    "                    return best_weights, best_h_prev, smooth_loss_evolution\n",
    "\n",
    "        return best_weights, best_h_prev, smooth_loss_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradients:\n",
    "\n",
    "    def __init__(self, RNN):\n",
    "        self.RNN = RNN\n",
    "\n",
    "    def ComputeGradients(self, X, Y, weight_parameters, hprev, with_clipping=True):\n",
    "        \"\"\"\n",
    "        Computes the analytical gradient updates of the network.\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "        :param hprev: Initial hidden state to be used in the forward and backwward pass of the RNN.\n",
    "        :param with_clipping: (Optional) Set ot False if you don't wish to apply clipping in the gradients.\n",
    "\n",
    "        :return: Gradients updates.\n",
    "        \"\"\"\n",
    "\n",
    "        a_list, h_list, p = self.RNN.ForwardPass(X, weight_parameters, h0=hprev)\n",
    "        gradients = self.RNN.BackwardPass(X, Y, p, weight_parameters[0], weight_parameters[3], a_list, h_list, with_clipping)\n",
    "\n",
    "        return gradients, h_list[0]\n",
    "\n",
    "    def ComputeGradsNumSlow(self, X, Y, weight_parameters, h=1e-4):\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        all_grads_num = []\n",
    "\n",
    "        for index, elem in enumerate(weight_parameters):\n",
    "\n",
    "            grad_elem = np.zeros(elem.shape)\n",
    "            # h_prev = np.zeros((W.shape[1], 1))\n",
    "\n",
    "            for i in tqdm(range(elem.shape[0])):\n",
    "                for j in range(elem.shape[1]):\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] -= h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c1 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] += h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c2 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    grad_elem[i, j] = (c2-c1) / (2*h)\n",
    "\n",
    "            all_grads_num.append(grad_elem)\n",
    "\n",
    "        return all_grads_num\n",
    "\n",
    "    def check_similarity(self, X, Y, weight_parameters, with_cliping = False):\n",
    "        \"\"\"\n",
    "        Computes and compares the analytical and numerical gradients.\n",
    "\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "        :param with_cliping: (Optional) Set to True to apply clipping in the gradients\n",
    "\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "\n",
    "        analytical_gradients, _ = self.ComputeGradients(X, Y, weight_parameters, hprev=np.zeros(shape=(self.RNN.m, 1)))\n",
    "        numerical_gradients = self.ComputeGradsNumSlow(X, Y, weight_parameters)\n",
    "\n",
    "        for weight_index in range(len(analytical_gradients)):\n",
    "            print('-----------------')\n",
    "            print(f'Weight parameter no. {weight_index+1}:')\n",
    "\n",
    "            weight_abs = np.abs(analytical_gradients[weight_index] - numerical_gradients[weight_index])\n",
    "\n",
    "            weight_nominator = np.average(weight_abs)\n",
    "\n",
    "            grad_weight_abs = np.absolute(analytical_gradients[weight_index])\n",
    "            grad_weight_num_abs = np.absolute(numerical_gradients[weight_index])\n",
    "\n",
    "            sum_weight = grad_weight_abs + grad_weight_num_abs\n",
    "\n",
    "            print(f'Deviation between analytical and numerical gradients: {weight_nominator / np.amax(sum_weight)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data, unique_characters = Load_Text_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Set hyper-parameters & initialize the RNN’s parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.1)\n",
    "\n",
    "weight_parameters = rnn.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Synthesize text from your randomly initialized RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6}Cf}KL9D qSWzI;)Jv}J\"!'A\n"
     ]
    }
   ],
   "source": [
    "input_sequence = book_data[:rnn.seq_length]\n",
    "\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "test = rnn.synthesize_sequence(h0=np.zeros((rnn.m, 1)), x0=input_sequence_one_hot, weight_parameters=weight_parameters, text_length=rnn.seq_length)\n",
    "test2 = Ind_to_Char(test, unique_characters)\n",
    "print(''.join(test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Implement the forward and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vallidate the correct performance of the forward and backward pass by comparing with the numerical gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data, unique_characters = Load_Text_Data()\n",
    "\n",
    "rnn_object = RNN(m=5, K=len(unique_characters), eta=0.01, seq_length=25, std=0.01)\n",
    "gradient_object = Gradients(rnn_object)\n",
    "\n",
    "weight_parameters = rnn_object.init_weights()\n",
    "\n",
    "input_sequence = book_data[:rnn_object.seq_length]\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "output_sequence = book_data[1:1 + rnn_object.seq_length]\n",
    "output_encoding = Char_to_Ind(output_sequence, unique_characters)\n",
    "output_sequence_one_hot = create_one_hot_endoding(output_encoding, len(unique_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 79.17it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.70it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 398.28it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 102.19it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 508.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Weight parameter no. 1:\n",
      "Deviation between analytical and numerical gradients: 0.01530391399828311\n",
      "-----------------\n",
      "Weight parameter no. 2:\n",
      "Deviation between analytical and numerical gradients: 9.347829544732643e-11\n",
      "-----------------\n",
      "Weight parameter no. 3:\n",
      "Deviation between analytical and numerical gradients: 6.028087082473465e-10\n",
      "-----------------\n",
      "Weight parameter no. 4:\n",
      "Deviation between analytical and numerical gradients: 8.20341705132328e-10\n",
      "-----------------\n",
      "Weight parameter no. 5:\n",
      "Deviation between analytical and numerical gradients: 6.804348250238339e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gradient_object.check_similarity(input_sequence_one_hot, output_sequence_one_hot, weight_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Train your RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include a graph of the smooth loss function for a longish training run (at least 2 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.01)\n",
    "\n",
    "# Create one-hot data\n",
    "integer_encoding = Char_to_Ind(book_data, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "output_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "weight_parameters, smoothed_loss_evolution, h_prev = rnn.fit(X=input_sequence_one_hot, Y=output_sequence_one_hot, epoches=3, unique_characters=unique_characters, verbose=False)\n",
    "inform_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the smooth loss evolution for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_smoothed_loss(h_prev, display=True, title='Smooth loss evolution', save_name='sm_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum loss achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.15176033004979"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(h_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generated by the learnt weights parameters is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y, \"Dumbardoon, bestar.  Therey wan.\n",
      "\"Has id, Hagrid, lech sare on they whe whot ever yel me to stundeny betatcotsing.  But a offeder, leet she loture steply to moing must, worly hee liken's dowk It i\n"
     ]
    }
   ],
   "source": [
    "synthesized_text = Ind_to_Char(rnn.synthesize_sequence(smoothed_loss_evolution, input_sequence_one_hot, weight_parameters=weight_parameters, text_length=200), unique_characters)\n",
    "print(''.join(synthesized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Show the evolution of the text synthesized by your RNN during training by including a sample of synthesized text (200 characters long) before the first and before every 10,000th update steps when you train for 100,000 update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Synthesized before any update step:\n",
      "KmInD;Me(R.DdbzDByXSbb•^\"^uSo2)b9,duWe/vmC?eFoeIUNK:4N!ol:\"oEj•J6p_UüOG}RF9AupIv.G;?iFtXBF):nPa401WDP0;c,BotwG:G-std,PfOF'pRFfHHJa^zWewfN)hjkFN)'f7QU\n",
      "7h2rün9GühnBp_HG\n",
      "UU7E3'PZZVfjX\"p^Qq?\t'P16p'2ACebTQ\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.999: 94.90170425180797\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.1999: 85.43124390976648\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.2999: 81.27946399189537\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.3999: 77.15339622157197\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.4999: 73.91599468892989\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.5999: 77.47131140824555\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.6999: 76.41810661654567\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.7999: 73.39839689018854\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.8999: 69.53735134413351\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.9999: 69.37443900750188\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.9999\n",
      "ing, roug cam al thik thinc.. \"o\" Fhang waok  phlrer rallnurlar or tham,  alery ured td antend mohe. \tf ad hle think fpfan.  Frend oy ee therto , arc dank (ithe n\n",
      "dsunde thicg thednc thes the reaf the\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.10999: 67.21312788128476\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.11999: 64.0275403405609\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.12999: 63.50432088975183\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.13999: 64.88173143653769\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.14999: 65.52470610577076\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.15999: 65.71091545904778\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.16999: 65.02054286334914\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.17999: 67.4560878325103\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.18999: 66.10341155863033\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.19999: 61.97599532712699\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.19999\n",
      "on.  Frolk anthea mt masyo' he wrald sumt ittitt tankryan..\"\tOphind wat an rorect, an way coasnHt athe knstls co thinfrishe -As thy and rat af ingigmvilkongo\n",
      "n\"I- wnrtcus hand inithe pad Frann an minc\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.20999: 60.241208108912744\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.21999: 60.23732569688767\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.22999: 60.36607834533919\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.23999: 59.718727293368886\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.24999: 59.50303549917517\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.25999: 57.87566503585283\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.26999: 59.356047430500126\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.27999: 60.79837862432151\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.28999: 59.9607926205079\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.29999: 59.182330873252276\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.29999\n",
      "ly be an tharrsqbore Fryin rodelle the ece ond.  Ha ne thoubls withe wicunt s ancoro, Howas haor hir', Ah ane Hat bees wat urll sunlwchitt Nore han.  Hurking woas tot ium popro cfot anl of obenk tpen \n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.30999: 59.19968434616823\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.31999: 62.54857797157748\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.32999: 62.01440335460598\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.33999: 63.575438075681745\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.34999: 61.99228858470831\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.35999: 60.89134594531514\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.36999: 59.07627843040076\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.37999: 59.59279757437919\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.38999: 62.8277965582598\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.39999: 72.57450315962508\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.39999\n",
      "lol bette harnlin wit aid se souve'k in Ha war hard hh ray Das rak har Pe B wit  tI tha s as wim  a  The tar ho he fad Su whit  how wa th o ko ds s ar ha 'a nennewarg ta we has te cat he fa urd an he \n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.40999: 69.54484181016309\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.41999: 66.07183864358215\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.42999: 63.17489417066686\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.43999: 65.8727744951956\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.44999: 62.434649704748246\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.45999: 64.21951191772649\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.46999: 65.21155637282365\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.47999: 62.39426854366761\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.48999: 61.42845389436116\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.49999: 61.91959189912365\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.49999\n",
      "es leopeantan.  Undacl ho dledlet lo Htad if laiched comlecoue boeclees wame sif the heotn thed sick feinlinge touss nule tiut isto f houlook.\" lo sorDing urow of thaure whan thea lor sson thigghe wan\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.50999: 62.72609634165095\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.51999: 62.93046012961803\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.52999: 61.0977625023377\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.53999: 59.99823088269322\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.54999: 61.761914584127005\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.55999: 63.36352059858804\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.56999: 62.99787447330653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.57999: 65.69446736638152\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.58999: 62.688786079324046\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.59999: 60.34838670788607\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.59999\n",
      "e vit tha undtre st blof tilt; re Drengad at thea win's somt be,\n",
      " hand ay thius in toop to thendly.  Swerat lakce to there kits Qurkiim.  by net Iitn on afing tour inon fo mived tokceuo the rinind to \n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.60999: 60.500594894155\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.61999: 58.56201967167257\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.62999: 60.43890845924481\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.63999: 61.14530090602712\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.64999: 62.78423771238982\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.65999: 60.72269873926506\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.66999: 61.24092310760296\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.67999: 59.8451631497824\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.68999: 58.99710705027517\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.69999: 57.39738717954684\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.69999\n",
      "o anly dedos.\n",
      "Herst.. Harr, Dudedigh seld meided ghe tlecatatt f\"\n",
      "Uncling ang.\n",
      " Theyt dict goome --whery ther csmind inkliscirit trou, ant Mrcenly Vermer, they lasliy rutlancar, Drscidk an\"ld.  rnon l\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.70999: 57.937703585389364\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.71999: 57.83079041689758\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.72999: 58.9105710900516\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.73999: 59.885734017442594\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.74999: 57.945098842863864\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.75999: 63.86272741432277\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.76999: 64.48478681000547\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.77999: 62.230437009425025\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.78999: 63.528687257526066\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.79999: 64.20703182513934\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.79999\n",
      "e slcislrygxs-p toom.\n",
      "\"Bes meinged Bo thad yore.\"\"Sbe singhin's nghinn Died camme,\"Syee-seepbed the! Hised mot ixtasled,\"R veis, thel cace Viughing?\" Thom sy stid irythind Houry tize\"\n",
      "Qgaver pinge hea\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.80999: 62.20031504059947\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.81999: 64.49957072373415\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.82999: 62.71423187053372\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.83999: 61.335181268964696\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.84999: 61.39918499734579\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.85999: 62.53037790806711\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.86999: 61.422877843821816\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.87999: 60.08778386370453\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.88999: 60.82335030789687\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.89999: 59.56707802909729\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.89999\n",
      "a lat!\"\n",
      "H\"I kong fed peand,\n",
      "s ther'.  't and chaved witghithed PonchI Hond Mrmurd.)\"H rugk, arabley.\"\n",
      "\"Og'sing at Shed havd ytowa dpemong ondingo checurry,?\" houmhiny ward,\"F ce tozdey'p siadby bickon\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.90999: 61.0316366696053\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.91999: 62.871118559421824\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.92999: 63.36621775510624\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.93999: 66.0063613418561\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.94999: 65.73407896469455\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.95999: 62.3040771645989\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.96999: 65.56447019951814\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.97999: 61.890816721154586\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.98999: 59.820539809169475\n",
      "---------------------------------------------------------\n",
      "Smooth loss at update step no.99999: 59.888617848764426\n",
      "---------------------------------------------------------\n",
      "Synthesized text of update step no.99999\n",
      "arry'n hid veryild ofordeng ingas hal whid. Rack gath peercescong\n",
      "ith Gouptors of owpottisf.\n",
      "\"Lilged be japsincundemanl -toup wase's Myy arecedly, wesund yam ye's fat.\n",
      "\"Ymine ha'nd anc(id At'cry tapey\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.01)\n",
    "\n",
    "# Create one-hot data\n",
    "integer_encoding = Char_to_Ind(book_data, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "output_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "weight_parameters, h_prev, smoothed_loss_evolution = rnn.fit(X=input_sequence_one_hot, Y=output_sequence_one_hot, epoches=3, unique_characters=unique_characters, with_break = True)\n",
    "inform_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothed loss evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_smoothed_loss(smoothed_loss_evolution, display=True, title='Smooth loss evolution', save_name='sm_loss_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum loss achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.505959327263554"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(smoothed_loss_evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesized text by best learnt parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erey voo gat Rte towet.\"\n",
      "As sposs thtobed hoursss. Weuscieg wit, t poreaned,\"s ded, sewone he pimas ap fpaastl, Un! \"Roig goot R Poug, Rot hid thea's,\" sice koucke tabeer ferly.  Tho gail os ofRivga dey pazats on thingsinpint, saced forly nhedney. wrs, thet, thecly out whing thead had Iped yow souct dica of)paaking al Remourilg,\" hin werly chtole hay re't otton cay.  Hay bid vey, brinkok.\n",
      "\"Yle saim.\n",
      "?\"We fuckint jf am ince pad tovertewest bouve Rist Harry.. Weat licl ubctmettoisiwt. \"Yo warlt tis awh miofand in Harry, the seas?\" h tamled yes Harly,\" has hewe'tter, ang te?v dith Leurdy.\" fat he ppeplaclnd, blit bothen (o keGropsling, foo hiv, cpasllyyapssty'ped Co tice spalfongel. asy'sis the stomKed war! Wpy, pipcevernint Couthe sauthey piig ytonon-d wht und evey ithered dtapgetpyf ove stoteroy inne pot sus'll they apd'd saad on\n",
      "\"Jomt ippey's for yive dari-,\"r tA of t eukl?.\n",
      "\"The daed anon hou lea wad?\"\n",
      "\"\"xessin's pay asleyg.  \"Oid I'?\" cne. .- Weackign, H's saik,\" sou't phemt, tho kre\n"
     ]
    }
   ],
   "source": [
    "synthesized_text = Ind_to_Char(rnn.synthesize_sequence(h_prev, input_sequence_one_hot, weight_parameters=weight_parameters, text_length=1000), unique_characters)\n",
    "print(''.join(synthesized_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
