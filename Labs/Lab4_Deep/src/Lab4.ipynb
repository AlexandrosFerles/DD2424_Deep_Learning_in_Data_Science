{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import everything you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Ring a bell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inform_exit():\n",
    "    \n",
    "    os.system('say \"Training process is completed\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Text_Data(file_path='../goblet_book.txt'):\n",
    "    \"\"\"\n",
    "    Reads the input data.\n",
    "\n",
    "    :param file_path: (Optional) Position of the txt file in the local system.\n",
    "\n",
    "    :return: book_data: all input characters and unique_characters: unique single characters of the input data.\n",
    "    \"\"\"\n",
    "    book_data = open(file_path, 'r').read()\n",
    "    unique_characters = list(set(book_data))\n",
    "\n",
    "    return book_data, unique_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters to Index transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Char_to_Ind(chars_list, unique_chars):\n",
    "    \"\"\"\n",
    "    Maps the original characters to integers.\n",
    "\n",
    "    :param chars_list: The list of characters to be encoded into integers.\n",
    "    :param unique_chars: The set of unique characters available.\n",
    "\n",
    "    :return: A list of integers that correspond to the characters.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_characters = []\n",
    "\n",
    "    for char in chars_list:\n",
    "        for index, letter in enumerate(unique_chars):\n",
    "\n",
    "            if char == letter:\n",
    "\n",
    "                encoded_characters.append(index)\n",
    "\n",
    "    return encoded_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index to Char transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ind_to_Char(one_hot_representation, unique_chars_list):\n",
    "    \"\"\"\n",
    "    Maps a list of integers to their corresponding characters,\n",
    "\n",
    "    :param one_hot_representation: A list of one_hot representations to be transformed to their correspoding characters.\n",
    "    :param unique_chars_list: The list of unique characters.\n",
    "\n",
    "    :return: The actual character sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    actual_character_sequence = []\n",
    "\n",
    "    for i in range(one_hot_representation.shape[1]):\n",
    "\n",
    "        letter_pos = np.where(one_hot_representation[:,i] == 1.0)[0][0]\n",
    "        actual_character_sequence.append(unique_chars_list[letter_pos])\n",
    "\n",
    "    return actual_character_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_endoding(x, K):\n",
    "    \"\"\"\n",
    "    Creates the one hot encoding representation of an array.\n",
    "\n",
    "\n",
    "    :param x: The array that we wish to map in an one-hot representation.\n",
    "    :param K: The number of distinct classes.\n",
    "\n",
    "    :return: One hot representation of this number.\n",
    "    \"\"\"\n",
    "\n",
    "    x_encoded = np.zeros((K, len(x)))\n",
    "    for index, elem in enumerate(x):\n",
    "\n",
    "        x_encoded[elem, index] = 1.0\n",
    "\n",
    "    return x_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    :param X: ND-Array. Probably should be floats.\n",
    "    :param theta: float parameter, used as a multiplier prior to exponentiation. Default = 1.0\n",
    "    :param axis (optional): axis to compute values along. Default is the first non-singleton axis.\n",
    "\n",
    "    :return: An array the same size as X. The result will sum to 1 along the specified axis\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_smoothed_loss(smoothed_loss, display=False, title=None, save_name=None, save_path='../figures/'):\n",
    "    \"\"\"\n",
    "        Visualization and saving the loss of the network.\n",
    "\n",
    "        :param smoothed_loss: The smooth loss of the RNN network.\n",
    "        :param display: (Optional) Boolean, set to True for displaying the loss evolution plot.\n",
    "        :param title: (Optional) Title of the plot.\n",
    "        :param save_name: (Optional) name of the file to save the plot.\n",
    "        :param save_path: (Optional) Path of the folder to save the plot in your local computer.\n",
    "\n",
    "        :return: None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.plot(smoothed_loss)\n",
    "\n",
    "    if save_name is not None:\n",
    "        if save_path[-1] != '/':\n",
    "            save_path += '/'\n",
    "        plt.savefig(save_path + save_name + '.png')\n",
    "\n",
    "    if display:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Recurrent Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m, K, eta, seq_length, std, epsilon=1e-10):\n",
    "        \"\"\"\n",
    "        Initial setting of the RNN.\n",
    "\n",
    "        :param m: Dimensionality of the hidden state.\n",
    "        :param K: Number of unique classes to identify.\n",
    "        :param eta: The learning rate of the training process.\n",
    "        :param seq_length: The length of the input sequence.\n",
    "        :param std: the variance of the normal distribution that initializes the weight matrices.\n",
    "        :param epsilon: epsilon parameter of ada-grad.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m = m\n",
    "        self.K = K\n",
    "        self.eta = eta\n",
    "        self.seq_length = seq_length\n",
    "        self.std = std\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and bias matrices\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(400)\n",
    "\n",
    "        U = np.random.normal(0, self.std, size=(self.m, self.K))\n",
    "        W = np.random.normal(0, self.std, size=(self.m, self.m))\n",
    "        V = np.random.normal(0, self.std, size=(self.K, self.m))\n",
    "\n",
    "        b = np.zeros((self.m, 1))\n",
    "        c = np.zeros((self.K, 1))\n",
    "\n",
    "        return [W, U, b, V, c]\n",
    "\n",
    "    def synthesize_sequence(self, h0, x0, weight_parameters, text_length):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters under the RNN values.\n",
    "\n",
    "        :param self: The RNN.\n",
    "        :param h0: Hidden state at time 0.\n",
    "        :param x0: First (dummy) input vector of the RNN.\n",
    "        :param weight_parameters: The weighst and biases of the RNN, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "        :param text_length: The length of the text you wish to generate\n",
    "\n",
    "        :return: Synthesized text through.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "        Y = np.zeros(shape=(x0.shape[0], text_length))\n",
    "\n",
    "        alpha = np.dot(W, h0) + np.dot(U, np.expand_dims(x0[:,0], axis=1)) + b\n",
    "        h = np.tanh(alpha)\n",
    "        o = np.dot(V, h) + c\n",
    "        p = softmax(o)\n",
    "\n",
    "        # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "        cumulative_sum = np.cumsum(p)\n",
    "        draw_number = np.random.sample()\n",
    "\n",
    "        # Find the element that corresponds to this random sample\n",
    "        pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "        # Create one-hot representation of the found position\n",
    "        Y[pos, 0] = 1.0\n",
    "\n",
    "        h0 = np.copy(h)\n",
    "        x0 = np.expand_dims(np.copy(Y[:,0]), axis=1)\n",
    "\n",
    "        for index in range(1, text_length):\n",
    "\n",
    "            alpha = np.dot(W, h0) + np.dot(U, x0) + b\n",
    "            h = np.tanh(alpha)\n",
    "            o = np.dot(V, h) + c\n",
    "            p = softmax(o)\n",
    "\n",
    "            # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "            cumulative_sum = np.cumsum(p)\n",
    "            draw_number = np.random.sample()\n",
    "\n",
    "            # Find the element that corresponds to this random sample\n",
    "            pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "            # Create one-hot representation of the found position\n",
    "            Y[pos, index] = 1.0\n",
    "\n",
    "            h0 = np.copy(h)\n",
    "            x0 = np.expand_dims(np.copy(Y[:, index]), axis=1)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def ComputeLoss(self, input_sequence, Y, weight_parameters):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss of the RNN.\n",
    "\n",
    "        :param input_sequence: The input sequence.\n",
    "        :param weight_parameters: Weights and matrices of the RNN, which in particularly are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "\n",
    "        :return: Cross entropy loss (divergence between the predictions and the true output)\n",
    "        \"\"\"\n",
    "\n",
    "        p = self.ForwardPass(input_sequence, weight_parameters)[2]\n",
    "\n",
    "        cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum()\n",
    "\n",
    "        return cross_entropy_loss\n",
    "\n",
    "    def ForwardPass(self, input_sequence, weight_parameters, h0 = None):\n",
    "        \"\"\"\n",
    "        Evaluates the predictions that the RNN does in an input character sequence.\n",
    "\n",
    "        :param input_sequence: The one-hot representation of the input sequence.\n",
    "        :param weight_parameters: The weights and biases of the network, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "        :param h0: Initial hidden state value.\n",
    "\n",
    "        :return: The predicted character sequence based on the input one.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "\n",
    "        p = np.zeros(input_sequence.shape)\n",
    "        if h0 is None:\n",
    "            h_list = [np.zeros((self.m, 1))]\n",
    "        else:\n",
    "            h_list = [h0]\n",
    "\n",
    "        a_list = []\n",
    "\n",
    "        for index in range(0, input_sequence.shape[1]):\n",
    "\n",
    "            alpha = np.dot(W, h_list[-1]) + np.dot(U, np.expand_dims(input_sequence[:,index], axis=1)) + b\n",
    "            a_list.append(alpha)\n",
    "            h = np.tanh(alpha)\n",
    "            h_list.append(h)\n",
    "            o = np.dot(V, h) + c\n",
    "            p[:, index] = softmax(o).reshape(p.shape[0],)\n",
    "\n",
    "        return a_list, h_list[1:], p\n",
    "\n",
    "    def BackwardPass(self, x, Y, p, W, V, a, h, with_clipping=True):\n",
    "        \"\"\"\n",
    "        Computes the gradient updates of the network's weight and bias matrices based on the divergence between the\n",
    "        prediction and the true output.\n",
    "\n",
    "        :param x: Input data to the network.\n",
    "        :param p: Predictions of the network.\n",
    "        :param Y: One-hot representation of the correct sequence.\n",
    "        :param W: Hidden-to-Hidden weight matrix.\n",
    "        :param V: Hidden-to-Output weight matrix.\n",
    "        :param a: Hidden states before non-linearity.\n",
    "        :param h: Hidden states of the network at each time step.\n",
    "        :param with_clipping: (Optional) Set to False for not clipping in he gradients\n",
    "\n",
    "        :return:  Gradient updates.\n",
    "        \"\"\"\n",
    "\n",
    "        # grad_W, grad_U, grad_b, grad_V, grad_c = \\\n",
    "        #     np.zeros(W.shape), np.zeros((W.shape[0], x.shape[1])), np.zeros((W.shape[0], 1)), np.zeros(V), np.zeros((x.shape[0], 1))\n",
    "\n",
    "        # Computing the gradients for the last time step\n",
    "\n",
    "        grad_c = np.expand_dims((p[:, x.shape[1]- 1] - Y[:, x.shape[1]- 1]).T, axis=1)\n",
    "        grad_V = np.dot(grad_c, h[-1].T)\n",
    "\n",
    "        grad_h = np.dot(V.T, grad_c)\n",
    "\n",
    "        grad_b = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[-1].reshape((a[-1].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "        grad_W = np.dot(grad_b, h[-2].T)\n",
    "        grad_U = np.dot(grad_b, np.expand_dims(x[:,-1], axis=0))\n",
    "\n",
    "        grad_a = grad_b\n",
    "\n",
    "        for time_step in reversed(range(x.shape[1]- 1)):\n",
    "\n",
    "            grad_o = np.expand_dims((p[:, time_step] - Y[:, time_step]).T, axis=1)\n",
    "            grad_V += np.dot(grad_o, np.transpose(h[time_step]))\n",
    "            grad_c += grad_o\n",
    "\n",
    "            grad_h = np.dot(V.T, grad_o) + np.dot(grad_a.T, W).T\n",
    "\n",
    "            grad_a = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[time_step].reshape((a[time_step].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "            grad_W += np.dot(grad_a, h[time_step-1].T)\n",
    "            grad_U += np.dot(grad_a, np.expand_dims(x[:,time_step], axis=1).T)\n",
    "\n",
    "            grad_b += grad_a\n",
    "\n",
    "        gradients = [grad_W, grad_U, grad_b, grad_V, grad_c]\n",
    "\n",
    "        if with_clipping:\n",
    "\n",
    "            for index, elem in enumerate(gradients):\n",
    "\n",
    "                gradients[index] = np.maximum(-5, np.minimum(elem, 5))\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def initialize_ada_grad(self, weight_parameters):\n",
    "        \"\"\"\n",
    "        Initializes the ada_grads of the weights parameters.\n",
    "\n",
    "        :param weight_parameters: The weights and biases of the RNN\n",
    "\n",
    "        :return: Initialized ada-grad parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        ada_grads = []\n",
    "\n",
    "        for elem in weight_parameters:\n",
    "\n",
    "            ada_grads.append(np.zeros(shape=elem.shape))\n",
    "\n",
    "        return ada_grads\n",
    "\n",
    "    def ada_grad_update(self, weight_parameters, ada_grads, gradients, eta):\n",
    "        \"\"\"\n",
    "        Conducts one update step of the ada-grants and weights parameters based on the currently estimated gradient updates.\n",
    "\n",
    "        :param weight_parameters: Weights and biases of the RNN network.\n",
    "        :param ada_grads: Ada grad parameters.\n",
    "        :param gradients: Gradient updates of a training step.\n",
    "        :param eta: Learning rate of the training process.\n",
    "\n",
    "        :return: Updated weight and ada_grad parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update ada-grads\n",
    "        for index, elem in enumerate(ada_grads):\n",
    "\n",
    "            elem += gradients[index] ** 2\n",
    "\n",
    "        for index, weight_elem in enumerate(weight_parameters):\n",
    "\n",
    "            weight_elem -= eta * gradients[index] / np.sqrt(ada_grads[index] + self.epsilon)\n",
    "\n",
    "        return weight_parameters, ada_grads\n",
    "\n",
    "\n",
    "    def fit(self, X, Y, epoches, unique_characters, verbose=True):\n",
    "        \"\"\"\n",
    "        Comnducts the training pprocess of the RNN nad estimates the model.\n",
    "\n",
    "        :param X: Input data (one-hot representation).\n",
    "        :param Y: Treu labels (one-hot representation).\n",
    "        :param epoches: Number of training epochs.\n",
    "        :param unique_characters: The unique characters that can be generated from the training process.\n",
    "\n",
    "        :return: The trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        weight_parameters = self.init_weights()\n",
    "        gradient_object = Gradients(self)\n",
    "\n",
    "        # Number of distinct training sequences per epoch\n",
    "        training_sequences_per_epoch = X.shape[1] - self.seq_length\n",
    "\n",
    "        ada_grads = self.initialize_ada_grad(weight_parameters)\n",
    "\n",
    "        for epoch in range(epoches):\n",
    "\n",
    "            hprev = np.zeros(shape=(self.m, 1))\n",
    "\n",
    "            for e in range(training_sequences_per_epoch):\n",
    "\n",
    "                current_update_step = epoch * training_sequences_per_epoch + e\n",
    "\n",
    "                x = X[:, e:e + self.seq_length]\n",
    "                y = Y[:, e + 1:e + self.seq_length + 1]\n",
    "\n",
    "                gradient_updates, hprev = gradient_object.ComputeGradients(x, y, weight_parameters, hprev)\n",
    "\n",
    "                weight_parameters, ada_grads = self.ada_grad_update(weight_parameters, ada_grads, gradient_updates, eta=self.eta)\n",
    "\n",
    "                if epoch ==0 and e ==0 :\n",
    "                    smooth_loss_evolution = [self.ComputeLoss(x, y, weight_parameters)]\n",
    "                    minimum_loss = smooth_loss_evolution[0]\n",
    "                else:\n",
    "                    current_loss = 0.999 * smooth_loss_evolution[-1] + 0.001 * self.ComputeLoss(x, y, weight_parameters)\n",
    "                    smooth_loss_evolution.append(current_loss)\n",
    "                    if current_loss < minimum_loss:\n",
    "                        best_weights = weight_parameters\n",
    "\n",
    "                if verbose:\n",
    "                    \n",
    "                    if len(smooth_loss_evolution) % 100 == 0 and len(smooth_loss_evolution) > 0:\n",
    "                        print('---------------------------------------------------------')\n",
    "                        print(f'Smooth loss at update step no.{current_update_step}: {smooth_loss_evolution[-1]}')\n",
    "    \n",
    "                        # Also generate synthesized text if 500 updates steps have been conducted\n",
    "                        if len(smooth_loss_evolution) % 500 == 0 and len(smooth_loss_evolution) > 0:\n",
    "    \n",
    "                            synthesized_text = Ind_to_Char(self.synthesize_sequence(h0=hprev, x0=X, weight_parameters=weight_parameters, text_length=200), unique_characters)\n",
    "                            print('---------------------------------------------------------')\n",
    "                            print(f'Synthesized text of update step no.{current_update_step}')\n",
    "                            print(''.join(synthesized_text))\n",
    "\n",
    "        return best_weights, smooth_loss_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradients:\n",
    "\n",
    "    def __init__(self, RNN):\n",
    "        self.RNN = RNN\n",
    "\n",
    "    def ComputeGradients(self, X, Y, weight_parameters, hprev, with_clipping=True):\n",
    "        \"\"\"\n",
    "        Computes the analytical gradient updates of the network.\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "        :param hprev: Initial hidden state to be used in the forward and backwward pass of the RNN.\n",
    "        :param with_clipping: (Optional) Set ot False if you don't wish to apply clipping in the gradients.\n",
    "\n",
    "        :return: Gradients updates.\n",
    "        \"\"\"\n",
    "\n",
    "        a_list, h_list, p = self.RNN.ForwardPass(X, weight_parameters, h0=hprev)\n",
    "        gradients = self.RNN.BackwardPass(X, Y, p, weight_parameters[0], weight_parameters[3], a_list, h_list, with_clipping)\n",
    "\n",
    "        return gradients, h_list[0]\n",
    "\n",
    "    def ComputeGradsNumSlow(self, X, Y, weight_parameters, h=1e-4):\n",
    "\n",
    "        from tqdm import tqdm\n",
    "        all_grads_num = []\n",
    "\n",
    "        for index, elem in enumerate(weight_parameters):\n",
    "\n",
    "            grad_elem = np.zeros(elem.shape)\n",
    "            # h_prev = np.zeros((W.shape[1], 1))\n",
    "\n",
    "            for i in tqdm(range(elem.shape[0])):\n",
    "                for j in range(elem.shape[1]):\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] -= h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c1 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] += h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c2 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    grad_elem[i, j] = (c2-c1) / (2*h)\n",
    "\n",
    "            all_grads_num.append(grad_elem)\n",
    "\n",
    "        return all_grads_num\n",
    "\n",
    "    def check_similarity(self, X, Y, weight_parameters, with_cliping = False):\n",
    "        \"\"\"\n",
    "        Computes and compares the analytical and numerical gradients.\n",
    "\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "        :param with_cliping: (Optional) Set to True to apply clipping in the gradients\n",
    "\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "\n",
    "        analytical_gradients, _ = self.ComputeGradients(X, Y, weight_parameters, hprev=np.zeros(shape=(self.RNN.m, 1)))\n",
    "        numerical_gradients = self.ComputeGradsNumSlow(X, Y, weight_parameters)\n",
    "\n",
    "        for weight_index in range(len(analytical_gradients)):\n",
    "            print('-----------------')\n",
    "            print(f'Weight parameter no. {weight_index+1}:')\n",
    "\n",
    "            weight_abs = np.abs(analytical_gradients[weight_index] - numerical_gradients[weight_index])\n",
    "\n",
    "            weight_nominator = np.average(weight_abs)\n",
    "\n",
    "            grad_weight_abs = np.absolute(analytical_gradients[weight_index])\n",
    "            grad_weight_num_abs = np.absolute(numerical_gradients[weight_index])\n",
    "\n",
    "            sum_weight = grad_weight_abs + grad_weight_num_abs\n",
    "\n",
    "            print(f'Deviation between analytical and numerical gradients: {weight_nominator / np.amax(sum_weight)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data, unique_characters = Load_Text_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Set hyper-parameters & initialize the RNN’s parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.1)\n",
    "\n",
    "weight_parameters = rnn.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Synthesize text from your randomly initialized RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6}Cf}KL9D qSWzI;)Jv}J\"!'A\n"
     ]
    }
   ],
   "source": [
    "input_sequence = book_data[:rnn.seq_length]\n",
    "\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "test = rnn.synthesize_sequence(h0=np.zeros((rnn.m, 1)), x0=input_sequence_one_hot, weight_parameters=weight_parameters, text_length=rnn.seq_length)\n",
    "test2 = Ind_to_Char(test, unique_characters)\n",
    "print(''.join(test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Implement the forward and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vallidate the correct performance of the forward and backward pass by comparing with the numerical gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data, unique_characters = Load_Text_Data()\n",
    "\n",
    "rnn_object = RNN(m=5, K=len(unique_characters), eta=0.01, seq_length=25, std=0.01)\n",
    "gradient_object = Gradients(rnn_object)\n",
    "\n",
    "weight_parameters = rnn_object.init_weights()\n",
    "\n",
    "input_sequence = book_data[:rnn_object.seq_length]\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "output_sequence = book_data[1:1 + rnn_object.seq_length]\n",
    "output_encoding = Char_to_Ind(output_sequence, unique_characters)\n",
    "output_sequence_one_hot = create_one_hot_endoding(output_encoding, len(unique_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 79.17it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.70it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 398.28it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 102.19it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 508.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Weight parameter no. 1:\n",
      "Deviation between analytical and numerical gradients: 0.01530391399828311\n",
      "-----------------\n",
      "Weight parameter no. 2:\n",
      "Deviation between analytical and numerical gradients: 9.347829544732643e-11\n",
      "-----------------\n",
      "Weight parameter no. 3:\n",
      "Deviation between analytical and numerical gradients: 6.028087082473465e-10\n",
      "-----------------\n",
      "Weight parameter no. 4:\n",
      "Deviation between analytical and numerical gradients: 8.20341705132328e-10\n",
      "-----------------\n",
      "Weight parameter no. 5:\n",
      "Deviation between analytical and numerical gradients: 6.804348250238339e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gradient_object.check_similarity(input_sequence_one_hot, output_sequence_one_hot, weight_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Train your RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include a graph of the smooth loss function for a longish training run (at least 2 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.01, seq_length=25, std=0.1)\n",
    "\n",
    "# Create one-hot data\n",
    "integer_encoding = Char_to_Ind(book_data, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "output_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "weight_parameters = rnn.fit(X=input_sequence_one_hot, Y=output_sequence_one_hot, epoches=3, unique_characters=unique_characters, verbose=False)\n",
    "inform_exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_loss_evolution = weight_parameters[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_smoothed_loss(smoothed_loss_evolution, display=True, title='Smooth loss evolution', save_name='sm_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
