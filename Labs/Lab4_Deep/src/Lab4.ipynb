{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import everything you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Text_Data(file_path='../goblet_book.txt'):\n",
    "    \"\"\"\n",
    "    Reads the input data.\n",
    "\n",
    "    :param file_path: (Optional) Position of the txt file in the local system.\n",
    "\n",
    "    :return: book_data: all input characters and unique_characters: unique single characters of the input data.\n",
    "    \"\"\"\n",
    "    book_data = open(file_path, 'r').read()\n",
    "    unique_characters = list(set(book_data))\n",
    "\n",
    "    return book_data, unique_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characters to Index transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Char_to_Ind(chars_list, unique_chars):\n",
    "    \"\"\"\n",
    "    Maps the original characters to integers.\n",
    "\n",
    "    :param chars_list: The list of characters to be encoded into integers.\n",
    "    :param unique_chars: The set of unique characters available.\n",
    "\n",
    "    :return: A list of integers that correspond to the characters.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_characters = []\n",
    "\n",
    "    for char in chars_list:\n",
    "        for index, letter in enumerate(unique_chars):\n",
    "\n",
    "            if char == letter:\n",
    "\n",
    "                encoded_characters.append(index)\n",
    "\n",
    "    return encoded_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index to Char transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ind_to_Char(one_hot_representation, unique_chars_list):\n",
    "    \"\"\"\n",
    "    Maps a list of integers to their corresponding characters,\n",
    "\n",
    "    :param one_hot_representation: A list of one_hot representations to be transformed to their correspoding characters.\n",
    "    :param unique_chars_list: The list of unique characters.\n",
    "\n",
    "    :return: The actual character sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    actual_character_sequence = []\n",
    "\n",
    "    for i in range(one_hot_representation.shape[1]):\n",
    "\n",
    "        letter_pos = np.where(one_hot_representation[:,i] == 1.0)[0][0]\n",
    "        actual_character_sequence.append(unique_chars_list[letter_pos])\n",
    "\n",
    "    return actual_character_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_endoding(x, K):\n",
    "    \"\"\"\n",
    "    Creates the one hot encoding representation of an array.\n",
    "\n",
    "\n",
    "    :param x: The array that we wish to map in an one-hot representation.\n",
    "    :param K: The number of distinct classes.\n",
    "\n",
    "    :return: One hot representation of this number.\n",
    "    \"\"\"\n",
    "\n",
    "    x_encoded = np.zeros((K, len(x)))\n",
    "    for index, elem in enumerate(x):\n",
    "\n",
    "        x_encoded[elem, index] = 1.0\n",
    "\n",
    "    return x_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    Softmax over numpy rows and columns, taking care for overflow cases\n",
    "    Many thanks to https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Usage: Softmax over rows-> axis =0, softmax over columns ->axis =1\n",
    "\n",
    "    :param X: ND-Array. Probably should be floats.\n",
    "    :param theta: float parameter, used as a multiplier prior to exponentiation. Default = 1.0\n",
    "    :param axis (optional): axis to compute values along. Default is the first non-singleton axis.\n",
    "\n",
    "    :return: An array the same size as X. The result will sum to 1 along the specified axis\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Recurrent Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network object\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m, K, eta, seq_length, std):\n",
    "        \"\"\"\n",
    "        Initial setting of the RNN.\n",
    "\n",
    "        :param m: Dimensionality of the hidden state.\n",
    "        :param K: Number of unique classes to identify.\n",
    "        :param eta: The learning rate of the training process.\n",
    "        :param seq_length: The length of the input sequence.\n",
    "        :param std: the variance of the normal distribution that initializes the weight matrices.\n",
    "        \"\"\"\n",
    "\n",
    "        self.m = m\n",
    "        self.K = K\n",
    "        self.eta = eta\n",
    "        self.seq_length = seq_length\n",
    "        self.std = std\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights and bias matrices\n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(400)\n",
    "\n",
    "        U = np.random.normal(0, self.std, size=(self.m, self.K))\n",
    "        W = np.random.normal(0, self.std, size=(self.m, self.m))\n",
    "        V = np.random.normal(0, self.std, size=(self.K, self.m))\n",
    "\n",
    "        b = np.zeros((self.m, 1))\n",
    "        c = np.zeros((self.K, 1))\n",
    "\n",
    "        return [W, U, b, V, c]\n",
    "\n",
    "    def synthesize_sequence(self, h0, x0, weight_parameters):\n",
    "        \"\"\"\n",
    "        Synthesizes a sequence of characters under the RNN values.\n",
    "\n",
    "        :param self: The RNN.\n",
    "        :param h0: Hidden state at time 0.\n",
    "        :param x0: First (dummy) input vector of the RNN.\n",
    "        :param weight_parameters: The weighst and biases of the RNN, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "\n",
    "        :return: Synthesized text through.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "        Y = np.zeros(x0.shape)\n",
    "\n",
    "        alpha = np.dot(W, h0) + np.dot(U, np.expand_dims(x0[:,0], axis=1)) + b\n",
    "        h = np.tanh(alpha)\n",
    "        o = np.dot(V, h) + c\n",
    "        p = softmax(o)\n",
    "\n",
    "        # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "        cumulative_sum = np.cumsum(p)\n",
    "        draw_number = np.random.sample()\n",
    "\n",
    "        # Find the element that corresponds to this random sample\n",
    "        pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "        # Create one-hot representation of the found position\n",
    "        Y[pos, 0] = 1.0\n",
    "\n",
    "        h0 = np.copy(h)\n",
    "        x0 = np.expand_dims(np.copy(Y[:,0]), axis=1)\n",
    "\n",
    "        for index in range(1, self.seq_length):\n",
    "\n",
    "            alpha = np.dot(W, h0) + np.dot(U, x0) + b\n",
    "            h = np.tanh(alpha)\n",
    "            o = np.dot(V, h) + c\n",
    "            p = softmax(o)\n",
    "\n",
    "            # Compute the cumulative sum of p and draw a random sample from [0,1)\n",
    "            cumulative_sum = np.cumsum(p)\n",
    "            draw_number = np.random.sample()\n",
    "\n",
    "            # Find the element that corresponds to this random sample\n",
    "            pos = np.where(cumulative_sum > draw_number)[0][0]\n",
    "\n",
    "            # Create one-hot representation of the found position\n",
    "            Y[pos, index] = 1.0\n",
    "\n",
    "            h0 = np.copy(h)\n",
    "            x0 = np.expand_dims(np.copy(Y[:, index]), axis=1)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def ComputeLoss(self, input_sequence, Y, weight_parameters):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss of the RNN.\n",
    "\n",
    "        :param input_sequence: The input sequence.\n",
    "        :param weight_parameters: Weights and matrices of the RNN, which in particularly are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "\n",
    "        :return: Cross entropy loss (divergence between the predictions and the true output)\n",
    "        \"\"\"\n",
    "\n",
    "        p = self.ForwardPass(input_sequence, weight_parameters)[2]\n",
    "\n",
    "        cross_entropy_loss = -np.log(np.diag(np.dot(Y.T, p))).sum()\n",
    "\n",
    "        return cross_entropy_loss\n",
    "\n",
    "    def ForwardPass(self, input_sequence, weight_parameters):\n",
    "        \"\"\"\n",
    "        Evaluates the predictions that the RNN does in an input character sequence.\n",
    "\n",
    "        :param input_sequence: The one-hot representation of the input sequence.\n",
    "        :param weight_parameters: The weights and biases of the network, which are:\n",
    "            :param W: Hidden-to-Hidden weight matrix.\n",
    "            :param U: Input-to-Hidden weight matrix.\n",
    "            :param b: Bias vector of the hidden layer.\n",
    "            :param V: Hidden-to-Output weight matrix.\n",
    "            :param c: Bias vector of the output layer.\n",
    "\n",
    "        :return: The predicted character sequence based on the input one.\n",
    "        \"\"\"\n",
    "\n",
    "        W, U, b, V, c = weight_parameters\n",
    "\n",
    "        p = np.zeros(input_sequence.shape)\n",
    "        h_list = [np.zeros((self.m, 1))]\n",
    "        a_list = []\n",
    "\n",
    "        for index in range(0, input_sequence.shape[1]):\n",
    "\n",
    "            alpha = np.dot(W, h_list[-1]) + np.dot(U, np.expand_dims(input_sequence[:,index], axis=1)) + b\n",
    "            a_list.append(alpha)\n",
    "            h = np.tanh(alpha)\n",
    "            h_list.append(h)\n",
    "            o = np.dot(V, h) + c\n",
    "            p[:, index] = softmax(o).reshape(p.shape[0],)\n",
    "\n",
    "        return a_list, h_list[1:], p\n",
    "\n",
    "    def BackwardPass(self, x, Y, p, W, V, a, h, clipping=True):\n",
    "        \"\"\"\n",
    "        Computes the gradient updates of the network's weight and bias matrices based on the divergence between the\n",
    "        prediction and the true output.\n",
    "\n",
    "        :param x: Input data to the network.\n",
    "        :param p: Predictions of the network.\n",
    "        :param Y: One-hot representation of the correct sequence.\n",
    "        :param W: Hidden-to-Hidden weight matrix.\n",
    "        :param V: Hidden-to-Output weight matrix.\n",
    "        :param a: Hidden states before non-linearity.\n",
    "        :param h: Hidden states of the network at each time step.\n",
    "        :param clipping: (Optional) Set to False for not clipping in he gradients\n",
    "\n",
    "        :return:  Gradient updates.\n",
    "        \"\"\"\n",
    "\n",
    "        # grad_W, grad_U, grad_b, grad_V, grad_c = \\\n",
    "        #     np.zeros(W.shape), np.zeros((W.shape[0], x.shape[1])), np.zeros((W.shape[0], 1)), np.zeros(V), np.zeros((x.shape[0], 1))\n",
    "\n",
    "        # Computing the gradients for the last time step\n",
    "\n",
    "        grad_c = np.expand_dims((p[:, x.shape[1]- 1] - Y[:, x.shape[1]- 1]).T, axis=1)\n",
    "        grad_V = np.dot(grad_c, h[-1].T)\n",
    "\n",
    "        grad_h = np.dot(V.T, grad_c)\n",
    "\n",
    "        grad_b = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[-1].reshape((a[-1].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "        grad_W = np.dot(grad_b, h[-2].T)\n",
    "        grad_U = np.dot(grad_b, np.expand_dims(x[:,-1], axis=0))\n",
    "\n",
    "        grad_a = grad_b\n",
    "\n",
    "        for time_step in reversed(range(x.shape[1]- 1)):\n",
    "\n",
    "            grad_o = np.expand_dims((p[:, time_step] - Y[:, time_step]).T, axis=1)\n",
    "            grad_V += np.dot(grad_o, np.transpose(h[time_step]))\n",
    "            grad_c += grad_o\n",
    "\n",
    "            grad_h = np.dot(V.T, grad_o) + np.dot(grad_a.T, W).T\n",
    "\n",
    "            grad_a = np.expand_dims(np.dot(grad_h.reshape((grad_h.shape[0],)), np.diag(1 - np.tanh(a[time_step].reshape((a[time_step].shape[0],))) ** 2)), axis=1)\n",
    "\n",
    "            grad_W += np.dot(grad_a, h[time_step-1].T)\n",
    "            grad_U += np.dot(grad_a, np.expand_dims(x[:,time_step], axis=1).T)\n",
    "\n",
    "            grad_b += grad_a\n",
    "\n",
    "        return [grad_W, grad_U, grad_b, grad_V, grad_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradients:\n",
    "\n",
    "    def __init__(self, RNN):\n",
    "        self.RNN = RNN\n",
    "\n",
    "    def ComputeGradients(self, X, Y, weight_parameters):\n",
    "        \"\"\"\n",
    "        Computes the analytical gradient updates of the network.\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "\n",
    "        :return: Gradients updates.\n",
    "        \"\"\"\n",
    "\n",
    "        a_list, h_list, p = self.RNN.ForwardPass(X, weight_parameters)\n",
    "        gradients = self.RNN.BackwardPass(X, Y, p, weight_parameters[0], weight_parameters[3], a_list, h_list)\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def ComputeGradsNumSlow(self, X, Y, weight_parameters, h=1e-4):\n",
    "\n",
    "        all_grads_num = []\n",
    "\n",
    "        for index, elem in enumerate(weight_parameters):\n",
    "\n",
    "            grad_elem = np.zeros(elem.shape)\n",
    "            # h_prev = np.zeros((W.shape[1], 1))\n",
    "\n",
    "            for i in range(elem.shape[0]):\n",
    "                for j in range(elem.shape[1]):\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] -= h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c1 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    elem_try = np.copy(elem)\n",
    "                    elem_try[i, j] += h\n",
    "                    all_weights_try = weight_parameters.copy()\n",
    "                    all_weights_try[index] = elem_try\n",
    "                    c2 = self.RNN.ComputeLoss(X, Y, weight_parameters=all_weights_try)\n",
    "\n",
    "                    grad_elem[i, j] = (c2-c1) / (2*h)\n",
    "\n",
    "            all_grads_num.append(grad_elem)\n",
    "\n",
    "        return all_grads_num\n",
    "\n",
    "    def check_similarity(self, X, Y, weight_parameters):\n",
    "        \"\"\"\n",
    "        Computes and compares the analytical and numerical gradients.\n",
    "\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "        analytical_gradients = self.ComputeGradients(X, Y, weight_parameters)\n",
    "        numerical_gradients = self.ComputeGradsNumSlow(X, Y, weight_parameters)\n",
    "\n",
    "        for weight_index in range(len(analytical_gradients)):\n",
    "            print('-----------------')\n",
    "            print(f'Weight parameter no. {weight_index+1}:')\n",
    "\n",
    "            weight_abs = np.abs(analytical_gradients[weight_index] - numerical_gradients[weight_index])\n",
    "\n",
    "            weight_nominator = np.average(weight_abs)\n",
    "\n",
    "            grad_weight_abs = np.absolute(analytical_gradients[weight_index])\n",
    "            grad_weight_num_abs = np.absolute(numerical_gradients[weight_index])\n",
    "\n",
    "            sum_weight = grad_weight_abs + grad_weight_num_abs\n",
    "\n",
    "            print(f'Deviation between analytical and numerical gradients: {weight_nominator / np.amax(sum_weight)}')\n",
    "\n",
    "    def new_check_similarity(self, analytical_gradients, numerical_gradients):\n",
    "        \"\"\"\n",
    "        Computes and compares the analytical and numerical gradients.\n",
    "\n",
    "        :param X: Input sequence.\n",
    "        :param Y: True output\n",
    "        :param weight_parameters: Weights and bias matrices of the network.\n",
    "\n",
    "        :return: None.\n",
    "        \"\"\"\n",
    "\n",
    "        for weight_index in range(len(analytical_gradients)):\n",
    "            print('-----------------')\n",
    "            print(f'Weight parameter no. {weight_index+1}:')\n",
    "\n",
    "            weight_abs = np.abs(analytical_gradients[weight_index] - numerical_gradients[weight_index])\n",
    "\n",
    "            weight_nominator = np.average(weight_abs)\n",
    "\n",
    "            grad_weight_abs = np.absolute(analytical_gradients[weight_index])\n",
    "            grad_weight_num_abs = np.absolute(numerical_gradients[weight_index])\n",
    "\n",
    "            sum_weight = grad_weight_abs + grad_weight_num_abs\n",
    "\n",
    "            print(f'Deviation between analytical and numerical gradients: {weight_nominator / np.amax(sum_weight)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data, unique_characters = Load_Text_Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Set hyper-parameters & initialize the RNN’s parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(m=100, K=len(unique_characters), eta=0.1, seq_length=25, std=0.1)\n",
    "\n",
    "weight_parameters = rnn.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Synthesize text from your randomly initialized RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UWS/eg()B1c.iwlN-'rEgT;^6\n"
     ]
    }
   ],
   "source": [
    "input_sequence = book_data[:rnn.seq_length]\n",
    "\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "test = rnn.synthesize_sequence(h0=np.zeros((rnn.m, 1)), x0=input_sequence_one_hot, weight_parameters=weight_parameters)\n",
    "test2 = Ind_to_Char(test, unique_characters)\n",
    "print(''.join(test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.4 Implement the forward and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vallidate the correct performance of the forward and backward pass by comparing with the numerical gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Weight parameter no. 1:\n",
      "Deviation between analytical and numerical gradients: 0.01530391399828311\n",
      "-----------------\n",
      "Weight parameter no. 2:\n",
      "Deviation between analytical and numerical gradients: 9.347829544732643e-11\n",
      "-----------------\n",
      "Weight parameter no. 3:\n",
      "Deviation between analytical and numerical gradients: 6.028087082473465e-10\n",
      "-----------------\n",
      "Weight parameter no. 4:\n",
      "Deviation between analytical and numerical gradients: 8.20341705132328e-10\n",
      "-----------------\n",
      "Weight parameter no. 5:\n",
      "Deviation between analytical and numerical gradients: 6.804348250238339e-11\n"
     ]
    }
   ],
   "source": [
    "book_data, unique_characters = Load_Text_Data()\n",
    "\n",
    "rnn_object = RNN(m=5, K=len(unique_characters), eta=0.1, seq_length=25, std=0.01)\n",
    "gradient_object = Gradients(rnn_object)\n",
    "\n",
    "weight_parameters = rnn_object.init_weights()\n",
    "\n",
    "input_sequence = book_data[:rnn_object.seq_length]\n",
    "integer_encoding = Char_to_Ind(input_sequence, unique_characters)\n",
    "input_sequence_one_hot = create_one_hot_endoding(integer_encoding, len(unique_characters))\n",
    "\n",
    "output_sequence = book_data[1:1 + rnn_object.seq_length]\n",
    "output_encoding = Char_to_Ind(output_sequence, unique_characters)\n",
    "output_sequence_one_hot = create_one_hot_endoding(output_encoding, len(unique_characters))\n",
    "\n",
    "gradient_object.check_similarity(input_sequence_one_hot, output_sequence_one_hot, weight_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5 Train your RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
